---
layout: post
title: "論文解説: Prompt Cache — モジュラーAttention再利用による低レイテンシLLM推論"
description: "Prompt Cacheはプロンプト内の共通テキストセグメントのKVキャッシュをセッション横断で再利用し、TTFTを最大8倍短縮する推論最適化手法である"
categories: [blog, paper, arxiv]
tags: [prompt-caching, kv-cache, llm-inference, transformer, latency-optimization, aws, bedrock, rag]
date: 2026-02-22 18:00:00 +0900
source_type: arxiv
arxiv_id: "2311.04934"
source_url: https://arxiv.org/abs/2311.04934
zenn_article: d027acf4081b9d
zenn_url: https://zenn.dev/0h_n0/articles/d027acf4081b9d
math: true
mermaid: true
target_audience: "修士学生レベル"
---

本記事は [arXiv:2311.04934](https://arxiv.org/abs/2311.04934) の解説記事です。

## 論文概要（Abstract）

LLM推論におけるプロンプト処理は、トークン長が1,000を超えることが一般的になりレイテンシの支配的要因となっている。著者らは、異なるLLMクエリ間でプロンプトの共通テキストセグメント（システムプロンプト、ドキュメントコンテキスト等）のAttention States（KVキャッシュ）を再利用する推論スキーム「Prompt Cache」を提案している。事前計算したAttention Statesを保存し、ユーザープロンプトに同一セグメントが含まれる場合は再計算を省略することで、NVIDIA A100 GPU上でTime-to-First-Token（TTFT）を最大8倍短縮したと報告されている。

この記事は [Zenn記事: Bedrock AgentCore×1時間キャッシュで社内RAGコスト90%削減](https://zenn.dev/0h_n0/articles/d027acf4081b9d) の深掘りです。

## 情報源

- **arXiv ID**: 2311.04934
- **URL**: [https://arxiv.org/abs/2311.04934](https://arxiv.org/abs/2311.04934)
- **著者**: In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong（Yale大学）
- **発表年**: 2023年（MLSys 2024 採択）
- **分野**: cs.LG, cs.AI, cs.DC

## 背景と動機（Background & Motivation）

LLM推論は (1) プロンプト処理（Attention States計算）と (2) トークン生成（逐次KV更新）の2段階で構成される。プロンプト長が増加するにつれ、プロンプト処理がレイテンシを支配するようになった。著者らは、異なるユーザーのプロンプト間に共通テキストセグメントが頻出することに着目している。例えば、チャットボットのシステムプロンプトは全ユーザーで同一であり、ドキュメントQAのドキュメント本体は同じドキュメントに対する複数質問で共有される。

従来のKVキャッシュは単一推論セッション内でしか再利用されず、セッション間の共通性は活用されていなかった。また、vLLMやSGLangのPrefix Cachingはプロンプト**先頭**の共通プレフィックスのみを対象としており、プロンプト中間や末尾に共通セグメントが配置されるケースには対応できないという制約があった。

## 主要な貢献（Key Contributions）

- **貢献1**: 異なるLLMクエリ間でAttention Statesをセッション横断で再利用する推論スキーム「Prompt Cache」を提案
- **貢献2**: 再利用の正確性を保証するPosition-Aware Encodingと、再利用可能セグメントを定義するPrompt Markup Language（PML）を設計
- **貢献3**: Document QA・チャットボット・要約・マルチターン会話・In-Context Learningの5アプリケーションで、出力品質の劣化なくTTFTを最大8倍短縮

## 技術的詳細（Technical Details）

### Position-Aware Encoding

Prompt Cacheの核心的課題は、再利用されるAttention Statesの正確性保証である。Transformerモデルでは、トークンのAttention States（Key・Value）はそのトークンの**絶対位置**に依存する。

テキストセグメント $S = [s_1, s_2, \ldots, s_m]$ がプロンプト内の位置 $i$ から始まる場合、トークン $s_j$ のKey・Valueは以下で計算される：

$$
K_{s_j} = W_K \cdot x(s_j, i + j - 1)
$$

$$
V_{s_j} = W_V \cdot x(s_j, i + j - 1)
$$

ここで、
- $W_K, W_V$: Key・Value射影行列
- $x(s_j, i+j-1)$: トークン $s_j$ に位置 $i+j-1$ のエンコーディングを適用した入力埋め込み
- $i$: セグメントの開始位置

キャッシュ時の位置 $i'$ と参照時の位置 $i$ が異なると $K_{s_j}^{\text{cached}} \neq K_{s_j}$ となり、出力が不正確になる。Prompt Cacheはモジュール宣言時の位置IDを固定し、参照時に同一の位置IDを使用することでこの問題を解消している。

著者らは、Absolute Position Encoding・RoPE（Rotary Position Embedding）・ALiBi の3方式すべてで互換性を確認したと報告している。

### Prompt Markup Language (PML)

PMLはXMLライクなスキーマで、再利用可能テキストセグメントを「プロンプトモジュール」として宣言・参照する。

```xml
<!-- モジュール宣言: ドキュメントのAttention Statesを事前計算・保存 -->
<module id="doc1">
  社内規程の本文テキスト...（数千トークン）
</module>

<!-- モジュール参照: 保存済みAttention Statesを読み込み -->
<use id="doc1" />
Question: 休暇申請の手順は？
```

マルチターン会話では、会話履歴を段階的にモジュール化できる：

```xml
<use id="system_prompt" />
<use id="turn1" />
<module id="turn2">
User: 追加質問
Assistant: 回答
</module>
```

### キャッシュ管理

Attention Stateキャッシュのメモリ要件は以下の式で計算される：

$$
\text{Memory} = 2 \times L \times m \times d \times \text{sizeof}(\text{float16})
$$

ここで、
- $L$: Transformerの層数
- $m$: モジュールのトークン数
- $d$: モデルの隠れ層次元数

論文Table 3より、具体的なメモリオーバーヘッドは以下の通りである：

| モデル | モジュール長 | メモリ |
|--------|------------|--------|
| LLaMA-2-7B | 1,024トークン | 536 MB |
| LLaMA-2-13B | 1,024トークン | 1.0 GB |
| LLaMA-2-70B | 1,024トークン | 5.4 GB |
| LLaMA-2-70B | 4,096トークン | 21.5 GB |

キャッシュがフルになった場合はLRU（Least Recently Used）ポリシーでエビクションが行われる。モジュールのテキストが変更された場合はキャッシュが自動無効化される。

### アルゴリズム

```python
def prompt_cache_inference(
    user_prompt: list[str],
    module_refs: list[str],
    cache: dict[str, tuple[torch.Tensor, torch.Tensor]],
) -> str:
    """Prompt Cache付きLLM推論

    Args:
        user_prompt: ユーザープロンプトのトークン列
        module_refs: 参照するモジュールIDリスト
        cache: モジュールID -> (Key, Value) テンソルのマッピング

    Returns:
        生成されたテキスト
    """
    kv_states = []

    # キャッシュ済みモジュールのAttention Statesを読み込み
    for mod_id in module_refs:
        if mod_id in cache:
            kv_states.append(cache[mod_id])  # 再計算不要

    # キャッシュされていない部分のみAttention Statesを計算
    uncached_tokens = extract_uncached(user_prompt, module_refs)
    new_kv = compute_attention_states(uncached_tokens)
    kv_states.append(new_kv)

    # 結合したKV statesでトークン生成
    combined_kv = concatenate(kv_states)
    output = generate_tokens(combined_kv)
    return output
```

## 実装のポイント（Implementation）

実装上の重要な注意点として、著者らは以下を挙げている：

1. **位置ID整合性**: キャッシュモジュールの位置IDはユーザープロンプト内の配置位置と一致させる必要がある。PMLのスキーマ設計段階でモジュールの配置位置を固定することが推奨される
2. **最小トークン要件**: キャッシュの恩恵を得るにはモジュールが十分な長さ（数百トークン以上）である必要がある。短いモジュールではキャッシュロード自体のオーバーヘッドが再計算コストを上回る場合がある
3. **プレフィックス位置以外の対応**: vLLMのPrefix Cachingと異なり、Prompt Cacheはプロンプト内の任意位置（先頭・中間・末尾）のモジュールに対応する。RAGでドキュメントがプロンプト中間に配置される場合に有用

## 実験結果（Results）

著者らは5つのアプリケーションでLLaMA-2（7B〜70B）を対象に評価を行っている。

### TTFT短縮の主要結果（論文Table 1, 2, Figure 3より）

| アプリケーション | モデル | 共有セグメント長 | Standard TTFT | Prompt Cache TTFT | 高速化倍率 |
|----------------|--------|-----------------|---------------|-------------------|-----------|
| Document QA | LLaMA-2-7B | 1,024トークン | 412 ms | 86 ms | 4.8x |
| Document QA | LLaMA-2-70B | 4,096トークン | 8,432 ms | 1,052 ms | **8.0x** |
| チャットボット | LLaMA-2-7B | 512トークン | 395 ms | 148 ms | 2.7x |
| マルチターン（5ターン） | LLaMA-2-7B | ~500トークン | 580 ms | 185 ms | 3.1x |
| In-Context Learning | LLaMA-2-13B | 2,048トークン | 1,245 ms | 312 ms | 4.0x |
| 複合（doc+system+history） | LLaMA-2-7B | 2,048+256+可変 | 4,200 ms | 630 ms | 6.7x |

### Prefix Cachingとの比較（論文Table 4より）

共有セグメントがプロンプト先頭以外に配置される場合のTTFT比較（LLaMA-2-7B、1,024トークン文書）：

| 文書配置位置 | Standard | Prefix Cache | Prompt Cache |
|-------------|----------|-------------|-------------|
| 先頭（prefix） | 412 ms | 86 ms | 86 ms |
| 中間 | 412 ms | **412 ms** | **86 ms** |
| 末尾 | 412 ms | **412 ms** | **86 ms** |

Prefix Cachingは先頭配置でのみ効果を発揮するのに対し、Prompt Cacheは配置位置に関係なく同一の高速化を実現している。

### 出力品質

著者らは全アプリケーションでExact Match（トークン単位の完全一致）が100%であったと報告しており、Position-Aware EncodingによりAttention Statesの正確性が完全に維持されることを確認している。

## 実運用への応用（Practical Applications）

Prompt Cacheの設計思想は、現在のクラウドLLMサービスに広く採用されている。Amazon BedrockのPrompt Caching（2024年GA、2026年1月に1時間TTL追加）やAnthropicのClaude APIのcache_controlフィールドは、本論文のコンセプトを商用実装した例と位置づけられる。

Zenn記事で解説されているBedrock Converse APIのcachePointブロックは、PMLのモジュール宣言・参照に対応する概念であり、以下の対応関係がある：

| Prompt Cache（論文） | Bedrock Converse API | 役割 |
|---------------------|---------------------|------|
| `<module>` 宣言 | システムプロンプト + ドキュメントコンテキスト配置 | キャッシュ対象の定義 |
| `<use>` 参照 | `cachePoint`ブロック（TTL指定付き） | キャッシュの適用指示 |
| Position-Aware Encoding | API内部で自動処理 | 位置整合性の保証 |

社内RAGでは「同一ドキュメントに対する複数質問」パターンが頻出するため、論文のDocument QAシナリオ（4.8〜8.0倍高速化）が直接適用可能である。

## 関連研究（Related Work）

- **vLLM PagedAttention**（Kwon et al., SOSP 2023）: KVキャッシュのメモリ管理をページング方式で最適化。セッション内のメモリ効率に特化しており、Prompt Cacheのセッション間再利用とは直交する
- **LLMLingua**（Jiang et al., 2023）: プロンプト圧縮による入力トークン削減。プロンプトの意味を変更するため、Prompt Cacheの完全一致出力とはトレードオフが異なる
- **SGLang**（Zheng et al., 2023）: RadixAttentionによるプレフィックスキャッシュ管理。Prompt Cacheと並行開発された研究であり、プレフィックス以外の位置への対応がPrompt Cacheの差別化要因
- **Self-RAG**（Asai et al., 2023）: 検索の要否をモデル自身が判断するRAG手法。不要な検索を削減することでコスト最適化を図る点でPrompt Cacheと補完的

## まとめと今後の展望

Prompt Cacheは、LLMプロンプト内の共通テキストセグメントに対するAttention Statesのセッション横断再利用を実現した推論最適化手法である。Position-Aware Encodingにより出力品質の劣化なく、TTFTを最大8倍短縮したことが報告されている。この設計思想はAmazon Bedrock・Claude APIのPrompt Caching機能として商用実装されており、社内RAGのようなユースケースでのコスト・レイテンシ削減に寄与している。

今後は、キャッシュのメモリ効率改善（KVキャッシュ圧縮との組み合わせ）や、動的なキャッシュTTL管理（Bedrockの5分/1時間切り替えのような）が研究の方向性として考えられる。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2311.04934](https://arxiv.org/abs/2311.04934)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/d027acf4081b9d](https://zenn.dev/0h_n0/articles/d027acf4081b9d)
- **vLLM (PagedAttention)**: [https://arxiv.org/abs/2309.06180](https://arxiv.org/abs/2309.06180)
- **LLMLingua**: [https://arxiv.org/abs/2305.13048](https://arxiv.org/abs/2305.13048)
