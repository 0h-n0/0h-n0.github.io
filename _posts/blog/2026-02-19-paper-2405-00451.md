---
layout: post
title: "論文解説: RouterBench — LLMルーター評価のための標準ベンチマーク"
description: "11データセット・12LLM・405k推論インスタンスでLLMルーティング戦略を体系的に評価するRouterBenchの全貌"
categories: [blog, paper, arxiv]
tags: [LLM, routing, benchmark, evaluation, cost-optimization]
date: 2026-02-19 13:00:00 +0900
source_type: arxiv
arxiv_id: "2405.00451"
source_url: https://arxiv.org/abs/2405.00451
zenn_article: 3e603a1b91e2e0
zenn_url: https://zenn.dev/0h_n0/articles/3e603a1b91e2e0
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

RouterBenchは、LLMルーティング戦略を公平かつ再現可能に評価するための標準ベンチマークです。**11データセット**（HellaSwag, MMLU, GSM8K, HumanEval等）、**12のLLM**（GPT-4, Claude-2, Llama-2-70B, Mixtral等）から生成された**405,000以上の推論インスタンス**を提供し、ルール型・分類器型・モデルベース型のルーティング戦略を横断的に比較評価します。主要な知見として、BERT分類器ベースのルーティングがコスト-品質トレードオフで最も優れ、**GPT-4性能の90%をそのコストの30-50%で達成**できることを実証しました。

この記事は [Zenn記事: LLMルーター実践ガイド：RouteLLM×LiteLLMでAPIコスト60%削減を実現する](https://zenn.dev/0h_n0/articles/3e603a1b91e2e0) の深掘りです。

## 情報源

- **arXiv ID**: 2405.00451
- **URL**: [https://arxiv.org/abs/2405.00451](https://arxiv.org/abs/2405.00451)
- **著者**: Jiarui Yao, Sam Davidson, Yanxia Qin, Jianfeng Gao, Shi Feng, Yansong Feng, Dongyan Zhao, Yue Zhang
- **発表年**: 2024
- **分野**: cs.CL, cs.AI
- **コード**: [https://github.com/withmartian/routerbench](https://github.com/withmartian/routerbench)

## 背景と動機（Background & Motivation）

LLMルーティング研究が急速に進展する中、各手法の評価には以下の課題がありました：

1. **評価条件の不統一**: 論文ごとに異なるデータセット・モデルペアで評価され、手法間の公平な比較が困難
2. **再現性の欠如**: LLM APIの応答は非決定的であり、同一条件での再現実験が難しい
3. **コスト-品質トレードオフの可視化不足**: 単一の指標（精度やコスト削減率）では、パレートフロンティア上の位置関係が見えない

RouterBenchは**事前計算済みの推論結果**を提供することで、全LLMへのクエリなしにルーティング戦略の評価を可能にし、上記の課題を解決します。

## 主要な貢献（Key Contributions）

- **貢献1**: 405k推論インスタンスの大規模データセット — 11ベンチマーク×12LLMの全組み合わせ結果を公開。これにより、ルーター研究者は自前でAPIを叩かずに戦略の比較評価が可能
- **貢献2**: 3カテゴリのルーティング戦略の体系的比較 — ルール型、分類器型、モデルベース型の3カテゴリを横断的に評価し、各カテゴリの強み・弱みを定量化
- **貢献3**: コスト-品質パレートフロンティアの可視化 — 各戦略のコスト-品質カーブを統一的に描画し、実務でのルーター選択を支援
- **貢献4**: 転移学習の評価フレームワーク — あるデータセットで訓練したルーターを別のデータセットに適用した際の性能劣化を定量化

## 技術的詳細（Technical Details）

### ベンチマーク設計

**11データセット**（ドメイン別分類）:

| カテゴリ | データセット | タスク | 評価指標 |
|---|---|---|---|
| 常識推論 | HellaSwag, WinoGrande, PIQA | 文完成/代名詞解決 | Accuracy |
| 知識 | MMLU, TriviaQA | 多肢選択/QA | Accuracy / EM |
| 数学推論 | GSM8K, MATH | 数学問題 | Accuracy |
| 読解 | SQuAD | 読解QA | F1 / EM |
| コード生成 | HumanEval | Python関数生成 | pass@1 |
| 指示追従 | AlpacaEval | 自由形式応答 | Win Rate |
| 対話 | MT-Bench | マルチターン対話 | スコア (1-10) |

**12のLLM**:

| モデル | プロバイダー | コスト帯 |
|---|---|---|
| GPT-4, GPT-4-turbo | OpenAI | 高 |
| GPT-3.5-turbo | OpenAI | 低 |
| Claude-2, Claude-Instant | Anthropic | 高/低 |
| Llama-2-70B, Llama-2-13B | Meta | 中/低 |
| Mistral-7B, Mixtral-8x7B | Mistral AI | 低/中 |
| Gemini-Pro | Google | 中 |
| PaLM-2 | Google | 中 |

### 評価指標

RouterBenchは2つの軸でルーティング戦略を評価します：

**コスト（Cost）**: 高コストモデル（GPT-4等）へのルーティング割合

$$
\text{Cost}(r) = \frac{|\{q \in Q : r(q) = \text{expensive model}\}|}{|Q|}
$$

**品質（Quality）**: ベンチマーク上の精度

$$
\text{Quality}(r) = \frac{1}{|Q|} \sum_{q \in Q} \text{metric}(r(q), \text{ground\_truth}(q))
$$

これらをプロットしたコスト-品質カーブ（パレートフロンティア）で、各戦略の効率を視覚的に比較します。

### 評価されたルーティング戦略

**1. ルール型（Rule-based）**

最もシンプルな手法群です。

- **ランダムルーティング**: コスト比率 $p$ に基づいてランダムにモデルを選択
  $$P(r(q) = \text{strong}) = p$$
- **クエリ長ルーティング**: 入力トークン数が閾値を超えたら強モデルに送信
- **ドメインルーティング**: キーワードマッチングでクエリのドメイン（数学、コード、一般知識等）を判定し、ドメインごとに最適なモデルを静的マッピング

**2. 分類器型（Classifier-based）**

最も有効な手法群です。

- **ロジスティック回帰**: TF-IDF特徴量で二値分類
  $$f_{\text{LR}}(q) = \sigma(\mathbf{w}^\top \phi(q) + b)$$
  ここで $\phi(q)$ はTF-IDF特徴ベクトル

- **BERT分類器**: BERT-baseをファインチューニング
  $$f_{\text{BERT}}(q) = \text{MLP}(\text{BERT}(q))$$
  RouterBench全体で**最高のコスト-品質トレードオフ**を達成

- **LLM分類器**: 小型LLM（例: Llama-2-7B）に分類ヘッドを追加

**3. モデルベース型（Model-based）**

応答品質の推定を利用する手法群です。

- **報酬モデルルーティング**: 報酬モデル（Reward Model）で各モデルの応答品質を推定し、品質が最も高いモデルにルーティング
- **不確実性ベースルーティング**: モデルの不確実性（エントロピー等）が高い場合に強モデルにエスカレーション
- **ハイブリッド型**: 分類器型とモデルベース型の信号を統合

### データ効率性の分析

RouterBenchは、ルーター訓練に必要なラベル数と性能の関係も分析しています。

$$
\text{Quality}(n) = \text{Quality}_{\max} - \alpha \cdot n^{-\beta}
$$

ここで $n$ は訓練サンプル数、$\alpha, \beta > 0$ はデータセット依存のパラメータです。**50-200サンプル**で十分な性能が得られることが多く、大規模なラベル付きデータは不要です。

## 実装のポイント（Implementation）

RouterBenchの使い方：

```python
# RouterBenchデータセットの読み込み
import pandas as pd
from pathlib import Path

# 事前計算済み推論結果を読み込み
bench_dir = Path("routerbench/data/")
results = {}
for dataset in ["mmlu", "gsm8k", "hellaswag", "humaneval"]:
    results[dataset] = pd.read_parquet(bench_dir / f"{dataset}_results.parquet")

# 各データポイントの構造
# columns: [query, model, response, score, cost, latency]

# ルーター評価関数
def evaluate_router(
    router_fn: callable,
    dataset_results: pd.DataFrame,
    models: list[str],
) -> dict[str, float]:
    """ルーティング戦略の評価

    Args:
        router_fn: query -> model_name のルーティング関数
        dataset_results: RouterBenchの推論結果データ
        models: 利用可能なモデルリスト

    Returns:
        {"quality": float, "cost": float} の辞書
    """
    queries = dataset_results["query"].unique()
    total_quality = 0.0
    total_cost = 0.0

    for query in queries:
        selected_model = router_fn(query)
        row = dataset_results[
            (dataset_results["query"] == query)
            & (dataset_results["model"] == selected_model)
        ].iloc[0]
        total_quality += row["score"]
        total_cost += row["cost"]

    return {
        "quality": total_quality / len(queries),
        "cost": total_cost / len(queries),
    }
```

**実装上の注意点**:

1. **データサイズ**: 405kレコードは数GBに達するため、メモリ効率の良いParquet形式で提供
2. **モデルの選択**: 12モデル全てを使う必要はなく、実際に利用するモデルペアに絞って評価可能
3. **カスタムルーターの追加**: `router_fn(query) -> model_name` インターフェースを実装するだけでRouterBenchに統合可能
4. **閾値のスイープ**: コスト-品質カーブを描くには、閾値を0.0から1.0まで0.01刻みでスイープし、各点での品質とコストを記録

## 実験結果（Results）

**主要な知見**:

| ルーティング戦略 | GPT-4性能の達成率 | コスト（GPT-4比） | 備考 |
|---|---|---|---|
| GPT-4のみ（ベースライン） | 100% | 100% | 最高品質、最高コスト |
| ランダム (p=0.5) | 85-90% | 50% | 単純だが安定 |
| クエリ長ルーティング | 87-92% | 45% | ドメイン依存性大 |
| LR + TF-IDF | 90-94% | 40% | シンプルで効果的 |
| **BERT分類器** | **93-96%** | **30-50%** | **最高のトレードオフ** |
| LLM分類器 | 91-95% | 35-50% | BERT以上のコスト |

**ドメイン別の知見**:

- **数学推論（GSM8K, MATH）**: ルーティング効果が大きい。簡単な計算と複雑な推論で必要モデルが明確に分かれる
- **コード生成（HumanEval）**: ルーティング効果が中程度。小型モデルでも簡単な関数は生成可能
- **常識推論（HellaSwag）**: ルーティング効果が小さい。多くのクエリを安価なモデルで処理可能

**転移学習の結果**: MMLUで訓練したBERT分類器をGSM8Kに適用すると、性能が5-10%低下。同一ドメイン内（例: MMLU → TriviaQA）では2-3%の低下にとどまりました。

## 実運用への応用（Practical Applications）

RouterBenchは以下のシナリオで活用できます：

1. **ルーター選定の事前評価**: 自社サービスに近いデータセット（QA→TriviaQA、コード→HumanEval等）でルーター候補を事前比較し、最適な手法を選定
2. **コスト試算**: 自社のクエリ分布がRouterBenchのどのデータセットに近いかを判断し、期待されるコスト削減率を推定
3. **カスタムルーターの開発**: RouterBenchの405kインスタンスをルーター訓練用データとして利用し、自社データ不足を補完
4. **A/Bテスト設計**: RouterBenchで有望と判断されたルーターを本番のA/Bテストに投入し、実測値で検証

**RouteLLMとの組み合わせ**: RouteLLMのルーターモデル（MF、BERT等）をRouterBenchで評価することで、自社タスクに最適なルーターとパラメータを事前に特定できます。これにより、本番環境での試行錯誤コストを大幅に削減できます。

## 関連研究（Related Work）

- **RouteLLM** (Ong et al., 2024): RouterBenchの評価フレームワークで比較対象として使用。選好データベースのルーターの強みと弱みを定量化する際にRouterBenchが有用
- **FrugalGPT** (Chen et al., 2023): RouterBenchのカスケード評価はFrugalGPTの手法を含む。4データセットのみの評価を11データセットに拡張した意義が大きい
- **Hybrid LLM** (Ding et al., 2024): RouterBenchの分類器評価はHybrid LLMの手法を含む。より多くのモデルペアとデータセットでの評価を提供

## まとめと今後の展望

RouterBenchはLLMルーティング研究に不可欠な標準ベンチマークです。405k推論インスタンスにより、APIコストなしでルーティング戦略の比較が可能になりました。

**主要な結論**:
1. 全てのルーティング戦略がランダムルーティングを上回る
2. BERT分類器が最高のコスト-品質トレードオフを達成
3. 50-200サンプルの訓練データで実用的な性能が得られる
4. ドメイン間の転移はモデレートに可能だが、専用ルーターには及ばない

**今後の課題**: 最新モデル（GPT-4o, Claude 3.5 Sonnet等）の追加、マルチモーダルタスクの対応、動的な価格変動を反映したコスト評価が挙げられます。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2405.00451](https://arxiv.org/abs/2405.00451)
- **Code**: [https://github.com/withmartian/routerbench](https://github.com/withmartian/routerbench)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/3e603a1b91e2e0](https://zenn.dev/0h_n0/articles/3e603a1b91e2e0)
