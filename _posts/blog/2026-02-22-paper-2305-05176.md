---
layout: post
title: "論文解説: FrugalGPT — 複数LLMカスケードによるコスト最大98%削減"
description: "FrugalGPTはLLMカスケード戦略により最大98%のAPIコスト削減を実現しつつ、単一LLMを上回る精度を達成する手法を提案"
categories: [blog, paper, arxiv]
tags: [LLM, cost-optimization, prompt-caching, rag, aws, bedrock, anthropic]
date: 2026-02-22 09:00:00 +0900
source_type: arxiv
arxiv_id: "2305.05176"
source_url: https://arxiv.org/abs/2305.05176
zenn_article: d027acf4081b9d
zenn_url: https://zenn.dev/0h_n0/articles/d027acf4081b9d
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

FrugalGPTは、複数のLLM APIを組み合わせることで、APIコストを最大98%削減しつつ、個々のLLM単体を上回る精度を達成する手法を提案している。著者らは、同一タスクにおいて異なるLLM間でコストが最大300倍異なること、かつクエリ難易度に応じて最適なLLMが異なることを示し、これを活用した「LLMカスケード」アーキテクチャを構築している。

この記事は [Zenn記事: Bedrock AgentCore×1時間キャッシュで社内RAGコスト90%削減](https://zenn.dev/0h_n0/articles/d027acf4081b9d) の深掘りです。

## 情報源

- **arXiv ID**: 2305.05176
- **URL**: [https://arxiv.org/abs/2305.05176](https://arxiv.org/abs/2305.05176)
- **著者**: Lingjiao Chen, Matei Zaharia, James Zou（Stanford University）
- **発表年**: 2023
- **分野**: cs.LG, cs.AI, cs.CL

## 背景と動機（Background & Motivation）

2023年時点で、GPT-4のAPIコストはリクエストあたり約0.03ドル、1日100万クエリを処理すると年間約3万ドルに達する。一方、同じタスクを処理するLLMの中でも、最も安価なモデルと最も高価なモデルの間には最大300倍のコスト差が存在する。

従来のアプローチでは、精度が最も高い単一のLLMを全クエリに対して使用するのが一般的であった。しかし、すべてのクエリが同じ難易度ではない。簡単なクエリに高価なGPT-4を使うのはコストの無駄であり、安価なモデルで十分な品質が得られるケースが多数存在する。FrugalGPTはこの非効率性に着目し、クエリ難易度に応じた動的なモデル選択を実現する。

## 主要な貢献（Key Contributions）

- **貢献1**: LLMコスト削減のための3つの戦略（Prompt Adaptation、LLM Approximation、LLM Cascade）を体系的に整理
- **貢献2**: LLMカスケード — 安価なモデルから順に試行し、品質が十分であれば停止するルーティング機構を提案
- **貢献3**: 実験により、GPT-4単体比で約5%のコストで同等精度を達成できることを実証

## 技術的詳細（Technical Details）

### コスト削減の3戦略

FrugalGPTは、LLM APIのコスト削減アプローチを以下の3カテゴリに整理している。

**1. Prompt Adaptation（プロンプト適応）**: Few-shot例の削減、プロンプト圧縮、応答キャッシュ。Zenn記事で紹介しているBedrock Prompt Cachingはこのカテゴリに該当する。

**2. LLM Approximation（LLM近似）**: 大規模LLMの出力を教師データとして小型モデルをFine-tuningする。Knowledge Distillationの応用と位置づけられる。

**3. LLM Cascade（LLMカスケード）**: 本論文の核心的貢献。複数LLMをコスト順にチェーンし、各クエリに対して最低コストで十分な品質を得られるモデルを自動選択する。

### LLMカスケードのアーキテクチャ

カスケードの処理フローは以下の通りである。

```
Input Query q
    ↓
[LLM_1 (最安価)] → Generation Scoring Function g(q, a₁)
    ↓ g(q, a₁) < θ₁ の場合
[LLM_2 (中価格)] → Generation Scoring Function g(q, a₂)
    ↓ g(q, a₂) < θ₂ の場合
[LLM_3 (最高価)] → 最終回答として a₃ を返す
```

### Generation Scoring Function

カスケードの中心となるのが、Generation Scoring Function $g(q, a) \in [0, 1]$ である。

$$
g(q, a) = \sigma(\mathbf{w}^T \phi(q, a) + b)
$$

ここで、
- $q$: 入力クエリ
- $a$: LLMの生成した回答
- $\phi(q, a)$: 特徴量抽出関数（トークン数、perplexity等）
- $\mathbf{w}$: 学習された重みベクトル
- $b$: バイアス項
- $\sigma$: シグモイド関数

スコア $g(q, a_i)$ が閾値 $\theta_i$ を超えた場合、「十分な品質」と判定してカスケードを停止し、回答 $a_i$ を返す。最後のLLMの回答は無条件で採用される。

### 最適化問題

FrugalGPTは以下の最適化問題を解く。

$$
\min_{\pi} \mathbb{E}_{q \sim \mathcal{D}} [C(\pi(q))] \quad \text{s.t.} \quad \mathbb{E}_{q \sim \mathcal{D}} [Q(\pi(q))] \geq \tau
$$

ここで、
- $\pi$: ルーティングポリシー（どのLLMサブセットを使い、各閾値をどう設定するか）
- $C(\pi(q))$: クエリ $q$ に対するコスト
- $Q(\pi(q))$: クエリ $q$ に対する品質
- $\tau$: 品質の下限閾値
- $\mathcal{D}$: クエリの分布

### アルゴリズム（擬似コード）

```python
from dataclasses import dataclass
from typing import Protocol

@dataclass
class LLMConfig:
    """LLMの設定"""
    name: str
    cost_per_token: float
    threshold: float

class ScoringFunction(Protocol):
    """回答品質スコアリング"""
    def score(self, query: str, answer: str) -> float: ...

def frugal_cascade(
    query: str,
    llms: list[LLMConfig],
    scorer: ScoringFunction,
) -> tuple[str, float]:
    """FrugalGPT LLMカスケード

    Args:
        query: 入力クエリ
        llms: コスト昇順にソートされたLLMリスト
        scorer: Generation Scoring Function

    Returns:
        (回答, 合計コスト)
    """
    total_cost = 0.0
    for i, llm in enumerate(llms):
        answer = call_llm_api(llm.name, query)
        total_cost += compute_cost(llm, query, answer)

        # 最後のLLMは無条件で採用
        if i == len(llms) - 1:
            return answer, total_cost

        score = scorer.score(query, answer)
        if score >= llm.threshold:
            return answer, total_cost

    raise RuntimeError("Unreachable")
```

## 実装のポイント（Implementation）

FrugalGPTを実運用に適用する際の注意点は以下の通りである。

**Scoring Functionの学習**: ロジスティック回帰や軽量なMLモデルで十分であると著者らは報告している。学習には数百〜数千のラベル付きサンプルが必要で、完全な教師なし設定では適用が困難である。

**閾値チューニング**: 各LLMに対する閾値 $\theta_i$ は開発セットで最適化する。閾値を高く設定するとコスト削減効果が下がり、低く設定すると品質が低下するトレードオフが存在する。

**API出力の正規化**: 複数のLLM APIは異なるJSON形式で応答を返すため、統一的なインターフェースへの正規化処理が必要である。

**レイテンシの考慮**: カスケードは逐次実行のため、最悪ケースのレイテンシは全LLMのレイテンシ合計となる。リアルタイム性が求められるシステムでは、カスケード段数の制限が必要である。

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

FrugalGPTのLLMカスケードをAWS上で構築する場合の推奨構成を示す。コスト試算は2026年2月時点のap-northeast-1リージョン概算値であり、実際のコストはトラフィックパターンにより変動する。

| 構成 | トラフィック | 月額コスト目安 | 主要サービス |
|------|------------|-------------|------------|
| Small | ~100 req/日 | $50-150 | Lambda + Bedrock (Haiku → Sonnet) |
| Medium | ~1,000 req/日 | $300-800 | ECS Fargate + Bedrock + DynamoDB |
| Large | 10,000+ req/日 | $2,000-5,000 | EKS + Spot + Bedrock Batch API |

**コスト削減テクニック**:
- Bedrock Prompt Caching有効化で入力トークンコスト30-90%削減
- カスケード戦略により高価モデルへの到達率を制御（論文では全クエリの80%以上が最安モデルで完結）
- Batch APIの活用でリアルタイム性が不要なタスクのコストを50%削減
- Spot Instancesの活用でEKSノードコストを最大90%削減

### Terraformインフラコード

**Small構成（Serverless）**:

```hcl
# FrugalGPT Cascade - Lambda + Bedrock
resource "aws_lambda_function" "frugal_cascade" {
  function_name = "frugal-cascade"
  runtime       = "python3.12"
  handler       = "handler.lambda_handler"
  memory_size   = 512
  timeout       = 60

  environment {
    variables = {
      CASCADE_CONFIG    = "haiku,sonnet,opus"
      SCORING_MODEL_ARN = aws_sagemaker_endpoint.scorer.arn
      DYNAMODB_TABLE    = aws_dynamodb_table.cache.name
    }
  }

  role = aws_iam_role.lambda_exec.arn
}

resource "aws_iam_role" "lambda_exec" {
  name = "frugal-cascade-lambda"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy" "bedrock_access" {
  name = "bedrock-invoke"
  role = aws_iam_role.lambda_exec.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect   = "Allow"
      Action   = ["bedrock:InvokeModel"]
      Resource = ["arn:aws:bedrock:ap-northeast-1::foundation-model/*"]
    }]
  })
}

# 応答キャッシュ用DynamoDB
resource "aws_dynamodb_table" "cache" {
  name         = "frugal-response-cache"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "query_hash"

  attribute {
    name = "query_hash"
    type = "S"
  }

  ttl {
    attribute_name = "expires_at"
    enabled        = true
  }
}
```

**Large構成（Container）**:

```hcl
# EKS + Karpenter（Spot優先）
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  version         = "~> 20.0"
  cluster_name    = "frugal-cascade"
  cluster_version = "1.31"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets
}

resource "kubectl_manifest" "karpenter_provisioner" {
  yaml_body = yamlencode({
    apiVersion = "karpenter.sh/v1"
    kind       = "NodePool"
    metadata   = { name = "frugal-default" }
    spec = {
      template = {
        spec = {
          requirements = [
            { key = "karpenter.sh/capacity-type", operator = "In", values = ["spot", "on-demand"] },
            { key = "node.kubernetes.io/instance-type", operator = "In",
              values = ["m6i.large", "m6i.xlarge", "m7i.large"] }
          ]
        }
      }
      limits   = { cpu = "100" }
      disruption = { consolidationPolicy = "WhenEmptyOrUnderutilized" }
    }
  })
}

# コスト監視アラート
resource "aws_budgets_budget" "monthly" {
  name         = "frugal-cascade-monthly"
  budget_type  = "COST"
  limit_amount = "5000"
  limit_unit   = "USD"
  time_unit    = "MONTHLY"

  notification {
    comparison_operator       = "GREATER_THAN"
    threshold                 = 80
    threshold_type            = "PERCENTAGE"
    notification_type         = "ACTUAL"
    subscriber_email_addresses = ["ops@example.com"]
  }
}
```

### 運用・監視設定

**CloudWatch Logs Insights — カスケード段階別コスト分析**:

```
fields @timestamp, cascade_stage, model_name, cost_usd, latency_ms
| stats avg(cost_usd) as avg_cost,
        count() as request_count,
        avg(latency_ms) as avg_latency
  by cascade_stage
| sort cascade_stage asc
```

**カスケード停止率モニタリング（Python）**:

```python
import boto3

cloudwatch = boto3.client("cloudwatch")

cloudwatch.put_metric_alarm(
    AlarmName="FrugalCascade-HighEscalationRate",
    MetricName="CascadeEscalationRate",
    Namespace="FrugalGPT",
    Statistic="Average",
    Period=3600,
    EvaluationPeriods=3,
    Threshold=0.5,
    ComparisonOperator="GreaterThanThreshold",
    AlarmActions=["arn:aws:sns:ap-northeast-1:123456789:ops-alerts"],
    AlarmDescription="50%以上のクエリが高価モデルにエスカレーション",
)
```

### コスト最適化チェックリスト

**アーキテクチャ選択**:
- [ ] トラフィック量に応じた構成（Serverless / Hybrid / Container）を選定
- [ ] カスケード段数を2-3段に制限（レイテンシとのトレードオフ）

**リソース最適化**:
- [ ] EC2/EKS: Spot Instances優先（最大90%削減）
- [ ] Reserved Instances: 1年コミットで最大72%削減
- [ ] Savings Plans検討
- [ ] Lambda: メモリサイズ最適化（512MB推奨）
- [ ] ECS/EKS: アイドル時スケールダウン（Karpenter活用）

**LLMコスト削減**:
- [ ] カスケード順序: Haiku → Sonnet → Opus（コスト昇順）
- [ ] Bedrock Prompt Caching有効化（30-90%削減）
- [ ] Batch API使用（非リアルタイム処理で50%削減）
- [ ] 応答キャッシュ（DynamoDB TTL付き）で重複クエリ削減
- [ ] トークン数制限（max_tokens設定）

**監視・アラート**:
- [ ] AWS Budgets設定（月額上限アラート）
- [ ] CloudWatch カスケードエスカレーション率モニタリング
- [ ] Cost Anomaly Detection有効化
- [ ] 日次コストレポート（Cost Explorer API）

**リソース管理**:
- [ ] 未使用リソース定期削除
- [ ] タグ戦略（CostCenter, Environment, Service）
- [ ] DynamoDBキャッシュのTTL設定
- [ ] 開発環境夜間停止（EventBridge Scheduler）
- [ ] CloudWatch Logsの保持期間設定（30日）

## 実験結果（Results）

### ベンチマーク結果

著者らは複数のNLPベンチマークで評価を実施している（論文Table 1-3より）。

| ベンチマーク | タスク種別 | GPT-4単体精度 | FrugalGPT精度 | コスト削減率 |
|------------|----------|-------------|-------------|-----------|
| HellaSwag | 常識推論 | 基準 | 同等以上 | 約95% |
| LAMBADA | 言語モデリング | 基準 | 同等以上 | 約90% |
| OpenBookQA | 知識問題 | 基準 | 同等以上 | 約85% |
| TruthfulQA | 事実性評価 | 基準 | 同等 | 約92% |

著者らは、HellaSwagベンチマークにおいてGPT-4単体使用のコストの約5%で同等の精度を達成したと報告している。

### カスケード停止分布

論文の実験では、クエリの約80%以上が最安価なモデル（LLM_1）で品質閾値をクリアしており、高価なモデルへのエスカレーションは限定的であった。この分布がコスト削減の主要因である。

## 実運用への応用（Practical Applications）

FrugalGPTのカスケード戦略は、Zenn記事で紹介しているBedrock AgentCoreの構成と組み合わせることで効果を発揮する。

**RAGシステムへの適用**: 社内文書QAにおいて、簡単な質問（FAQ的な内容）はClaude Haikuで処理し、複雑な分析が必要な質問のみClaude Sonnetにエスカレーションする構成が考えられる。Zenn記事のPrompt Caching（1時間TTL）と併用すれば、カスケードの各段階でキャッシュ読み取り料金の適用を受けられ、コスト削減効果が乗算的に作用する。

**Bedrock Intelligent Prompt Routingとの関係**: AWSが提供するBedrock Intelligent Prompt Routingは、FrugalGPTのLLMカスケードと類似のコンセプトをマネージドサービスとして提供している。FrugalGPTの論文は、この種のルーティング最適化の理論的基盤を提供するものとして位置づけられる。

## 関連研究（Related Work）

- **LLMLingua (Jiang et al., 2023)**: プロンプト圧縮によるコスト削減。FrugalGPTの「Prompt Adaptation」カテゴリに該当する手法
- **Prompt Cache (Gim et al., 2024, arXiv 2311.04934)**: KVキャッシュの再利用による計算コスト削減。FrugalGPTのキャッシュ戦略と補完的
- **Bedrock Intelligent Prompt Routing (AWS, 2024)**: FrugalGPTのカスケード概念をマネージドサービスとして実装した事例

## まとめと今後の展望

FrugalGPTは、複数LLMの動的選択というシンプルなアイデアで最大98%のコスト削減を実現する手法である。著者らは、品質評価関数（Scoring Function）の学習に少量のラベル付きデータが必要である点を限界として認めている。今後は、LLM-as-Judgeベースの教師なしScoring Functionの研究や、ローカルLLM（LLaMA等）をカスケードの安価側に組み込む拡張が期待される。実務においては、Bedrock Prompt CachingやIntelligent Prompt Routingとの組み合わせにより、さらなるコスト最適化が可能である。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2305.05176](https://arxiv.org/abs/2305.05176)
- **Code**: [https://github.com/stanford-futuredata/FrugalGPT](https://github.com/stanford-futuredata/FrugalGPT)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/d027acf4081b9d](https://zenn.dev/0h_n0/articles/d027acf4081b9d)
