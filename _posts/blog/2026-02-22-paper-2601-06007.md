---
layout: post
title: "論文解説: Don't Break the Cache — プロンプトキャッシュのエージェントタスクにおける体系的評価"
description: "OpenAI・Anthropic・Googleの3プロバイダでプロンプトキャッシュの効果を500+セッションで評価し、コスト41-80%削減・TTFT13-31%改善を報告"
categories: [blog, paper, arxiv]
tags: [prompt-caching, LLM, agent, inference-optimization, claudesonnet, codereview, python]
date: 2026-02-22 23:20:00 +0900
source_type: arxiv
arxiv_id: "2601.06007"
source_url: https://arxiv.org/abs/2601.06007
zenn_article: a41a3cb117cc46
zenn_url: https://zenn.dev/0h_n0/articles/a41a3cb117cc46
math: true
mermaid: true
target_audience: "修士学生レベル"
---

本記事は [Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks (arXiv:2601.06007)](https://arxiv.org/abs/2601.06007) の解説記事です。

## 論文概要（Abstract）

本論文は、プロンプトキャッシュ（Prompt Caching）がマルチターンエージェントタスクにおいてAPIコストと応答レイテンシに与える影響を体系的に評価した初の研究である。著者らは、OpenAI、Anthropic、Googleの3大LLMプロバイダを対象に、500以上のエージェントセッションで3つのキャッシュ戦略を比較評価し、コスト41-80%削減・Time to First Token（TTFT）13-31%改善を報告している。さらに、ナイーブなフルコンテキストキャッシュが逆効果となるケースを特定し、プロバイダごとの最適戦略を提案している。

この記事は [Zenn記事: Claude Sonnet 4.6の1Mコンテキストで大規模コードレビューエージェントを構築する](https://zenn.dev/0h_n0/articles/a41a3cb117cc46) の深掘りです。

## 情報源

- **arXiv ID**: 2601.06007
- **URL**: [https://arxiv.org/abs/2601.06007](https://arxiv.org/abs/2601.06007)
- **著者**: Lumer, Nizar, Jangiti, Frank, Gulati, Phadate, Subbiah
- **発表年**: 2026
- **分野**: cs.AI, cs.SE

## 背景と動機（Background & Motivation）

LLMをエージェントとして使用する場合、マルチターンの対話が不可避となる。各ターンでシステムプロンプト、会話履歴、ツール呼び出し結果などを含むプロンプト全体をAPIに送信するため、ターン数の増加に伴いAPIコストとレイテンシが線形に増大する。

プロンプトキャッシュは、この問題に対するプロバイダ側のソリューションである。プロンプトの共通プレフィックス部分のKVテンソルをサーバー側でキャッシュし、後続リクエストで再利用することで、入力トークンの再計算を回避する。しかし、著者らが指摘するように、キャッシュの実装はプロバイダごとに大きく異なり、「どの戦略がどの条件で効果的か」の体系的な評価が存在しなかった。

Zenn記事で紹介されているコードレビューエージェントは、500Kトークンのリポジトリをキャッシュして繰り返しレビューを実行するユースケースであり、本論文の知見が直接的に適用できる。

## 主要な貢献（Key Contributions）

- **貢献1**: マルチターンエージェントタスクにおけるプロンプトキャッシュの効果を3プロバイダ（OpenAI、Anthropic、Google）で比較した初の体系的評価
- **貢献2**: 3つのキャッシュ戦略（フルコンテキスト、システムプロンプトのみ、動的ツール結果除外）の比較分析と、プロバイダごとの最適戦略の特定
- **貢献3**: ナイーブなフルコンテキストキャッシュが逆にレイテンシを増加させるケースの発見と、その原因分析

## 技術的詳細（Technical Details）

### プロンプトキャッシュの仕組み

プロンプトキャッシュの基本原理は、LLMのprefill（入力トークンのKVテンソル計算）フェーズの結果をサーバー側で保持し、共通プレフィックスを持つ後続リクエストで再利用することである。

$$
\text{Cost}_{\text{cached}} = \text{Cost}_{\text{write}} \cdot N_{\text{new}} + \text{Cost}_{\text{read}} \cdot N_{\text{cached}} + \text{Cost}_{\text{output}} \cdot N_{\text{output}}
$$

ここで、
- $N_{\text{new}}$: 新規（キャッシュ未ヒット）の入力トークン数
- $N_{\text{cached}}$: キャッシュから読み取られた入力トークン数
- $\text{Cost}_{\text{write}}$: キャッシュ書き込み時のトークン単価（通常入力の1.0-1.25倍）
- $\text{Cost}_{\text{read}}$: キャッシュ読み取り時のトークン単価（通常入力の0.1倍）
- $\text{Cost}_{\text{output}}$: 出力トークン単価

### 3つのキャッシュ戦略

著者らが評価した3つの戦略を以下に示す。

**戦略1: フルコンテキストキャッシュ**

プロンプト全体（システムプロンプト、会話履歴、ツール結果すべて）をキャッシュ対象とする。最も単純なアプローチだが、動的コンテンツがキャッシュの無効化を引き起こしやすい。

**戦略2: システムプロンプトのみキャッシュ**

システムプロンプト部分のみをキャッシュし、会話履歴やツール結果はキャッシュ対象外とする。安定したキャッシュヒット率が得られるが、キャッシュ範囲が限定的。

**戦略3: 動的ツール結果除外キャッシュ**

システムプロンプトと会話履歴をキャッシュし、動的なツール呼び出し結果のみを除外する。戦略1と2の中間的アプローチ。

### 評価ベンチマーク

著者らはDeepResearch Benchを使用している。これはマルチターンのエージェントベンチマークであり、エージェントが自律的にWeb検索ツールを呼び出して複雑な調査質問に回答する。プロンプトサイズ500-50,000トークン、ツール呼び出し回数3-50の範囲でアブレーション実験を実施している。

### アルゴリズム

```python
from dataclasses import dataclass

@dataclass
class CacheStrategy:
    """プロンプトキャッシュ戦略の設定"""
    cache_system_prompt: bool = True
    cache_conversation_history: bool = False
    exclude_tool_results: bool = True


def apply_cache_strategy(
    system_prompt: str,
    messages: list[dict],
    strategy: CacheStrategy,
) -> list[dict]:
    """キャッシュ戦略に基づいてcache_controlマーカーを付与する

    Args:
        system_prompt: システムプロンプト
        messages: 会話メッセージリスト
        strategy: 使用するキャッシュ戦略

    Returns:
        cache_controlが付与されたメッセージリスト
    """
    result = []

    # システムプロンプトのキャッシュ
    system_block = {"type": "text", "text": system_prompt}
    if strategy.cache_system_prompt:
        system_block["cache_control"] = {"type": "ephemeral"}
    result.append(system_block)

    # 会話履歴のキャッシュ
    for msg in messages:
        block = dict(msg)
        if strategy.cache_conversation_history:
            if strategy.exclude_tool_results and msg.get("role") == "tool":
                pass  # ツール結果はキャッシュしない
            else:
                block["cache_control"] = {"type": "ephemeral"}
        result.append(block)

    return result
```

## 実装のポイント（Implementation）

### 戦略選択の指針

著者らの実験結果に基づく戦略選択の指針を以下に示す。

1. **Anthropic APIの場合**: 動的コンテンツをシステムプロンプトの末尾に配置し、戦略3（動的ツール結果除外）を使用する。Anthropicのキャッシュはプレフィックスマッチングに基づくため、プロンプト先頭部分の安定性が重要
2. **OpenAI APIの場合**: フルコンテキストキャッシュ（戦略1）が比較的効果的。OpenAIのキャッシュは自動的にプレフィックスを検出するため、明示的なキャッシュ制御が不要
3. **Google APIの場合**: システムプロンプトのみキャッシュ（戦略2）が安定。Context Cachingの最小トークン数要件に注意

### プロバイダ間の差異への対応

```python
from typing import Literal

Provider = Literal["anthropic", "openai", "google"]

def get_optimal_strategy(
    provider: Provider,
    system_prompt_tokens: int,
    avg_tool_calls_per_turn: int,
) -> CacheStrategy:
    """プロバイダとワークロード特性に基づく最適戦略の選択

    著者らの実験結果（論文Table 3）に基づく判定ロジック

    Args:
        provider: LLMプロバイダ
        system_prompt_tokens: システムプロンプトのトークン数
        avg_tool_calls_per_turn: ターンあたりの平均ツール呼び出し回数

    Returns:
        推奨キャッシュ戦略
    """
    if provider == "anthropic":
        # Anthropic: 動的コンテンツ除外が効果的
        return CacheStrategy(
            cache_system_prompt=True,
            cache_conversation_history=True,
            exclude_tool_results=True,
        )
    elif provider == "openai":
        # OpenAI: フルキャッシュが効果的（自動プレフィックス検出）
        return CacheStrategy(
            cache_system_prompt=True,
            cache_conversation_history=True,
            exclude_tool_results=False,
        )
    else:
        # Google: システムプロンプトのみが安定
        return CacheStrategy(
            cache_system_prompt=True,
            cache_conversation_history=False,
            exclude_tool_results=True,
        )
```

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

プロンプトキャッシュを活用したエージェントシステムのAWS構成を示す。

| 規模 | 月間リクエスト | 推奨構成 | 月額コスト | 主要サービス |
|------|--------------|---------|-----------|------------|
| **Small** | ~3,000 (100/日) | Serverless | $50-150 | Lambda + Bedrock + DynamoDB |
| **Medium** | ~30,000 (1,000/日) | Hybrid | $300-800 | Lambda + ECS Fargate + ElastiCache |
| **Large** | 300,000+ (10,000/日) | Container | $2,000-5,000 | EKS + Karpenter + EC2 Spot |

**コスト試算の注意事項**: 上記は2026年2月時点のAWS ap-northeast-1料金に基づく概算値です。最新料金は [AWS料金計算ツール](https://calculator.aws/) で確認してください。

**コスト削減テクニック**:
- Prompt Caching有効化で入力コスト30-90%削減（本論文の知見に基づく）
- Bedrock Batch API使用で50%割引
- Spot Instances使用で最大90%削減

### Terraformインフラコード

```hcl
resource "aws_lambda_function" "agent_handler" {
  filename      = "lambda.zip"
  function_name = "agent-cache-handler"
  role          = aws_iam_role.lambda_bedrock.arn
  handler       = "index.handler"
  runtime       = "python3.12"
  timeout       = 120
  memory_size   = 1024

  environment {
    variables = {
      BEDROCK_MODEL_ID     = "anthropic.claude-3-5-haiku-20241022-v1:0"
      CACHE_STRATEGY       = "exclude_tool_results"
      DYNAMODB_TABLE       = aws_dynamodb_table.session_cache.name
      ENABLE_PROMPT_CACHE  = "true"
    }
  }
}

resource "aws_dynamodb_table" "session_cache" {
  name         = "agent-session-cache"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "session_id"
  range_key    = "turn_number"

  attribute {
    name = "session_id"
    type = "S"
  }
  attribute {
    name = "turn_number"
    type = "N"
  }

  ttl {
    attribute_name = "expire_at"
    enabled        = true
  }
}
```

### コスト最適化チェックリスト

- [ ] Prompt Caching有効化で30-90%削減
- [ ] キャッシュ戦略をプロバイダに合わせて選択（本論文の知見）
- [ ] 動的コンテンツをプロンプト末尾に配置
- [ ] Bedrock Batch API使用で50%割引
- [ ] Spot Instances優先（最大90%削減）
- [ ] Lambda: メモリサイズ最適化
- [ ] AWS Budgets: 月額予算設定
- [ ] CloudWatch: トークン使用量スパイク検知
- [ ] Cost Anomaly Detection: 自動異常検知
- [ ] 日次コストレポート自動送信
- [ ] DynamoDB TTL設定でセッション自動削除
- [ ] ライフサイクルポリシー: S3キャッシュ30日自動削除

## 実験結果（Results）

著者らが報告している主要な実験結果を以下に整理する（論文Table 1, Table 2より）。

### コスト削減効果

| プロバイダ | 最適戦略 | コスト削減率 | TTFT改善率 |
|-----------|---------|------------|-----------|
| OpenAI | フルコンテキスト | 41% | 13% |
| Anthropic | 動的ツール結果除外 | 80% | 31% |
| Google | システムプロンプトのみ | 55% | 18% |

Anthropicが最も高いコスト削減率を示しているのは、明示的なキャッシュ制御（`cache_control`パラメータ）により、キャッシュ対象を精密に指定できるためである。

### ナイーブキャッシュの逆効果

著者らの重要な発見は、ナイーブなフルコンテキストキャッシュが逆にレイテンシを増加させるケースがあることである。具体的には、ツール呼び出し結果のような動的コンテンツがプロンプト中間に挿入されると、キャッシュのプレフィックスマッチが失敗し、キャッシュ書き込みのオーバーヘッドのみが発生する。このオーバーヘッドは通常入力の1.25倍のコストとなるため、キャッシュが効かない場合は非キャッシュよりも高コストになる。

### アブレーション実験

プロンプトサイズ500-50,000トークン、ツール呼び出し回数3-50の範囲でのアブレーション実験により、以下の知見が得られている。

- コスト削減効果はプロンプトサイズに対して線形に増加（プロバイダのキャッシュ最小トークン数を超えた後）
- ツール呼び出し回数が多いほど、動的コンテンツ除外戦略の優位性が顕著になる
- プロバイダ間で最適戦略が異なるため、マルチプロバイダ環境ではプロバイダごとの戦略切り替えが必要

## 実運用への応用（Practical Applications）

本論文の知見は、Zenn記事のコードレビューエージェントに直接適用できる。

**リポジトリコンテンツのキャッシュ戦略**:
- Zenn記事の設計では、リポジトリ全体（500Kトークン）をシステムプロンプトにキャッシュし、レビュー指示を差し替えて複数回呼び出している。本論文の結果は、この設計が最もコスト効率が高いことを裏付ける
- Anthropic APIの場合、リポジトリコンテンツを`cache_control: ephemeral`で明示的にキャッシュすることで、2回目以降のレビューコストを90%削減できる

**キャッシュ戦略の改善ポイント**:
- ツール呼び出し結果（テスト実行結果、lint出力等）はキャッシュ対象から除外すべき
- レビュー観点の切り替え（セキュリティ→パフォーマンス→設計）を行う場合、レビュー指示はプロンプト末尾に配置する
- キャッシュTTLの管理：Anthropicの5分TTLを考慮し、バッチ的に連続実行する

## 関連研究（Related Work）

- **LMCache** (arXiv:2510.09665): エンタープライズ向けKVキャッシュレイヤー。プロバイダ側ではなくインフラ側でのキャッシュ最適化アプローチ
- **SemanticALLI** (arXiv:2601.16286): 推論結果そのものをキャッシュするセマンティックキャッシュ。プロンプトキャッシュとは異なるレイヤーでの最適化
- **RocketKV** (arXiv:2502.14051): 2段階KVキャッシュ圧縮手法。推論効率化の別アプローチ

## まとめと今後の展望

本論文は、プロンプトキャッシュがマルチターンエージェントタスクで41-80%のコスト削減と13-31%のTTFT改善をもたらすことを定量的に示した。重要な実務上の教訓は、ナイーブなフルコンテキストキャッシュではなく、プロバイダの実装特性に合わせた戦略的なキャッシュ制御が必要であるという点である。

コードレビューエージェントのような長文コンテキスト×マルチターンのユースケースでは、システムプロンプト部分（リポジトリコンテンツ）を明示的にキャッシュし、動的コンテンツ（レビュー指示、前回の指摘事項）をプロンプト末尾に配置する設計が、本論文の知見に基づく最適解である。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2601.06007](https://arxiv.org/abs/2601.06007)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/a41a3cb117cc46](https://zenn.dev/0h_n0/articles/a41a3cb117cc46)
