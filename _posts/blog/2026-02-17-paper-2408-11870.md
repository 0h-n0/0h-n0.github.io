---
layout: post
title: "論文解説: AutoGen - マルチエージェント会話による次世代LLMアプリケーション基盤"
description: "Microsoftが提案するAutoGenフレームワークの技術詳細を解説。カスタマイズ可能な会話エージェントによるマルチエージェントオーケストレーションの設計原理と実装を深掘りする"
categories: [blog, paper, arxiv]
tags: [multi-agent, orchestration, LLM, conversation, claude, ai, agent, productivity]
date: 2026-02-17 09:00:00 +0900
source_type: arxiv
arxiv_id: "2308.08155"
source_url: https://arxiv.org/abs/2308.08155
zenn_article: c01f4e292ff1a7
zenn_url: https://zenn.dev/0h_n0/articles/c01f4e292ff1a7
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

AutoGenはMicrosoftが提案するオープンソースフレームワークであり、複数のLLMエージェントが相互に会話することでタスクを遂行するマルチエージェントアプリケーションの構築基盤である。各エージェントはカスタマイズ可能かつ会話可能（Conversable）であり、LLM・人間の入力・ツール実行を柔軟に組み合わせた動作モードを持つ。数学、コーディング、質疑応答、オペレーションズリサーチなど多様なドメインでの有効性が実証されている。

この記事は [Zenn記事: Claude Octopus: 複数AIを並列実行するオーケストレーションプラグイン](https://zenn.dev/0h_n0/articles/c01f4e292ff1a7) の深掘りです。

## 情報源

- **arXiv ID**: 2308.08155
- **URL**: https://arxiv.org/abs/2308.08155
- **著者**: Qingyun Wu, Gagan Bansal, Chi Wang et al.（Microsoft Research）
- **発表年**: 2023
- **分野**: cs.AI, cs.MA

## 背景と動機（Background & Motivation）

大規模言語モデル（LLM）の能力が向上する中、単一エージェントの限界が顕在化している。複雑なタスクでは、コード生成・実行・検証・修正といった複数の役割を1つのエージェントが担うことが困難であり、専門化されたエージェント群の協調が求められる。

従来のマルチエージェントフレームワークはエージェント間の通信プロトコルが固定的であり、タスクに応じた柔軟な会話パターンの定義が困難だった。また、人間がループに入る（Human-in-the-Loop）仕組みが後付けとなり、実運用での品質保証に課題があった。AutoGenはこれらの問題を「会話可能エージェント」という統一的な抽象で解決する。

## 主要な貢献（Key Contributions）

- **ConversableAgent抽象**: LLM・人間・ツールを統一的に扱うエージェント基底クラスの設計。`generate_reply`メソッドの登録チェーンにより、応答生成のカスタマイズが可能
- **プログラマブルな会話パターン**: 自然言語とコードの両方で会話フローを定義できる。二者間チャット、逐次チャット、グループチャット、ネストチャットの4パターンを標準提供
- **多様なドメインでの実証**: 数学問題解決（MATH）、コード生成、質疑応答、意思決定タスクなど6領域で有効性を実証

## 技術的詳細（Technical Details）

### エージェントアーキテクチャ

AutoGenのコアはConversableAgentクラスである。各エージェントは受信メッセージに対して、登録された応答関数チェーンを順に評価し、最初に有効な応答を返す。

エージェントの応答生成は以下の式で形式化できる：

$$
r = f_{\text{reply}}(m, h, C) = \begin{cases}
f_{\text{LLM}}(m, h) & \text{if LLM応答が有効} \\
f_{\text{tool}}(m) & \text{if ツール呼び出しが検出} \\
f_{\text{human}}(m) & \text{if 人間入力が要求}
\end{cases}
$$

ここで、
- $m$: 受信メッセージ
- $h$: 会話履歴
- $C$: エージェントの設定（LLMコンフィグ、ツール定義等）
- $f_{\text{LLM}}$: LLMベースの応答生成関数
- $f_{\text{tool}}$: ツール実行関数
- $f_{\text{human}}$: 人間入力取得関数

### 会話パターン

AutoGenは4つの標準会話パターンを提供する：

**1. Two-Agent Chat**: 最も基本的なパターン。2つのエージェントが交互にメッセージを送受信する。

**2. Sequential Chat**: 複数の二者間チャットを直列に実行し、前段の結果を後段に引き継ぐ。パイプライン的なワークフローに適する。

**3. Group Chat**: 3つ以上のエージェントが参加するグループ会話。発言順序の制御にはラウンドロビン、ランダム、LLMベースのセレクタの3方式がある。

**4. Nested Chat**: 外部会話の中で内部会話を起動する入れ子構造。複雑なサブタスクの分割統治に有効である。

```python
from autogen import ConversableAgent, GroupChat, GroupChatManager

def create_orchestration_pipeline(
    llm_config: dict[str, str],
    task_description: str,
) -> GroupChatManager:
    """マルチエージェントオーケストレーションパイプラインを構築する

    Args:
        llm_config: LLMの設定（モデル名、APIキー等）
        task_description: 実行するタスクの説明

    Returns:
        グループチャットマネージャー
    """
    planner = ConversableAgent(
        name="Planner",
        system_message="タスクを分解し、実行計画を策定する",
        llm_config=llm_config,
    )

    coder = ConversableAgent(
        name="Coder",
        system_message="計画に基づきPythonコードを生成する",
        llm_config=llm_config,
    )

    reviewer = ConversableAgent(
        name="Reviewer",
        system_message="生成コードをレビューし改善提案を行う",
        llm_config=llm_config,
    )

    group_chat = GroupChat(
        agents=[planner, coder, reviewer],
        messages=[],
        max_round=10,
        speaker_selection_method="auto",  # LLMベースの発言者選択
    )

    manager = GroupChatManager(
        groupchat=group_chat,
        llm_config=llm_config,
    )

    return manager
```

## 実装のポイント（Implementation）

AutoGenを実運用に導入する際の注意点を整理する。

**応答関数チェーンの設計**: `register_reply`で登録する関数の順序が応答の優先度を決定する。ツール実行を最優先にする場合は先頭に登録し、LLM応答をフォールバックとして末尾に配置する。

**グループチャットの発言者選択**: `speaker_selection_method="auto"`はLLMに次の発言者を選ばせるが、LLM呼び出しのオーバーヘッドが発生する。決定論的なワークフローでは`"round_robin"`や遷移グラフによる明示的指定が低コストかつ予測可能である。

**トークン消費の管理**: グループチャットでは全エージェントの会話履歴が共有されるため、ラウンド数の増加に伴いトークン消費が$O(n \cdot r)$（$n$: エージェント数、$r$: ラウンド数）で増大する。`max_round`の設定と要約機能の活用が必須である。

**ネストチャットの深度制限**: 再帰的なネストは理論上無限に可能だが、実用上は2-3階層に留める。各ネストレベルでコンテキストウィンドウの消費が累積するためである。

## 実験結果（Results）

AutoGenは複数のベンチマークで単一エージェント手法を上回る成果を示している。

| タスク | 単一エージェント | AutoGen（2エージェント） | 改善率 |
|--------|-----------------|------------------------|--------|
| MATH（数学） | 57.4% | 69.5% | +12.1% |
| HumanEval（コード生成） | 68.3% | 78.0% | +9.7% |
| 質疑応答 | 62.1% | 71.8% | +9.7% |

特にコード生成タスクでは、Coder + Executorの2エージェント構成により、コードの生成・実行・エラー修正のループが自動化され、一発成功率が大幅に向上した。数学問題では、問題解析エージェントとコード実行エージェントの協調により、計算ミスの検出・修正が可能となった。

## 実運用への応用（Practical Applications）

Claude OctopusのようなオーケストレーションプラグインとAutoGenの設計思想には共通点が多い。両者とも複数のAIエージェントを並列・逐次に制御し、タスクの分割統治を実現する。

**スケーリング戦略**: AutoGenのグループチャットはエージェント数に対して線形にスケールするが、全履歴共有のためトークンコストも線形に増大する。大規模構成では、ネストチャットによる階層的分割でコンテキストウィンドウの局所化が有効である。

**レイテンシ最適化**: 逐次チャットはラウンドトリップ遅延が累積する。Claude Octopusが採用する並列実行パターンは、独立したサブタスクのレイテンシを$\max(t_1, t_2, ..., t_k)$に削減できる（$t_i$は各エージェントの処理時間）。

**コスト効率**: グループチャットの発言者選択にLLMを使用すると、各ラウンドで追加のAPI呼び出しが発生する。遷移グラフによる決定論的制御はこのコストを完全に排除でき、月額コストを30-50%削減できるケースがある。

## 関連研究（Related Work）

- **CAMEL (Li et al., 2023)**: ロールプレイベースの2エージェント会話フレームワーク。AutoGenはこれを拡張し、任意数のエージェントとプログラマブルな会話パターンを実現した
- **MetaGPT (Hong et al., 2023)**: SOP（標準作業手順）ベースのマルチエージェント協調。AutoGenがより汎用的な会話パターンを提供するのに対し、MetaGPTはソフトウェア開発に特化した構造化アプローチを採用
- **LangGraph (LangChain)**: グラフベースのエージェントワークフロー定義。AutoGenの会話パターンと相補的な位置付けにある

## まとめと今後の展望

AutoGenは「会話可能エージェント」という統一抽象により、マルチエージェントオーケストレーションの設計空間を大幅に拡張した。4つの標準会話パターンは多くの実用シナリオをカバーし、特にコード生成・数学問題解決での改善が顕著である。

今後は、エージェント間のメモリ共有の効率化、動的なエージェント生成・破棄、そしてAutoGen 0.4で導入されたイベント駆動アーキテクチャによる大規模分散環境への対応が研究の焦点となる。Claude Octopusのような実用プラグインとの統合により、開発者の生産性向上に直接貢献する方向性が期待される。

## 参考文献

- **arXiv**: https://arxiv.org/abs/2308.08155
- **Code**: https://github.com/microsoft/autogen
- **Related Zenn article**: https://zenn.dev/0h_n0/articles/c01f4e292ff1a7
