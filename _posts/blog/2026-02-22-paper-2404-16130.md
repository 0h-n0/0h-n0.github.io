---
layout: post
title: "論文解説: From Local to Global — GraphRAGによるクエリ指向要約の知識グラフアプローチ"
description: "Microsoft提案のGraphRAG論文を詳細解説。LLMによるKG自動構築、Leidenコミュニティ検出、ローカル/グローバル二層検索アーキテクチャの技術的仕組みを解説する"
categories: [blog, paper, arxiv]
tags: [GraphRAG, knowledge-graph, RAG, community-detection, LLM, multi-hop]
date: 2026-02-22 20:00:00 +0900
source_type: arxiv
arxiv_id: "2404.16130"
source_url: https://arxiv.org/abs/2404.16130
zenn_article: f894fb3fa04a59
zenn_url: https://zenn.dev/0h_n0/articles/f894fb3fa04a59
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

本論文は、テキストコーパス全体を対象とした質問応答において、LLMを用いて知識グラフ（KG）を自動構築し、Leidenアルゴリズムによるコミュニティ検出を組み合わせた二層検索アーキテクチャ「GraphRAG」を提案している。著者らは、従来のベクトル検索ベースRAGが「データセット横断的な質問」に対して根本的に弱いことを指摘し、グラフベースの階層的要約によってこの限界を克服できると主張している。

本記事は [Zenn記事: LangGraph×GraphRAGハイブリッド検索で社内文書の複合質問精度を向上させる](https://zenn.dev/0h_n0/articles/f894fb3fa04a59) の深掘りです。

## 情報源

- **arXiv ID**: 2404.16130
- **URL**: [https://arxiv.org/abs/2404.16130](https://arxiv.org/abs/2404.16130)
- **著者**: Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson (Microsoft Research)
- **発表年**: 2024
- **分野**: cs.CL, cs.AI, cs.IR

## 背景と動機（Background & Motivation）

Retrieval Augmented Generation（RAG）は、外部知識を検索してLLMの回答を補強する手法として広く普及している。しかし、従来のRAGはベクトル類似度検索に依存しており、以下の2つの根本的な制約を持つと著者らは指摘している。

1. **情報の接続性問題**: 回答に必要な情報が複数のドキュメントに散在しており、共有属性（エンティティ、概念）を介して横断的にたどる必要がある場合、ベクトル検索の1ショット検索では到達できない。
2. **データセット全体の俯瞰問題**: 「このドキュメント群の主要テーマは何か？」といった集約的・要約的な質問に対して、ベクトル検索は個別チャンクの類似度スコアのみを返すため、全体像を把握する能力がない。

著者らはこれらの問題を「sensemaking」タスク — 大量のデータから意味を抽出し、構造化する認知プロセス — と位置付け、グラフ構造を利用した新しいRAGアプローチを提案している。

## 主要な貢献（Key Contributions）

- **貢献1**: テキストからLLMを用いてエンティティ・関係・クレームを自動抽出し、知識グラフを構築するパイプラインの設計
- **貢献2**: Leidenアルゴリズムによるコミュニティ検出を適用し、階層的なコミュニティレポートを生成するアーキテクチャ
- **貢献3**: ローカル検索（エンティティ近傍）とグローバル検索（コミュニティレポートのmap-reduce集約）の二層検索モードの提案と評価

## 技術的詳細（Technical Details）

### GraphRAGインデキシングパイプライン

GraphRAGのインデキシングは以下の4段階で構成される。

**Phase 1: テキストチャンク → エンティティ・関係抽出**

入力テキストを固定サイズのチャンクに分割し、各チャンクに対してLLMを呼び出してエンティティ（人物、組織、場所、概念など）と関係性を抽出する。著者らは「gleaning」と呼ぶ複数回抽出パスを導入しており、1回目の抽出で見落としたエンティティを2回目以降のパスで補完する。

**Phase 2: グラフ構築 → Element Summaries**

抽出されたエンティティと関係をグラフ$G = (V, E)$として構築する。ここで$V$はエンティティノードの集合、$E$は関係エッジの集合である。同一エンティティの複数出現をマージし、各ノード・エッジに対してLLMで「element summary」（要素要約）を生成する。

**Phase 3: コミュニティ検出**

Leidenアルゴリズムを適用してグラフ$G$をコミュニティ$\{C_1, C_2, ..., C_k\}$に分割する。Leidenアルゴリズムはモジュラリティ$Q$を最大化するクラスタリング手法で、以下の目的関数を最適化する：

$$
Q = \frac{1}{2m} \sum_{ij} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j)
$$

ここで、
- $A_{ij}$: ノード$i$と$j$の間の隣接行列要素
- $k_i$: ノード$i$の次数
- $m$: グラフ全体のエッジ数
- $c_i$: ノード$i$が属するコミュニティ
- $\delta(c_i, c_j)$: 同一コミュニティなら1、そうでなければ0

Leidenアルゴリズムは階層的なコミュニティ構造を検出するため、解像度パラメータを調整することで異なる粒度のコミュニティを得られる。著者らはレベル0（最も細粒度）からレベル3（最も粗粒度）までの複数階層を使用している。

**Phase 4: コミュニティレポート生成**

各コミュニティ$C_k$内のノードとエッジの要素要約を入力として、LLMで「コミュニティレポート」を生成する。このレポートは、コミュニティ内の主要テーマ、キーエンティティ、重要な関係性を要約したテキストであり、グローバル検索の基盤となる。

### グローバル検索アルゴリズム

グローバル検索は、コミュニティレポートに対するmap-reduce方式で実装されている。

**Map Phase**: クエリ$q$に対して、各コミュニティレポート$R_k$をLLMに入力し、関連する知見（findings）のリストを抽出する。各知見には0〜100の関連度スコアが付与される。

$$
\text{Map}(q, R_k) = \{(f_1, s_1), (f_2, s_2), ..., (f_n, s_n)\}
$$

ここで$f_i$は知見テキスト、$s_i$は関連度スコアである。

**Reduce Phase**: 全コミュニティから得られた知見を関連度スコアで降順ソートし、LLMのコンテキストウィンドウに収まる量を選択して最終回答を生成する。

$$
\text{Answer}(q) = \text{LLM}\left(q, \text{TopK}\left(\bigcup_{k} \text{Map}(q, R_k), \text{by score}\right)\right)
$$

### ローカル検索アルゴリズム

ローカル検索は、特定のエンティティの近傍情報に基づく検索モードである。

1. クエリからエンティティを抽出
2. エンティティ埋め込みでベクトル類似度検索
3. マッチしたエンティティの近傍グラフをトラバーサル
4. 関連するコミュニティレポート、テキストチャンク、会話履歴を統合
5. LLMで回答を生成

著者らは、ローカル検索がNaive RAGと同等の事実検索タスクに強く、グローバル検索は俯瞰的・集約的な質問に強いと報告している。

### アルゴリズムの擬似コード

```python
from dataclasses import dataclass

@dataclass
class CommunityReport:
    """コミュニティレポート"""
    community_id: str
    summary: str
    entities: list[str]
    relationships: list[tuple[str, str, str]]
    level: int  # 階層レベル (0: 最細粒度)

def global_search(
    query: str,
    community_reports: list[CommunityReport],
    llm: object,
    max_tokens: int = 12000,
) -> str:
    """GraphRAGグローバル検索のmap-reduce実装

    Args:
        query: ユーザーのクエリ
        community_reports: 全コミュニティレポート
        llm: LLMインスタンス
        max_tokens: コンテキストウィンドウの制限

    Returns:
        最終回答テキスト
    """
    # Map Phase: 各レポートから知見を抽出
    all_findings: list[tuple[str, float]] = []
    for report in community_reports:
        findings = llm.extract_findings(query, report.summary)
        all_findings.extend(findings)

    # Reduce Phase: スコア降順でソート → トップK選択
    all_findings.sort(key=lambda x: x[1], reverse=True)
    selected = select_within_token_limit(all_findings, max_tokens)

    # 最終回答生成
    context = "\n".join(f"[Score: {s:.0f}] {f}" for f, s in selected)
    answer = llm.generate(query=query, context=context)
    return answer
```

## 実装のポイント（Implementation）

**インデキシングコスト**: 著者らはGPT-4 Turboを使用した場合、100万トークンのコーパスに対して数十ドルのAPI費用が発生すると報告している。gleaningの回数を増やすと抽出精度は向上するが、コストは線形に増加する。実用上は1〜2回のgleaningが推奨される。

**コミュニティレベルの選択**: グローバル検索の品質はコミュニティレベルの選択に大きく依存する。著者らの実験では、中間レベル（レベル1〜2）がコスト・品質のバランスが取れていると報告されている。レベル0は詳細すぎてトークン消費が大きく、レベル3は抽象的すぎて具体性を欠く。

**動的更新の課題**: 著者らは動的更新については明示的に議論していないが、コミュニティ構造はグラフ全体に依存するため、差分更新は原理的に困難である。新規ドキュメント追加時にコミュニティ検出を再実行する必要がある点は、実運用上のボトルネックとなりうる。

**コード公開**: GitHubリポジトリ（https://github.com/microsoft/graphrag ）でMITライセンスのもと公開されており、`pip install graphrag`でインストール可能である。

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

**トラフィック量別の推奨構成**:

| 規模 | 月間リクエスト | 推奨構成 | 月額コスト | 主要サービス |
|------|--------------|---------|-----------|------------|
| **Small** | ~3,000 (100/日) | Serverless | $80-200 | Lambda + Bedrock + Neptune Serverless |
| **Medium** | ~30,000 (1,000/日) | Hybrid | $500-1,200 | ECS Fargate + Neptune + ElastiCache |
| **Large** | 300,000+ (10,000/日) | Container | $3,000-8,000 | EKS + Neptune + OpenSearch |

**Small構成の詳細** (月額$80-200):
- **Lambda**: 2GB RAM, 60秒タイムアウト ($30/月) — グラフクエリのレイテンシ考慮
- **Bedrock**: Claude 3.5 Haiku, Prompt Caching有効 ($100/月)
- **Neptune Serverless**: 1-2 NCU ($40/月) — KGストレージ
- **CloudWatch**: 基本監視 ($5/月)

**コスト削減テクニック**:
- Neptune Serverlessで未使用時自動スケールダウン（最大90%削減）
- Bedrock Prompt Cachingでシステムプロンプト部分を30-90%削減
- インデキシングはBatch処理でBedrock Batch API 50%割引を活用
- コミュニティレポートはS3にキャッシュし、再計算を最小化

**コスト試算の注意事項**: 上記は2026年2月時点のAWS ap-northeast-1（東京）リージョン料金に基づく概算値です。Neptune Serverlessの料金はNCU時間に依存し、実際のコストはクエリ頻度・グラフサイズで大きく変動します。最新料金は [AWS料金計算ツール](https://calculator.aws/) で確認してください。

### Terraformインフラコード

**Small構成 (Serverless): Lambda + Bedrock + Neptune Serverless**

```hcl
# --- Neptune Serverless (知識グラフストレージ) ---
resource "aws_neptune_cluster" "graphrag" {
  cluster_identifier  = "graphrag-kg"
  engine              = "neptune"
  serverless_v2_scaling_configuration {
    min_capacity = 1.0   # 最小1 NCU（コスト最小化）
    max_capacity = 4.0   # ピーク時4 NCU
  }
  iam_database_authentication_enabled = true
  storage_encrypted                   = true
  kms_key_id                          = aws_kms_key.neptune.arn
}

resource "aws_neptune_cluster_instance" "graphrag" {
  cluster_identifier = aws_neptune_cluster.graphrag.id
  instance_class     = "db.serverless"
  engine             = "neptune"
}

# --- Lambda関数 (GraphRAG検索) ---
resource "aws_lambda_function" "graphrag_search" {
  filename      = "graphrag_search.zip"
  function_name = "graphrag-search"
  role          = aws_iam_role.lambda_graphrag.arn
  handler       = "index.handler"
  runtime       = "python3.12"
  timeout       = 60
  memory_size   = 2048

  environment {
    variables = {
      NEPTUNE_ENDPOINT = aws_neptune_cluster.graphrag.endpoint
      BEDROCK_MODEL_ID = "anthropic.claude-3-5-haiku-20241022-v1:0"
      COMMUNITY_REPORTS_BUCKET = aws_s3_bucket.reports.id
    }
  }

  vpc_config {
    subnet_ids         = module.vpc.private_subnets
    security_group_ids = [aws_security_group.lambda.id]
  }
}

# --- IAMロール（最小権限） ---
resource "aws_iam_role_policy" "lambda_neptune" {
  role = aws_iam_role.lambda_graphrag.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect   = "Allow"
        Action   = ["neptune-db:ReadDataViaQuery"]
        Resource = aws_neptune_cluster.graphrag.arn
      },
      {
        Effect   = "Allow"
        Action   = ["bedrock:InvokeModel"]
        Resource = "arn:aws:bedrock:ap-northeast-1::foundation-model/anthropic.claude-3-5-haiku*"
      },
      {
        Effect   = "Allow"
        Action   = ["s3:GetObject"]
        Resource = "${aws_s3_bucket.reports.arn}/*"
      }
    ]
  })
}

# --- CloudWatch コスト監視 ---
resource "aws_cloudwatch_metric_alarm" "neptune_cost" {
  alarm_name          = "neptune-ncu-spike"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "ServerlessDatabaseCapacity"
  namespace           = "AWS/Neptune"
  period              = 3600
  statistic           = "Average"
  threshold           = 3.0
  alarm_description   = "Neptune NCU使用量が閾値超過（コスト急増の可能性）"
}
```

### セキュリティベストプラクティス

- **Neptune**: VPC内配置、IAM認証有効化、KMS暗号化
- **Lambda**: VPC内配置、最小権限IAMロール
- **S3**: デフォルト暗号化、バージョニング有効化、パブリックアクセスブロック
- **Secrets**: 環境変数へのハードコード禁止、Secrets Manager使用

### 運用・監視設定

**CloudWatch Logs Insights クエリ**:
```sql
-- コミュニティレポート検索のレイテンシ分析
fields @timestamp, search_mode, latency_ms, community_level
| stats pct(latency_ms, 95) as p95, pct(latency_ms, 99) as p99 by search_mode, bin(5m)
| filter search_mode IN ["global", "local"]
```

### コスト最適化チェックリスト

- [ ] ~100 req/日 → Lambda + Neptune Serverless (Serverless) - $80-200/月
- [ ] ~1000 req/日 → ECS Fargate + Neptune (Hybrid) - $500-1,200/月
- [ ] 10000+ req/日 → EKS + Neptune + OpenSearch (Container) - $3,000-8,000/月
- [ ] Neptune Serverless: 最小NCU=1で未使用時コスト最小化
- [ ] Bedrock Batch API: インデキシング時の50%割引活用
- [ ] Prompt Caching: コミュニティレポートのシステムプロンプト固定で30-90%削減
- [ ] S3キャッシュ: コミュニティレポートの再生成を最小化
- [ ] AWS Budgets: 月額予算設定（80%で警告）
- [ ] CloudWatch: Neptune NCU使用量とBedrock トークン量の監視
- [ ] Cost Anomaly Detection: 自動異常検知有効化

## 実験結果（Results）

著者らはPodcast transcript（約1時間分）とニュース記事（CS分野）の2つのデータセットで評価を行っている。評価指標にはLLMジャッジ（GPT-4）を用い、以下の4指標で比較している（論文Table 1およびFigure 5より）。

| 指標 | Naive RAG | GraphRAG (Global) | 勝率 |
|------|----------|-------------------|------|
| Comprehensiveness | ベースライン | **+72%** | GraphRAG優位 |
| Diversity | ベースライン | **+62%** | GraphRAG優位 |
| Empowerment | ベースライン | **+55%** | GraphRAG優位 |
| Faithfulness | 同等 | 同等 | 差なし |

著者らはComprehensivenessとDiversityでGraphRAGが大幅に勝利したと報告している。一方、Faithfulness（事実忠実度）はSelfCheckGPTで測定され、両手法間で有意差がなかったと報告されている。

**制約として著者らが明記している点**: グローバル検索のmap-reduceはコミュニティレポート全体を処理するため、ローカル検索と比較してトークン消費が大きい。また、単純な事実検索タスクではNaive RAGの方が効率的であることも著者らは認めている。

## 実運用への応用（Practical Applications）

Zenn記事で紹介されているLangGraph×GraphRAGハイブリッド検索の文脈では、本論文のアプローチは以下のように活用できる。

**ルーティングとの組み合わせ**: Zenn記事のクエリルーターで「hybrid」と判定されたクエリに対して、グローバル検索を適用する。「このプロジェクト群の技術的傾向は？」のような俯瞰的質問にはグローバルモード、「田中さんの担当プロジェクトは？」のような個別質問にはローカルモードを選択する。

**コスト管理**: インデキシングコストはドキュメント規模に比例するため、Zenn記事で紹介されている差分更新パターン（`incremental_update`関数）との組み合わせが実用上重要となる。ただし、コミュニティ構造はグラフ全体に依存するため、差分更新後のコミュニティ再計算コストは残る。

**レイテンシ**: グローバル検索のmap-reduceは複数回のLLM呼び出しを伴うため、リアルタイム応答には不向きである。Zenn記事で紹介されているモデル分離パターン（ルーティングにHaiku、回答生成にSonnet）を適用し、map phaseのLLM呼び出しに軽量モデルを使用することでレイテンシを圧縮できる。

## 関連研究（Related Work）

- **RAPTOR (Sarthi et al., 2024)**: テキストチャンクをクラスタリングして階層的要約ツリーを構築する手法。GraphRAGと異なり、エンティティ・関係の構造化は行わない。著者らはGraphRAGがRAPTORに対してもComprehensiveness・Diversityで優位であったと報告している。
- **KnowledGPT (Wang et al., 2023)**: コード生成を介してKGを検索する手法。ローカルなエンティティ検索に特化しており、グローバル要約の機能はない。
- **LightRAG (Guo et al., 2024)**: GraphRAGの軽量代替として、同様のデュアルレベル検索をより少ないリソースで実現する手法。本サーベイの次記事で詳細に解説する。

## まとめと今後の展望

Microsoft GraphRAGは、ベクトル検索ベースRAGの構造的限界（情報接続性・俯瞰理解の欠如）に対して、知識グラフ＋コミュニティ検出＋階層的要約という明確な解決策を提示した論文である。著者らはComprehensiveness・Diversityでの大幅な改善を実証しているが、インデキシングコストの高さとグローバル検索のレイテンシは実運用での考慮事項として残る。

今後の研究方向として著者らは、動的更新への対応、コミュニティレベルの自動選択、ドメイン特化型エンティティ抽出の改善を挙げている。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2404.16130](https://arxiv.org/abs/2404.16130)
- **Code**: [https://github.com/microsoft/graphrag](https://github.com/microsoft/graphrag)
- **Microsoft Research Blog**: [GraphRAG: Unlocking LLM discovery on narrative private data](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/f894fb3fa04a59](https://zenn.dev/0h_n0/articles/f894fb3fa04a59)
