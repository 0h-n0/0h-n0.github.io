---
layout: post
title: "論文解説: Gemini — ネイティブマルチモーダルモデルの設計と画像・音声・動画統合処理"
description: "Google DeepMindが提案したGeminiモデルファミリーの技術的詳細を解説。画像・音声・動画をネイティブに統合処理するアーキテクチャ設計と各モダリティの評価結果を分析する。"
categories: [blog, paper, arxiv]
tags: [gemini, multimodal, transformer, google, vision-language-model, ai]
date: 2026-02-22 09:00:00 +0900
source_type: arxiv
arxiv_id: "2312.11805"
source_url: https://arxiv.org/abs/2312.11805
zenn_article: 3d32da8cfe0ac1
zenn_url: https://zenn.dev/0h_n0/articles/3d32da8cfe0ac1
math: true
mermaid: true
target_audience: "修士学生レベル"
---

本記事は [Gemini: A Family of Highly Capable Multimodal Models (arXiv:2312.11805)](https://arxiv.org/abs/2312.11805) の解説記事です。

## 論文概要（Abstract）

Google DeepMindが2023年12月に発表したGeminiは、画像・音声・動画・テキストの4モダリティをネイティブに理解・生成できるマルチモーダルモデルファミリーである。著者らは、従来の「個別モダリティモデルの結合」アプローチとは異なり、学習段階から複数モダリティを統合的に扱うことで、MMLU（90.04%、5-shot）において人間の専門家水準（89.8%）を初めて超えたと報告している。

この記事は [Zenn記事: Gemini 2.0マルチモーダルAPI実践ガイド：画像・音声・動画をPythonで統合処理する](https://zenn.dev/0h_n0/articles/3d32da8cfe0ac1) の深掘りです。

## 情報源

- **arXiv ID**: 2312.11805
- **URL**: [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805)
- **著者**: Gemini Team, Google
- **発表年**: 2023
- **分野**: cs.CL, cs.AI, cs.CV

## 背景と動機（Background & Motivation）

マルチモーダルAIの研究において、2023年時点では主に2つのアプローチが存在していた。第1に、画像エンコーダ（CLIP等）と言語モデル（LLaMA等）を結合するモジュラー方式（LLaVA、Flamingo等）。第2に、個別タスク向けに設計されたマルチタスクモデルである。

著者らは、これらのアプローチには「モダリティ間の情報統合が事後的であり、学習時に各モダリティの表現空間が十分に共有されない」という構造的限界があると指摘している。Geminiはこの課題に対し、学習段階から画像・音声・動画・テキストを統合的にトレーニングすることで、モダリティ間のシームレスな推論を実現するネイティブマルチモーダルモデルとして設計された。

## 主要な貢献（Key Contributions）

- **貢献1**: 画像・音声・動画・テキストの4モダリティを学習段階から統合的に処理するネイティブマルチモーダルアーキテクチャの提案
- **貢献2**: MMLU（90.04%）で人間の専門家水準を初めて超えたマルチモーダルモデル。従来のテキスト専用モデルではなく、マルチモーダル設定での達成である点が特徴的
- **貢献3**: Ultra（データセンター向け）、Pro（汎用）、Nano（モバイル向け）の3サイズ展開により、推論環境に応じた柔軟なデプロイを可能にした設計

## 技術的詳細（Technical Details）

### アーキテクチャ

Geminiのコアアーキテクチャは、効率的なアテンション機構を備えたTransformerデコーダである。論文によると、Google TPU上での学習と推論に最適化されている。

マルチモーダル入力の処理フローは以下の通りである：

1. **モダリティ別エンコーダ**: 画像・音声・動画はそれぞれ専用のエンコーダで埋め込みベクトルに変換される
2. **トークンインターリーブ**: 各モダリティのembeddingがテキストトークンと交互に配置され、単一のシーケンスとしてTransformerデコーダに入力される
3. **統一的な自己回帰生成**: デコーダがマルチモーダルコンテキストを統合的に処理し、テキストまたは画像の出力を生成する

数式で表現すると、入力シーケンス $\mathbf{x}$ は以下のように構成される：

$$
\mathbf{x} = [e_{\text{text}}^{(1)}, e_{\text{img}}^{(1)}, e_{\text{text}}^{(2)}, e_{\text{audio}}^{(1)}, \ldots]
$$

ここで、
- $e_{\text{text}}^{(i)}$: $i$番目のテキストトークン埋め込み
- $e_{\text{img}}^{(j)}$: $j$番目の画像パッチ埋め込み（モダリティ別エンコーダの出力）
- $e_{\text{audio}}^{(k)}$: $k$番目の音声フレーム埋め込み

Transformerデコーダの自己注意機構は、これらの異なるモダリティのトークンを区別せず、統一的にアテンション計算を行う：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

ここで $Q, K, V$ はマルチモーダルトークン列全体から生成されるため、画像トークンがテキストトークンにアテンドし、音声トークンが画像トークンにアテンドする、というクロスモーダルな情報統合が自然に実現される。

### 画像・動画エンコーダ

論文ではエンコーダの詳細なアーキテクチャは公開されていないが、以下の特徴が記述されている：

- 画像はパッチ分割後にViT（Vision Transformer）ベースのエンコーダで処理される
- 動画はフレーム列として処理され、時間方向の情報も埋め込みに含まれる
- 音声はスペクトログラム表現に変換後、音声エンコーダで処理される

### コンテキスト長

Gemini 1.0はコンテキスト長32Kトークンに対応しており、論文ではこれを「long context」として位置づけている。後継のGemini 1.5では100万トークンに拡張されたが、1.0時点では32Kが上限である。

### モデルサイズとデプロイ

| モデル | 対象環境 | 用途 |
|--------|---------|------|
| Gemini Ultra | データセンター | 高精度タスク |
| Gemini Pro | データセンター | 汎用マルチモーダル |
| Gemini Nano | モバイルデバイス | エッジ推論 |

Nano版はモデル蒸留により小型化されており、Pixelスマートフォン等でのオンデバイス推論を想定している。

## 実装のポイント（Implementation）

Geminiのアーキテクチャを理解する上で重要なのは、「モダリティ別エンコーダの出力を言語モデルのembedding空間に投影する」というパターンである。これはGemini APIを利用する際の入力形式にも反映されている。

```python
from google import genai
from google.genai import types

def multimodal_inference(
    image_bytes: bytes,
    audio_bytes: bytes,
    text_prompt: str,
) -> str:
    """Geminiのネイティブマルチモーダル推論を実行する

    Args:
        image_bytes: 画像のバイナリデータ
        audio_bytes: 音声のバイナリデータ
        text_prompt: テキストプロンプト

    Returns:
        モデルの応答テキスト
    """
    client = genai.Client()

    # 複数モダリティを単一リクエストで送信
    # → 内部ではエンコーダ出力がインターリーブされる
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=[
            types.Part.from_bytes(data=image_bytes, mime_type="image/jpeg"),
            types.Part.from_bytes(data=audio_bytes, mime_type="audio/mp3"),
            text_prompt,
        ],
    )
    return response.text
```

このAPIの設計は、論文で述べられている「マルチモーダルトークンのインターリーブ」アーキテクチャを反映しており、複数モダリティを1つのリクエストで統合的に処理できる。

**実装上の注意点**:
- 画像入力は258トークン/画像に換算される（論文時点の仕様）
- 音声入力は約32トークン/秒で処理される
- 動画入力は約300トークン/秒で処理される（フレーム+音声）
- 32Kトークン制限（Gemini 1.0）下では、約1分40秒の動画が上限

## 実験結果（Results）

著者らは広範なベンチマークでGemini Ultraを評価している。主要な結果を以下に示す（論文Table 2, 3, 5より）。

### テキスト理解

| ベンチマーク | GPT-4 | Gemini Ultra | 改善 |
|-------------|-------|-------------|------|
| MMLU (5-shot) | 86.4% | **90.04%** | +3.6% |
| HellaSwag (10-shot) | 95.3% | **87.8%** | - |
| DROP | 80.9% | **82.4%** | +1.5% |

### 画像理解

| ベンチマーク | GPT-4V | Gemini Ultra | 改善 |
|-------------|--------|-------------|------|
| MMMU (val) | 56.8% | **59.4%** | +2.6% |
| VQAv2 | 77.2% | **77.8%** | +0.6% |
| TextVQA | 78.0% | **82.3%** | +4.3% |

### 動画理解

| ベンチマーク | 従来最良 | Gemini Ultra |
|-------------|---------|-------------|
| VATEX (ZeroShot) | - | **84.7%** |
| VideoMME (推定) | ~70% | **75%+** |

### 音声理解

| タスク | 従来最良 | Gemini Ultra |
|--------|---------|-------------|
| FLEURS ASR (62言語) | - | **best-in-class** |
| CoVoST 2 翻訳 | - | **best-in-class** |

著者らは、Gemini Ultraが30以上のベンチマークのうち大半でSOTA（State-of-the-Art）を達成したと報告している。特に、MMLUでの90.04%は人間の専門家水準（89.8%）を上回る初の結果であるとしている。

**結果の制約**: 論文ではGemini Ultra単体の詳細なレイテンシ・スループットは報告されていない。また、画像生成の品質についても定量的な評価は限定的である。

## 実運用への応用（Practical Applications）

Zenn記事で解説されているGemini 2.0/2.5 APIの利用パターンは、この論文のアーキテクチャ設計に直接基づいている。具体的には：

1. **画像理解API**: モダリティ別エンコーダ → トークンインターリーブ → テキスト生成という処理フローが、`generate_content`の1回のAPI呼び出しに対応
2. **動画処理API**: 動画をフレーム列として処理する設計が、Files APIを通じた動画アップロード→分析の流れに反映
3. **音声処理API**: 音声エンコーダの出力がテキストトークンと統合される設計が、音声ファイルの直接入力を可能にしている

**本番運用での考慮事項**:
- Gemini APIはクローズドモデルであり、ローカルデプロイは不可。API依存のアーキテクチャ設計が必要
- 動画入力のトークン消費量（300トークン/秒）はコストに直結する。10分の動画で約180,000トークンを消費する
- 論文では安全性評価も実施されており、StereoSet、CrowS-Pairs等のバイアス評価が報告されている。本番運用では出力フィルタリングの実装が推奨される

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

Geminiのマルチモーダル推論をAWSで運用するパターンを以下に示す。Gemini自体はGoogle Cloud APIだが、バックエンドシステムをAWS上に構築するケースを想定する。

**トラフィック量別の推奨構成**:

| 規模 | 月間リクエスト | 推奨構成 | 月額コスト | 主要サービス |
|------|--------------|---------|-----------|------------|
| **Small** | ~3,000 (100/日) | Serverless | $50-150 | Lambda + Gemini API + DynamoDB |
| **Medium** | ~30,000 (1,000/日) | Hybrid | $300-800 | Lambda + ECS Fargate + ElastiCache |
| **Large** | 300,000+ (10,000/日) | Container | $2,000-5,000 | EKS + Karpenter + EC2 Spot |

**Small構成の詳細** (月額$50-150):
- **Lambda**: 1GB RAM, 60秒タイムアウト ($20/月) — Gemini API呼び出しのラッパー
- **Gemini API**: Flash モデル使用、入力$0.10/Mトークン ($80/月)
- **DynamoDB**: On-Demand、結果キャッシュ ($10/月)
- **CloudWatch**: 基本監視 ($5/月)
- **API Gateway**: REST API ($5/月)

**コスト削減テクニック**:
- Gemini API Prompt Cachingで30-90%削減（システムプロンプト固定部分）
- 動画入力の低解像度設定で トークン消費を1/3に削減
- DynamoDBキャッシュで同一ファイルへの重複リクエストを排除

**コスト試算の注意事項**:
- 上記は2026年2月時点のGemini API料金とAWS ap-northeast-1リージョン料金に基づく概算値です
- Gemini API料金は Google の料金改定により変動します
- 最新料金は [AWS料金計算ツール](https://calculator.aws/) および [Gemini API Pricing](https://ai.google.dev/gemini-api/docs/pricing) で確認してください

### Terraformインフラコード

**Small構成 (Serverless): Lambda + Gemini API + DynamoDB**

```hcl
# --- IAMロール（最小権限） ---
resource "aws_iam_role" "lambda_gemini" {
  name = "lambda-gemini-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy" "dynamodb_access" {
  role = aws_iam_role.lambda_gemini.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect   = "Allow"
      Action   = ["dynamodb:GetItem", "dynamodb:PutItem", "dynamodb:Query"]
      Resource = aws_dynamodb_table.cache.arn
    }]
  })
}

# --- Lambda関数 ---
resource "aws_lambda_function" "gemini_handler" {
  filename      = "lambda.zip"
  function_name = "gemini-multimodal-handler"
  role          = aws_iam_role.lambda_gemini.arn
  handler       = "index.handler"
  runtime       = "python3.12"
  timeout       = 120
  memory_size   = 1024

  environment {
    variables = {
      GEMINI_MODEL_ID  = "gemini-2.5-flash"
      DYNAMODB_TABLE   = aws_dynamodb_table.cache.name
    }
  }
}

# --- DynamoDB (On-Demand) ---
resource "aws_dynamodb_table" "cache" {
  name         = "gemini-response-cache"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "request_hash"

  attribute {
    name = "request_hash"
    type = "S"
  }

  ttl {
    attribute_name = "expire_at"
    enabled        = true
  }
}
```

### 運用・監視設定

**CloudWatch Logs Insights クエリ**:
```sql
-- Gemini APIレイテンシ分析
fields @timestamp, duration_ms, model_id, input_tokens, output_tokens
| stats pct(duration_ms, 95) as p95, pct(duration_ms, 99) as p99 by bin(5m)
| filter duration_ms > 0
```

**CloudWatch アラーム**:
```python
import boto3

cloudwatch = boto3.client('cloudwatch')

cloudwatch.put_metric_alarm(
    AlarmName='gemini-api-latency-spike',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='Duration',
    Namespace='AWS/Lambda',
    Period=300,
    Statistic='Average',
    Threshold=60000,
    AlarmDescription='Gemini API応答時間異常（60秒超過）'
)
```

### コスト最適化チェックリスト

- [ ] ~100 req/日 → Lambda + Gemini API (Serverless) - $50-150/月
- [ ] ~1000 req/日 → ECS Fargate + Gemini API (Hybrid) - $300-800/月
- [ ] Gemini API: Flash モデル優先（Proの1/10コスト）
- [ ] Prompt Caching有効化で30-90%削減
- [ ] 動画入力: 低解像度設定でトークン1/3削減
- [ ] DynamoDB キャッシュで重複リクエスト排除
- [ ] AWS Budgets: 月額予算設定（80%で警告）
- [ ] CloudWatch: API応答時間とエラー率の監視

## 関連研究（Related Work）

- **GPT-4V (OpenAI, 2023)**: 画像理解に対応した大規模言語モデル。Geminiとの主な差異は、GPT-4Vが画像のみのマルチモーダル対応であるのに対し、Geminiは音声・動画も含む4モダリティ対応である点
- **LLaVA (Liu et al., 2023)**: CLIP画像エンコーダとLLaMAを結合したモジュラー方式のVLM。学習効率は高いが、モダリティ間の統合はGeminiのネイティブ方式ほど深くない
- **Flamingo (Alayrac et al., 2022)**: DeepMindの先行研究。Perceiverベースのクロスアテンション機構で視覚情報を言語モデルに注入する。Geminiはこのアプローチをさらに発展させ、全モダリティのエンコーダ出力を直接Transformerデコーダに統合した

## まとめと今後の展望

Gemini 1.0は、マルチモーダルAIの設計パラダイムにおいて「ネイティブ統合」の有効性を実証した。MMLU 90.04%での人間専門家水準の超越は、テキスト理解においてもマルチモーダル学習が有効であることを示唆している。

後継のGemini 1.5（MoEアーキテクチャ、100万トークンコンテキスト）、Gemini 2.0（ネイティブ画像生成、TTS）、Gemini 2.5（動画理解の大幅向上）へと進化しており、この論文で確立されたアーキテクチャの基本設計は後続モデルにも継承されている。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/3d32da8cfe0ac1](https://zenn.dev/0h_n0/articles/3d32da8cfe0ac1)
- **Gemini API Documentation**: [https://ai.google.dev/gemini-api/docs](https://ai.google.dev/gemini-api/docs)
