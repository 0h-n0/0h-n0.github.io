---
layout: post
title: "ICLR 2025論文解説: RouteLLM — 選好データで学習するLLMルーティングフレームワーク"
description: "RouteLLMは人間の選好データを活用してLLMルーターを訓練し、コスト85%削減と品質95%維持を両立するフレームワークです"
categories: [blog, paper, arxiv]
tags: [LLM, routing, cost-optimization, preference-data, matrix-factorization, ICLR]
date: 2026-02-19 10:00:00 +0900
source_type: arxiv
arxiv_id: "2406.02658"
source_url: https://arxiv.org/abs/2406.02658
zenn_article: 3e603a1b91e2e0
zenn_url: https://zenn.dev/0h_n0/articles/3e603a1b91e2e0
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

RouteLLMは、人間の選好データ（Chatbot Arena等）を活用してLLMルーターを訓練するフレームワークです。強モデル（GPT-4）と弱モデル（GPT-3.5-turbo）の間でクエリを動的に振り分けることで、GPT-4単独利用と比較して**コストを85%以上削減**しつつ、**応答品質の95%を維持**することに成功しています。4種類のルーターモデル（類似度加重ランキング、行列分解、BERT分類器、因果LLM分類器）を提案し、MT-Bench・MMLU・GSM8Kで有効性を実証しました。ICLR 2025にて正式採択されています。

この記事は [Zenn記事: LLMルーター実践ガイド：RouteLLM×LiteLLMでAPIコスト60%削減を実現する](https://zenn.dev/0h_n0/articles/3e603a1b91e2e0) の深掘りです。

## 情報源

- **arXiv ID**: 2406.02658
- **URL**: [https://arxiv.org/abs/2406.02658](https://arxiv.org/abs/2406.02658)
- **著者**: Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez et al.
- **発表年**: 2024（ICLR 2025採択）
- **分野**: cs.CL, cs.LG
- **コード**: [https://github.com/lm-sys/RouteLLM](https://github.com/lm-sys/RouteLLM)

## 背景と動機（Background & Motivation）

LLMの商用APIは性能とコストのトレードオフが顕著です。GPT-4oの入力トークン単価はGPT-4o-miniの約7倍であり、全クエリを最高性能モデルに送るのはコスト面で非現実的です。一方、すべてを安価なモデルに送ると品質が低下します。

従来のルーティング手法には以下の課題がありました：

- **ルールベース手法**: キーワードやタスク種別で静的に振り分けるが、曖昧なクエリの分類精度が低い
- **ベンチマークデータ利用**: 特定タスクのベンチマーク結果で訓練するが、汎用的なチャットタスクへの転移性が不十分
- **LLMベースの事前判定**: 小型LLMでクエリ複雑度を判定するが、判定自体にコストとレイテンシが発生する

RouteLLMの革新性は、RLHF（Reinforcement Learning from Human Feedback）用に大量に蓄積された**選好データ（preference data）**を転用する点にあります。選好データには「どのクエリでどのモデルが優位か」という情報が暗黙的に含まれており、これがルーティングの訓練信号として機能します。

## 主要な貢献（Key Contributions）

- **貢献1**: 選好データを活用したLLMルーティング手法の提案 — Chatbot Arenaの約88kペアワイズ比較データから、クエリ難易度の暗黙的ラベルを抽出し、4種類のルーターモデルを訓練する枠組みを確立
- **貢献2**: 4種類のルーターモデルの設計と比較 — 類似度加重ランキング（SW）、行列分解（MF）、BERT分類器、因果LLM分類器の4手法を提案し、コスト-品質のパレートフロンティアを系統的に評価
- **貢献3**: 転移性の実証 — 1つの選好データセットで訓練したルーターが、新しいデータセット・モデルペア・ルーティング設定に対して最小限の性能劣化で適用可能であることを実証
- **貢献4**: OSSフレームワークの公開 — OpenAI互換APIとして`pip install routellm`でドロップイン導入できるPythonパッケージを公開

## 技術的詳細（Technical Details）

### 問題定式化

2つのLLM（強モデル $S$ と弱モデル $W$）が与えられたとき、各クエリ $q$ に対して、品質を最大化しつつコストを最小化するルーティング関数 $r(q) \in \{S, W\}$ を学習する問題として定式化します。

$$
\min_{r} \sum_{q \in Q} \text{cost}(r(q)) \quad \text{s.t.} \quad \frac{1}{|Q|}\sum_{q \in Q} \text{quality}(r(q), q) \geq \tau
$$

ここで、
- $Q$: クエリ集合
- $\text{cost}(r(q))$: ルーティング先モデルの呼び出しコスト（$\text{cost}(S) \gg \text{cost}(W)$）
- $\text{quality}(r(q), q)$: クエリ $q$ に対するルーティング先モデルの応答品質
- $\tau$: 品質の下限閾値

### 選好データの活用

Chatbot Arenaでは、ユーザーが2つのモデル出力を比較して「どちらが良いか」を投票します。このデータから、各クエリについて「強モデルが弱モデルに勝つ確率 $p(S \succ W \mid q)$」を推定できます。

- $p(S \succ W \mid q)$ が高い → クエリが難しく、強モデルが必要
- $p(S \succ W \mid q)$ が低い → クエリが簡単で、弱モデルで十分

この確率をルーティングスコアとして利用します。閾値 $\theta$ を設定し：

$$
r(q) = \begin{cases} S & \text{if } \hat{p}(S \succ W \mid q) \geq \theta \\ W & \text{otherwise} \end{cases}
$$

### 4種類のルーターモデル

**1. 類似度加重ランキング（SW: Similarity-Weighted Ranking）**

新しいクエリ $q$ に対して、選好データ中の既知クエリとのコサイン類似度を計算し、類似クエリの選好結果を加重平均します。

$$
\hat{p}(S \succ W \mid q) = \frac{\sum_{i} \text{sim}(q, q_i) \cdot y_i}{\sum_{i} \text{sim}(q, q_i)}
$$

ここで $\text{sim}(q, q_i)$ はBERT埋め込みのコサイン類似度、$y_i \in \{0, 1\}$ は強モデルが選好されたかどうかのラベルです。

**2. 行列分解（MF: Matrix Factorization）**

選好データを $(\text{クエリ}, \text{モデル})$ ペアの行列として表現し、低ランク分解で潜在特徴を学習します。

$$
\hat{p}(S \succ W \mid q) = \sigma(\mathbf{u}_q^\top \mathbf{v}_S - \mathbf{u}_q^\top \mathbf{v}_W + b_S - b_W)
$$

ここで、
- $\mathbf{u}_q \in \mathbb{R}^d$: クエリ $q$ の潜在ベクトル
- $\mathbf{v}_S, \mathbf{v}_W \in \mathbb{R}^d$: 各モデルの潜在ベクトル
- $b_S, b_W$: モデルごとのバイアス項
- $\sigma$: シグモイド関数

MFルーターは推論が高速（行列積のみ）で、追加レイテンシが5-10msと極めて小さいのが特長です。

**3. BERT分類器**

BERT-baseをファインチューニングし、クエリテキストから直接ルーティング判断を出力する二値分類器です。

$$
\hat{p}(S \succ W \mid q) = \text{MLP}(\text{BERT}(q))
$$

入力はクエリテキスト、出力は強モデルを選ぶ確率です。4つのルーター中で最高の分類精度を達成します。

**4. 因果LLM分類器**

LLaMA-3-8B等の小型因果LLMに分類ヘッドを追加してファインチューニングします。言語理解能力を活用してルーティング精度を向上させます。

### 閾値キャリブレーション

閾値 $\theta$ はコスト-品質のトレードオフを制御するハイパーパラメータです。RouteLLMは`calibrate_threshold`ユーティリティを提供しており、少量のラベル付きデータから最適閾値を自動推定できます。

閾値を高く設定するほど、より多くのクエリが弱モデルに振り分けられます：

| 閾値 $\theta$ | 強モデル使用率 | 品質維持率（MT-Bench基準） | コスト削減率 |
|---|---|---|---|
| 0.05 | ~70% | ~98% | ~25% |
| 0.11593 | ~50% | ~97% | ~43% |
| 0.5 | ~15% | ~90% | ~73% |

## 実装のポイント（Implementation）

RouteLLMは`pip install routellm`でインストール可能で、OpenAI互換クライアントとして動作します。

```python
from routellm.controller import Controller

client = Controller(
    routers=["mf"],
    strong_model="gpt-4o",
    weak_model="gpt-4o-mini",
)

response = client.chat.completions.create(
    model="router-mf-0.11593",  # mfルーター、閾値0.11593
    messages=[{"role": "user", "content": "量子コンピュータの仕組みを説明して"}],
)
```

**実装上の注意点**:

- **モデル文字列の書式**: `router-{router_type}-{threshold}` の形式で指定（例: `router-mf-0.11593`）
- **事前学習済み重み**: MFルーターとBERT分類器は事前学習済み重みが同梱されている
- **LiteLLMとの統合**: 内部的にLiteLLMを使用しているため、100以上のLLMプロバイダーに対応
- **閾値の選択**: まず`calibrate_threshold`で自社クエリ分布に合わせた最適閾値を探索すべき
- **バッチ推論**: 大量クエリの場合、ルーティング判定をバッチ化することでオーバーヘッドを削減可能

**ハマりやすいポイント**:

1. 閾値を高く設定しすぎると品質が急降下する（0.5以上は要注意）
2. デフォルトのMFルーターはGPT-4/Mixtralペアで訓練されており、異なるモデルペアでは再キャリブレーションが必要
3. 専門ドメイン（医療・法律等）ではChatbot Arenaデータが不足するため、ドメイン固有データでの増強が推奨される

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

RouteLLMをAWS上で本番運用する際の推奨構成を、トラフィック量別に示します。

**トラフィック量別の推奨構成**:

| 規模 | 月間リクエスト | 推奨構成 | 月額コスト | 主要サービス |
|------|--------------|---------|-----------|------------|
| **Small** | ~3,000 (100/日) | Serverless | $50-150 | Lambda + Bedrock + DynamoDB |
| **Medium** | ~30,000 (1,000/日) | Hybrid | $300-800 | Lambda + ECS Fargate + ElastiCache |
| **Large** | 300,000+ (10,000/日) | Container | $2,000-5,000 | EKS + Karpenter + EC2 Spot |

**Small構成の詳細**（月額$50-150）:
- **Lambda**: RouteLLMのルーティング判定 + Bedrock呼び出し（1GB RAM, 60秒タイムアウト, $20/月）
- **Bedrock**: Claude 3.5 Haiku（弱モデル）+ Claude 3.5 Sonnet（強モデル）, Prompt Caching有効化（$80/月）
- **DynamoDB**: ルーティング結果キャッシュ, On-Demand（$10/月）
- **CloudWatch**: 基本監視（$5/月）

**Medium構成の詳細**（月額$300-800）:
- **ECS Fargate**: RouteLLMサーバー常駐, 0.5 vCPU / 1GB RAM × 2タスク（$120/月）
- **Bedrock**: Batch API活用で50%割引（$400/月）
- **ElastiCache Redis**: ルーティングキャッシュ + セッション管理（$15/月）
- **ALB**: ロードバランシング（$20/月）

**コスト削減テクニック**:
- Bedrock Batch APIで50%割引（非リアルタイム処理向け）
- Prompt Cachingで30-90%削減（RouteLLMのシステムプロンプト固定化）
- EC2 Spot Instances使用で最大90%削減（EKS + Karpenter構成）

**コスト試算の注意事項**: 上記は2026年2月時点のAWS ap-northeast-1（東京）リージョン料金に基づく概算値です。実際のコストはトラフィックパターンやBedrock利用モデルにより大きく変動します。最新料金は [AWS料金計算ツール](https://calculator.aws/) で確認してください。

### Terraformインフラコード

**Small構成 (Serverless): Lambda + Bedrock + DynamoDB**

```hcl
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"
  name = "routellm-vpc"
  cidr = "10.0.0.0/16"
  azs  = ["ap-northeast-1a", "ap-northeast-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  enable_nat_gateway   = false
  enable_dns_hostnames = true
}

resource "aws_iam_role" "lambda_bedrock" {
  name = "routellm-lambda-bedrock-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action    = "sts:AssumeRole"
      Effect    = "Allow"
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy" "bedrock_invoke" {
  role = aws_iam_role.lambda_bedrock.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect   = "Allow"
      Action   = ["bedrock:InvokeModel", "bedrock:InvokeModelWithResponseStream"]
      Resource = [
        "arn:aws:bedrock:ap-northeast-1::foundation-model/anthropic.claude-3-5-haiku*",
        "arn:aws:bedrock:ap-northeast-1::foundation-model/anthropic.claude-3-5-sonnet*"
      ]
    }]
  })
}

resource "aws_lambda_function" "routellm_handler" {
  filename      = "lambda.zip"
  function_name = "routellm-bedrock-handler"
  role          = aws_iam_role.lambda_bedrock.arn
  handler       = "index.handler"
  runtime       = "python3.12"
  timeout       = 60
  memory_size   = 1024
  environment {
    variables = {
      STRONG_MODEL     = "anthropic.claude-3-5-sonnet-20241022-v2:0"
      WEAK_MODEL       = "anthropic.claude-3-5-haiku-20241022-v1:0"
      ROUTER_TYPE      = "mf"
      ROUTER_THRESHOLD = "0.11593"
      DYNAMODB_TABLE   = aws_dynamodb_table.routing_cache.name
    }
  }
}

resource "aws_dynamodb_table" "routing_cache" {
  name         = "routellm-routing-cache"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "query_hash"
  attribute { name = "query_hash"; type = "S" }
  ttl { attribute_name = "expire_at"; enabled = true }
}

resource "aws_cloudwatch_metric_alarm" "lambda_cost" {
  alarm_name          = "routellm-lambda-cost-spike"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "Duration"
  namespace           = "AWS/Lambda"
  period              = 3600
  statistic           = "Sum"
  threshold           = 100000
  alarm_description   = "RouteLLM Lambda実行時間異常（コスト急増の可能性）"
  dimensions = { FunctionName = aws_lambda_function.routellm_handler.function_name }
}
```

**Large構成 (Container): EKS + Karpenter + Spot Instances**

```hcl
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  version         = "~> 20.0"
  cluster_name    = "routellm-cluster"
  cluster_version = "1.31"
  vpc_id          = module.vpc.vpc_id
  subnet_ids      = module.vpc.private_subnets
  cluster_endpoint_public_access = true
  enable_cluster_creator_admin_permissions = true
}

resource "kubectl_manifest" "karpenter_provisioner" {
  yaml_body = <<-YAML
    apiVersion: karpenter.sh/v1alpha5
    kind: Provisioner
    metadata:
      name: routellm-spot
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]
        - key: node.kubernetes.io/instance-type
          operator: In
          values: ["m6i.xlarge", "m6i.2xlarge"]
      limits:
        resources:
          cpu: "32"
          memory: "128Gi"
      ttlSecondsAfterEmpty: 30
  YAML
}

resource "aws_budgets_budget" "routellm_monthly" {
  name         = "routellm-monthly-budget"
  budget_type  = "COST"
  limit_amount = "5000"
  limit_unit   = "USD"
  time_unit    = "MONTHLY"
  notification {
    comparison_operator        = "GREATER_THAN"
    threshold                  = 80
    threshold_type             = "PERCENTAGE"
    notification_type          = "ACTUAL"
    subscriber_email_addresses = ["ops@example.com"]
  }
}
```

### セキュリティベストプラクティス

- **IAMロール**: 最小権限の原則。Bedrock呼び出しは特定モデルARNに限定
- **ネットワーク**: EKSはprivate subnet配置、VPCエンドポイント経由でBedrock接続
- **シークレット管理**: AWS Secrets Manager使用、環境変数ハードコード禁止
- **暗号化**: DynamoDB/S3はKMS暗号化、TLS 1.2以上必須
- **監査**: CloudTrail全リージョン有効化、GuardDuty脅威検知有効化

### 運用・監視設定

**CloudWatch Logs Insights クエリ**:

```sql
-- ルーティング比率の時系列分析
fields @timestamp, routed_model, query_length
| stats count(*) as total,
        sum(case when routed_model = 'strong' then 1 else 0 end) as strong_count
  by bin(1h)
| sort @timestamp desc
```

**CloudWatch アラーム設定**:

```python
import boto3
cloudwatch = boto3.client('cloudwatch')

cloudwatch.put_metric_alarm(
    AlarmName='routellm-strong-model-ratio-spike',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='StrongModelRatio',
    Namespace='RouteLLM/Custom',
    Period=300,
    Statistic='Average',
    Threshold=0.7,  # 強モデル使用率70%超過でアラート
    AlarmDescription='RouteLLM強モデル比率異常（コスト急増リスク）'
)
```

### コスト最適化チェックリスト

- [ ] ~100 req/日 → Lambda + Bedrock (Serverless) - $50-150/月
- [ ] ~1000 req/日 → ECS Fargate + Bedrock (Hybrid) - $300-800/月
- [ ] 10000+ req/日 → EKS + Spot Instances (Container) - $2,000-5,000/月
- [ ] Bedrock Batch API: 50%割引（非リアルタイム処理）
- [ ] Prompt Caching: 30-90%削減（システムプロンプト固定）
- [ ] モデル選択: Haiku（弱）$0.25/MTok / Sonnet（強）$3/MTok
- [ ] RouteLLM閾値: `calibrate_threshold`で自社データに最適化
- [ ] AWS Budgets: 月額予算設定（80%で警告）
- [ ] Cost Anomaly Detection: 自動異常検知有効化
- [ ] 未使用リソース削除: Lambda Insights + Trusted Advisor

## 実験結果（Results）

**MT-Bench**（マルチターン会話ベンチマーク）:

| ルーターモデル | コスト削減率 | 品質維持率 |
|---|---|---|
| BERT分類器 | 85% | 95% |
| MF（行列分解） | 82% | 95% |
| 因果LLM分類器 | 80% | 95% |
| SW（類似度加重） | 75% | 95% |

BERT分類器が最も効率的なコスト-品質トレードオフを実現しています。MFルーターはBERT分類器に近い性能を達成しつつ、推論レイテンシが大幅に小さい（5-10ms vs 20-50ms）ため、レイテンシが重要なユースケースでは有利です。

**転移性の検証**:

Chatbot Arenaデータで訓練したルーターを、異なるベンチマーク（MMLU, GSM8K）に適用した結果、最小限の性能劣化で転移できることを確認しました。また、GPT-4/GPT-3.5-turboペアで訓練したルーターをClaude-3-Opus/Haikuペアに適用する際も、簡単なキャリブレーション（閾値調整のみ）で対応可能です。

## 実運用への応用（Practical Applications）

**RouteLLMが特に有効な場面**:

1. **チャットボット**: ユーザーの質問の70-80%は簡単（FAQ、雑談等）であり、安価なモデルで十分。複雑な推論が必要な質問のみ強モデルに送る
2. **バッチ処理パイプライン**: 大量のドキュメント処理で、品質要件が均一でない場合に有効。RouteLLM + Bedrock Batch APIの組み合わせで大幅なコスト削減が可能
3. **マルチテナントSaaS**: テナントごとに品質要件が異なる場合、閾値をテナント別に設定することでコスト-品質を個別最適化

**本番環境での運用上の注意**:

- ルーティング判定のログを必ず記録し、強モデル/弱モデルの使用率を日次で監視すること
- クエリ分布が変化した場合（新機能追加、ユーザー層の変化等）は閾値の再キャリブレーションが必要
- 専門ドメイン向けの追加訓練データがある場合は、ルーターのファインチューニングで品質を向上できる

## 関連研究（Related Work）

- **FrugalGPT** (Chen et al., 2023): LLMカスケードの先駆的研究。安い順にモデルを試し、スコアリング関数で品質を評価して停止する方式。最大98%コスト削減を報告するが、タスク固有のラベル付きデータが必要
- **Hybrid LLM** (Ding et al., 2024, ICLR 2024): クエリ難易度予測による軽量ルーター。Microsoft Researchによる研究で、コスト40%削減・品質96%維持。RouteLLMはより大幅なコスト削減を実現
- **AutoMix** (Aggarwal et al., 2023): 自己検証（Self-Verification）とPOMDPによるカスケードルーティング。メタ検証器で品質を評価し、段階的にモデルをエスカレーション。異なるアプローチだが補完的

## まとめと今後の展望

RouteLLMは、選好データという既存リソースを活用してLLMルーターを訓練するフレームワークです。85%以上のコスト削減と95%の品質維持を両立し、OSSとして公開されているため即座に導入可能です。

**今後の研究方向**:

1. **3モデル以上へのルーティング拡張**: 現在はバイナリルーティングのみ。中間モデルを含む多段ルーティングの実現が今後の課題
2. **オンライン学習**: 運用中のフィードバックからルーターを継続的に更新する仕組み
3. **ドメイン適応**: 専門ドメインでの性能向上のための効率的なファインチューニング手法

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2406.02658](https://arxiv.org/abs/2406.02658)
- **Code**: [https://github.com/lm-sys/RouteLLM](https://github.com/lm-sys/RouteLLM)
- **LMSYS Blog**: [https://lmsys.org/blog/2024-07-01-routellm/](https://lmsys.org/blog/2024-07-01-routellm/)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/3e603a1b91e2e0](https://zenn.dev/0h_n0/articles/3e603a1b91e2e0)
