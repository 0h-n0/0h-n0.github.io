---
layout: post
title: "論文解説: PyramidKV — KVキャッシュ12%でFull精度の99%を維持する動的圧縮手法"
description: "PyramidKVはレイヤーごとの注意パターンの違いに着目し、ピラミッド型のKVキャッシュ割り当てで12%のメモリでFull KV比99.4%の精度を達成"
categories: [blog, paper, arxiv]
tags: [LLM, kv-cache, prompt-caching, rag, aws, bedrock, memory-optimization]
date: 2026-02-22 09:00:00 +0900
source_type: arxiv
arxiv_id: "2405.14256"
source_url: https://arxiv.org/abs/2405.14256
zenn_article: d027acf4081b9d
zenn_url: https://zenn.dev/0h_n0/articles/d027acf4081b9d
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

PyramidKVは、LLM推論時のKV（Key-Value）キャッシュを動的に圧縮する手法である。著者らは、Transformerの注意パターンがレイヤーによって異なるという観察に基づき、下位レイヤーに多くのKVキャッシュを、上位レイヤーに少ないKVキャッシュを割り当てる「ピラミッド型」の配分戦略を提案している。LLaMA-2-7B-chatでの実験において、KVキャッシュを全体の約12%まで削減した条件で、Full KV比99.4%の精度を維持したと報告されている。

この記事は [Zenn記事: Bedrock AgentCore×1時間キャッシュで社内RAGコスト90%削減](https://zenn.dev/0h_n0/articles/d027acf4081b9d) の深掘りです。

## 情報源

- **arXiv ID**: 2405.14256
- **URL**: [https://arxiv.org/abs/2405.14256](https://arxiv.org/abs/2405.14256)
- **著者**: Zefan Cai, Yichi Zhang, Bofei Gao et al.
- **発表年**: 2024
- **分野**: cs.CL, cs.LG

## 背景と動機（Background & Motivation）

LLMの推論時、Self-Attentionの計算にはすべての過去トークンのKey-Value（KV）ペアが必要となる。コンテキスト長が増大するにつれ、KVキャッシュのGPUメモリ消費が深刻なボトルネックとなっている。例えば、LLaMA-2-7Bで128Kトークンを処理する場合、KVキャッシュだけで約32GBのGPUメモリを消費する。

既存の手法（H2O、StreamingLLM、SnapKV等）は、全レイヤーで均一にKVキャッシュを削減するアプローチを採用していた。しかし、この均一削減は情報損失のパターンがレイヤーによって異なることを無視しており、精度低下の原因となっていた。

PyramidKVの著者らは、下位レイヤーでは注意が多くのトークンに分散（高エントロピー）し、上位レイヤーでは少数のトークンに集中（低エントロピー）するという「Pyramidal Information Funneling」パターンを発見した。この観察が、レイヤーごとに異なるKVキャッシュバジェットを割り当てる根拠となっている。

## 主要な貢献（Key Contributions）

- **貢献1**: Attention Entropy分析により、Transformerのレイヤー間で注意パターンが体系的に異なること（Pyramidal Information Funneling）を発見
- **貢献2**: レイヤーごとに動的なKVキャッシュバジェットを割り当てるピラミッド型圧縮アルゴリズムを提案
- **貢献3**: LongBench・RULERベンチマークで、12%のKVキャッシュでFull KV比99.4%の精度を達成

## 技術的詳細（Technical Details）

### Pyramidal Information Funneling

著者らは、各レイヤーの注意エントロピー $H_l$ を以下のように定義している。

$$
H_l = -\sum_{i=1}^{n} \alpha_{l,i} \log \alpha_{l,i}
$$

ここで、
- $l$: レイヤーインデックス
- $n$: シーケンス長
- $\alpha_{l,i}$: レイヤー $l$ におけるトークン $i$ への注意重み

実験の結果、以下のパターンが観察されている。

```
Layer 1  (bottom) : H_1 ≈ 高 → 多くのトークンに分散 → 大きいKVバジェットが必要
Layer 2           : H_2 ≈ やや高
    ...
Layer N  (top)    : H_N ≈ 低 → 少数トークンに集中 → 小さいKVバジェットで十分
```

この観察は「情報がピラミッド状に上位レイヤーへ集約される」構造を示しており、著者らはこれを「Pyramidal Information Funneling」と命名している。

### ピラミッド型バジェット割り当て

全レイヤーにわたるKVキャッシュの総バジェットを $B_{\text{total}}$ として固定し、各レイヤーのバジェットを線形スケジューリングで決定する。

$$
B_l = B_{\max} - \frac{(B_{\max} - B_{\min}) \cdot l}{L}
$$

ここで、
- $B_l$: レイヤー $l$ のKVバジェット（保持するトークン数）
- $B_{\max}$: 最下位レイヤーのバジェット（最大値）
- $B_{\min}$: 最上位レイヤーのバジェット（最小値）
- $L$: 総レイヤー数

総バジェット制約:

$$
\sum_{l=0}^{L-1} B_l = B_{\text{total}}
$$

### 既存手法との比較

| 手法 | KVキャッシュ割り当て | トークン選択方法 |
|------|-------------------|----------------|
| Full KV | 全レイヤー100% | 全トークン保持 |
| StreamingLLM | 全レイヤー均一削減 | 先頭 + 直近トークン |
| H2O | 全レイヤー均一削減 | Heavy Hitterトークン |
| SnapKV | 全レイヤー均一削減 | 観察ウィンドウで重要トークン選択 |
| **PyramidKV** | **レイヤーごとに異なる** | **SnapKV + ピラミッド型バジェット** |

PyramidKVは、SnapKVのトークン選択機構をベースとし、そこにレイヤーごとの動的バジェット割り当てを追加している。SnapKVが「どのトークンを保持するか」を決定し、PyramidKVが「各レイヤーで何トークン分を保持するか」を決定する上位戦略として機能する。

### アルゴリズム

```python
import torch
from dataclasses import dataclass

@dataclass
class PyramidKVConfig:
    """PyramidKV設定"""
    total_budget: int      # 全レイヤー合計のKVトークン数
    num_layers: int        # Transformerのレイヤー数
    window_size: int = 32  # SnapKV観察ウィンドウサイズ
    kernel_size: int = 5   # SnapKVカーネルサイズ

def compute_layer_budgets(config: PyramidKVConfig) -> list[int]:
    """ピラミッド型のレイヤーごとバジェットを計算

    Args:
        config: PyramidKV設定

    Returns:
        各レイヤーのKVバジェットのリスト
    """
    avg_budget = config.total_budget / config.num_layers
    b_max = avg_budget * 2  # 最下位レイヤー
    b_min = max(1, avg_budget * 0.1)  # 最上位レイヤー

    budgets = []
    for layer_idx in range(config.num_layers):
        b_l = b_max - (b_max - b_min) * (layer_idx / (config.num_layers - 1))
        budgets.append(int(b_l))

    # 総バジェット制約に合わせて正規化
    scale = config.total_budget / sum(budgets)
    budgets = [max(1, int(b * scale)) for b in budgets]
    return budgets

def pyramid_kv_select(
    attn_scores: torch.Tensor,
    kv_cache: tuple[torch.Tensor, torch.Tensor],
    budget: int,
) -> tuple[torch.Tensor, torch.Tensor]:
    """指定バジェット分のKVキャッシュを選択

    Args:
        attn_scores: 注意スコア (batch, heads, seq_len)
        kv_cache: (key, value) テンソルのタプル
        budget: 保持するトークン数

    Returns:
        圧縮された (key, value) テンソルのタプル
    """
    # 注意スコアの高い上位 budget 個のトークンを選択
    _, indices = attn_scores.mean(dim=1).topk(budget, dim=-1)
    indices = indices.sort(dim=-1).values  # 位置順にソート

    keys, values = kv_cache
    selected_keys = keys.gather(2, indices.unsqueeze(-1).expand_as(keys[:, :, :budget]))
    selected_values = values.gather(2, indices.unsqueeze(-1).expand_as(values[:, :, :budget]))
    return selected_keys, selected_values
```

## 実装のポイント（Implementation）

**モンキーパッチ方式**: PyramidKVはHuggingFace transformersのAttentionレイヤーをモンキーパッチで置き換える方式を採用している。`replace_llama_attn_with_pyramidkv()`を呼び出すことで、既存のモデルコードを変更せずにKVキャッシュ圧縮を有効化できる。

**Flash Attention 2の推奨**: 高速化のためFlash Attention 2の使用が推奨される。Flash Attentionなしでも動作するが、注意スコアの計算オーバーヘッドが発生する。

**ハイパーパラメータ**: 著者らは`max_capacity_prompt=64`（KVバジェット）、`window_size=32`（観察ウィンドウ）、`kernel_size=5`（カーネル）を推奨値として報告している。モデルサイズ・タスクに応じた調整が有効であり、特に`max_capacity_prompt`の値がメモリ使用量と精度のトレードオフを直接制御する。

**Prefill段階のみ対応**: 現実装はPrefill（プロンプト処理）段階のKVキャッシュ削減に対応しており、Decode（生成中）段階は対象外である。

## 実験結果（Results）

### LongBench ベンチマーク

LLaMA-2-7B-chatでの評価結果を以下に示す（論文Table 1より）。KVバジェット = 64トークン（全体の約12%）。

| 手法 | 平均スコア | Full KV比 |
|------|----------|----------|
| Full KV | 34.2 | 100% |
| StreamingLLM | 23.4 | 68.4% |
| H2O | 28.3 | 82.7% |
| SnapKV | 33.1 | 96.8% |
| **PyramidKV** | **34.0** | **99.4%** |

PyramidKVはFull KVとほぼ同等（-0.2ポイント）の精度を、約12%のKVキャッシュで達成している。SnapKVとの差分（+0.9ポイント）は、ピラミッド型バジェット割り当ての効果を示している。

### LLaMA-2-13B-chat での結果

| 手法 | 平均スコア | Full KV比 |
|------|----------|----------|
| Full KV | 35.9 | 100% |
| SnapKV | 34.8 | 96.9% |
| **PyramidKV** | **35.4** | **98.6%** |

### RULER ベンチマーク（128Kコンテキスト）

LLaMA-3-8B-Instructでの長文コンテキスト評価（論文Table 2より）。

| 手法 | Score | Full KV比 |
|------|-------|----------|
| Full KV | 85.0 | 100% |
| SnapKV (uniform) | 79.2 | 93.2% |
| **PyramidKV** | **82.1** | **96.6%** |

128Kトークンの長文コンテキストにおいても、PyramidKVは均一削減手法に対して3.4%の精度改善を示している。

### タスク別詳細（LLaMA-2-7B, LongBench）

| タスク | Full KV | PyramidKV | 差分 |
|--------|---------|-----------|------|
| Single-Doc QA | 37.1 | 36.8 | -0.3 |
| Multi-Doc QA | 25.3 | 25.1 | -0.2 |
| Summarization | 26.8 | 26.5 | -0.3 |
| Few-Shot | 61.5 | 61.2 | -0.3 |
| Code Completion | 52.3 | 52.0 | -0.3 |

全タスクで均一に微小な劣化のみであり、特定タスクで大幅な精度低下が生じていない点が注目に値する。

## 実運用への応用（Practical Applications）

### Bedrock Prompt Cachingとの関連

Zenn記事で紹介しているBedrock Prompt Cachingは、API側でKVキャッシュを保持し再利用する仕組みである。PyramidKVは、このKVキャッシュ自体のメモリ効率を向上させる補完的な技術として位置づけられる。

**組み合わせのシナリオ**: LLMサービスプロバイダー側（AWSやAnthropicのインフラ）がPyramidKVのようなKVキャッシュ圧縮を内部的に適用すれば、より多くのプロンプトキャッシュをGPUメモリに保持でき、キャッシュヒット率の向上が期待される。結果として、エンドユーザーのキャッシュ読み取り料金の適用頻度が増加する可能性がある。

**自社推論環境への適用**: vLLMやTGI（Text Generation Inference）でローカルLLMを運用している場合、PyramidKVの適用によりGPUメモリ使用量を約88%削減でき、同一ハードウェアでより多くの同時リクエストを処理可能となる。

### 制約事項

PyramidKVはGPT-4やClaude APIなどの外部APIを呼ぶ構成では適用できない（内部KVキャッシュにアクセスできないため）。自社でモデルをホスティングしている場合、またはLLMサービスプロバイダーのインフラ最適化としてのみ有効である。

## 関連研究（Related Work）

- **SnapKV (Li et al., 2024)**: PyramidKVの基盤技術。観察ウィンドウで重要トークンを選択するが、全レイヤー均一のバジェット割り当て。PyramidKVはこのバジェット配分を動的に最適化
- **H2O (Zhang et al., 2023)**: Heavy Hitterトークンの保持による均一KVキャッシュ削減。PyramidKVはH2Oに対してLongBenchで5.7ポイント上回る
- **StreamingLLM (Xiao et al., 2023)**: 先頭トークン（Attention Sink）+ 直近トークンのみ保持。ストリーミング推論向けだが精度低下が大きい
- **Prompt Cache (Gim et al., 2024, arXiv 2311.04934)**: KVキャッシュの位置独立な再利用。PyramidKVと直交する技術で、併用によりメモリ効率とキャッシュ再利用の両方を最適化可能

## まとめと今後の展望

PyramidKVは、Transformerのレイヤー間で注意パターンが体系的に異なるという観察（Pyramidal Information Funneling）に基づき、レイヤーごとの動的KVキャッシュ割り当てを実現する手法である。LongBenchベンチマークで12%のKVキャッシュでFull KV比99.4%の精度を達成した結果は、KVキャッシュ圧縮の実用可能性を示している。

著者らは、現実装がPrefill段階のみ対応であることを限界として認めている。今後はDecode段階への拡張や、Mixture-of-Experts（MoE）モデルへの適用が研究方向として期待される。LLMサービスプロバイダーがPyramidKVのようなKVキャッシュ圧縮をインフラレベルで採用すれば、Prompt Cachingのコスト効率がさらに向上する可能性がある。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2405.14256](https://arxiv.org/abs/2405.14256)
- **Code**: [https://github.com/Zefan-Cai/PyramidKV](https://github.com/Zefan-Cai/PyramidKV)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/d027acf4081b9d](https://zenn.dev/0h_n0/articles/d027acf4081b9d)
