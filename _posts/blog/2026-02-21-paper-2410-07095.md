---
layout: post
title: "論文解説: MLE-bench — 75のKaggleコンペでMLエンジニアリングエージェントを評価する"
description: "OpenAIが開発したMLE-benchの技術的詳細を解説。75のKaggleコンペティションを用いたMLエンジニアリング能力評価、メダル率ベースのスコアリング、ICLR 2025 Oral採択の研究を網羅"
categories: [blog, paper, arxiv]
tags: [MLE-bench, LLM, agent, benchmark, evaluation, machine-learning, Kaggle]
date: 2026-02-21 12:00:00 +0900
source_type: arxiv
arxiv_id: "2410.07095"
source_url: https://arxiv.org/abs/2410.07095
zenn_article: cdb9712312bdbf
zenn_url: https://zenn.dev/0h_n0/articles/cdb9712312bdbf
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

MLE-bench（Machine Learning Engineering Benchmark）は、OpenAIが2024年に開発した、AIエージェントのMLエンジニアリング能力を評価するためのベンチマークです。Kaggleの実コンペティション75件を「データリーク防止」処置を施した上で再構成し、エージェントにデータ分析、特徴量エンジニアリング、モデル学習、ハイパーパラメータチューニング、提出ファイル生成までの**ML開発全プロセス**を自律的に実行させます。Kaggleメダル（銅・銀・金）の獲得基準で評価するユニークな仕組みが特徴で、ICLR 2025にOral論文として採択されています。

この記事は [Zenn記事: LLMエージェント評価ベンチマーク完全ガイド：SWE-bench・GAIA・τ-benchの選び方と実践](https://zenn.dev/0h_n0/articles/cdb9712312bdbf) の深掘りです。

## 情報源

- **arXiv ID**: 2410.07095
- **URL**: [https://arxiv.org/abs/2410.07095](https://arxiv.org/abs/2410.07095)
- **著者**: Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Gatys, Tejal Patwardhan, Lilian Weng, Aleksander Mądry
- **発表年**: 2024（ICLR 2025 Oral採択）
- **分野**: cs.AI, cs.LG

## 背景と動機（Background & Motivation）

MLエンジニアリングは、データサイエンティストの日常業務の中核です。データの前処理、特徴量設計、モデル選択、ハイパーパラメータチューニング、クロスバリデーション、結果分析という一連のプロセスは、専門知識と経験を要する複雑なタスクです。

従来のコード生成ベンチマーク（HumanEval, SWE-bench等）は、MLエンジニアリング特有の以下のスキルを評価できません。

1. **データの探索的分析（EDA）**: データの分布、欠損値、外れ値の発見
2. **特徴量エンジニアリング**: ドメイン知識に基づく特徴量の設計
3. **モデル選択と実験管理**: 複数のアルゴリズムの比較と選択
4. **計算リソースの効率的利用**: GPU/CPU時間とメモリの最適化

MLE-benchは、Kaggleコンペティションという「実世界のMLタスク」を用いることで、これらすべてのスキルを統合的に評価します。

## 主要な貢献（Key Contributions）

- **貢献1**: Kaggle 75コンペティションを用いた、MLエンジニアリング全プロセスを評価するベンチマーク。分類・回帰・画像認識・NLP・推薦システム等、幅広いMLタスクをカバー
- **貢献2**: Kaggleメダル（銅・銀・金）ベースのスコアリング。人間のKagglerとの直接比較が可能
- **貢献3**: データリーク防止メカニズム。Public Leaderboardスコアの非公開化、テストデータの保護

## 技術的詳細（Technical Details）

### ベンチマーク設計

MLE-benchの75コンペティションは以下の難易度分布を持ちます。

| 難易度 | コンペ数 | 割合 | 特徴 |
|-------|---------|------|------|
| Low | 22 | 30% | 表形式データ、基本的な分類・回帰 |
| Medium | 38 | 50% | 画像認識、NLP、特徴量設計が必要 |
| High | 15 | 20% | 複雑なアーキテクチャ、ドメイン固有の工夫が必要 |

### 評価メトリクス

MLE-benchのスコアリングは、Kaggleメダルの閾値に基づきます。

$$
\text{Medal}(s, c) = \begin{cases}
\text{Gold} & \text{if } s \geq \text{gold\_threshold}(c) \\
\text{Silver} & \text{if } s \geq \text{silver\_threshold}(c) \\
\text{Bronze} & \text{if } s \geq \text{bronze\_threshold}(c) \\
\text{None} & \text{otherwise}
\end{cases}
$$

ここで、$s$はエージェントの提出スコア、$c$はコンペティション、各閾値はKaggle Public Leaderboardの人間参加者の順位パーセンタイルに基づきます。

- **Gold**: 上位10%相当
- **Silver**: 上位25%相当
- **Bronze**: 上位40%相当

メダル率は以下の式で計算されます。

$$
\text{Medal Rate}(\%) = \frac{|\{c \in \mathcal{C} : \text{Medal}(s_c, c) \neq \text{None}\}|}{|\mathcal{C}|} \times 100
$$

ここで、$\mathcal{C}$は75コンペの集合、$s_c$はコンペ$c$でのエージェントのスコアです。

### pass@kによるスケーリング効果

MLE-benchでは、同一コンペを複数回試行する**pass@k**が報告されています。

$$
\text{pass@k medal rate} = \frac{|\{c \in \mathcal{C} : \exists j \in [k] : \text{Medal}(s_c^{(j)}, c) \neq \text{None}\}|}{|\mathcal{C}|} \times 100
$$

ここで、$s_c^{(j)}$はコンペ$c$の$j$回目の試行のスコアです。k回の試行で1回でもメダルを獲得すればカウントされます。

```python
import math
from dataclasses import dataclass

@dataclass
class MLEBenchResult:
    """MLE-benchの結果を管理するデータ構造"""
    model_name: str
    competition_results: dict[str, list[float]]  # comp_id -> スコアリスト

    def medal_rate_at_k(self, k: int, medal_thresholds: dict) -> float:
        """pass@kでのメダル率を計算する

        Args:
            k: 試行回数
            medal_thresholds: コンペごとの銅メダル閾値

        Returns:
            メダル率（%）
        """
        n_comps = len(self.competition_results)
        n_medals = 0

        for comp_id, scores in self.competition_results.items():
            threshold = medal_thresholds.get(comp_id, float('inf'))
            # k回の試行で1回でも閾値を超えればメダル
            top_k_scores = sorted(scores, reverse=True)[:k]
            if any(s >= threshold for s in top_k_scores):
                n_medals += 1

        return (n_medals / n_comps) * 100

# 使用例
result = MLEBenchResult(
    model_name="o1-preview + AIDE",
    competition_results={
        "titanic": [0.78, 0.80, 0.79, 0.81, 0.78, 0.82, 0.80, 0.79],
        "house_prices": [0.12, 0.11, 0.13, 0.10, 0.12, 0.11, 0.09, 0.13],
    }
)

print(f"pass@1 medal rate: {result.medal_rate_at_k(1, thresholds):.1f}%")
print(f"pass@8 medal rate: {result.medal_rate_at_k(8, thresholds):.1f}%")
```

### データリーク防止メカニズム

MLベンチマークで最も重大な問題の一つがデータリークです。MLE-benchでは以下の対策を実施しています。

1. **Public LBスコアの非公開化**: Kaggleのpublic leaderboardの具体的なスコアを除去。エージェントがスコアを「暗記」できない
2. **テストラベルの保護**: テストデータのラベルはKaggle APIを通じてのみアクセス可能。直接ダウンロードを防止
3. **実行時間制限**: 各コンペに8時間のタイムリミット。探索的なアプローチ（多数のモデルを試す）にペナルティ

### エージェントアーキテクチャ

MLE-benchでは、AIDE（AI-Driven Development Environment）というスカフォールディングが最も高いスコアを達成しています。

```python
# AIDEの基本的なアプローチ（概念図）
from typing import Optional

class AIDEAgent:
    """AIDE scaffoldingの簡略化モデル

    AIDEは以下のサイクルを繰り返す:
    1. データを分析する
    2. 解法を計画する
    3. コードを生成・実行する
    4. 結果を評価する
    5. 改善案を立案する
    """

    def __init__(self, model: str, max_iterations: int = 20):
        self.model = model
        self.max_iterations = max_iterations
        self.best_score: Optional[float] = None
        self.best_submission: Optional[str] = None

    def solve_competition(
        self, comp_data: dict, time_limit_hours: float = 8.0
    ) -> str:
        """コンペティションを解く

        Args:
            comp_data: コンペのデータ（説明、データファイル等）
            time_limit_hours: 制限時間

        Returns:
            提出ファイルのパス
        """
        # Phase 1: データ探索
        eda_results = self._explore_data(comp_data)

        # Phase 2: 反復的な改善ループ
        for i in range(self.max_iterations):
            # 解法を計画
            plan = self._plan_approach(comp_data, eda_results, i)

            # コードを生成・実行
            code = self._generate_code(plan)
            result = self._execute_code(code)

            # 結果を評価
            if result.score and (
                self.best_score is None
                or result.score > self.best_score
            ):
                self.best_score = result.score
                self.best_submission = result.submission_path

            # 時間制限チェック
            if self._is_time_exceeded(time_limit_hours):
                break

        return self.best_submission

    def _explore_data(self, comp_data: dict) -> dict:
        """データの探索的分析を実行"""
        ...

    def _plan_approach(
        self, comp_data: dict, eda: dict, iteration: int
    ) -> str:
        """解法を計画（前回の試行結果を考慮）"""
        ...

    def _generate_code(self, plan: str) -> str:
        """Pythonコードを生成"""
        ...

    def _execute_code(self, code: str) -> dict:
        """コードを実行し結果を取得"""
        ...
```

### 評価環境の仕様

| リソース | 値 | 備考 |
|---------|-----|------|
| 制限時間 | 8時間 | コンペあたり |
| ディスク | 512GB | データセット + 中間ファイル |
| GPU RAM | 56GB | A100相当 |
| CPU | 16コア | データ前処理用 |
| ネットワーク | 制限あり | pip installは許可、外部API禁止 |

## 実装のポイント（Implementation）

MLE-benchでエージェントを評価する際の実装上の重要ポイントを解説します。

**Kaggle利用規約の遵守**: MLE-benchのデータはKaggle API経由でダウンロードする必要があります。再配布は禁止されており、各ユーザーがKaggleアカウントで同意する必要があります。

**GPU環境の準備**: 75コンペの多くは画像認識やNLPタスクを含み、GPU環境が必須です。A100 40GBまたは同等以上を推奨。

```python
# MLE-benchの評価セットアップ
# 1. 環境準備
# pip install mle-bench

# 2. Kaggle APIキーの設定
import os
os.environ['KAGGLE_USERNAME'] = 'your_username'
os.environ['KAGGLE_KEY'] = 'your_api_key'

# 3. データセットのダウンロード
from mle_bench.data import download_competitions

download_competitions(
    competition_ids=['titanic', 'house-prices-advanced'],
    output_dir='./data'
)

# 4. 評価の実行
from mle_bench.evaluate import evaluate_submission

result = evaluate_submission(
    competition_id='titanic',
    submission_path='./submissions/titanic.csv',
)
print(f"Score: {result.score}")
print(f"Medal: {result.medal}")  # Gold, Silver, Bronze, or None
print(f"Percentile: {result.percentile}")
```

**実行コストの管理**: 75コンペ × 8時間 = 最大600時間のGPU時間が必要です。A100のオンデマンド価格で約$1,800、Spotインスタンスで約$600に削減可能。

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

MLE-benchで評価されるMLエンジニアリングエージェントをAWSにデプロイする場合の構成です。

| 規模 | 月間ジョブ数 | 推奨構成 | 月額コスト | 主要サービス |
|------|------------|---------|-----------|------------|
| **Small** | ~50 | Serverless | $200-500 | SageMaker + S3 + Step Functions |
| **Medium** | ~200 | Hybrid | $1,000-3,000 | SageMaker + ECS + S3 |
| **Large** | 500+ | Container | $5,000-15,000 | EKS + GPU Spot + S3 |

**Small構成の詳細**（月額$200-500）:
- **SageMaker Processing**: ml.g5.xlarge（GPU）、ジョブ単位課金 ($150/月)
- **S3**: データセット・モデル・提出ファイル保存 ($20/月)
- **Step Functions**: MLパイプラインオーケストレーション ($10/月)
- **Bedrock**: エージェントのLLM推論（Claude Haiku）($100/月)
- **CloudWatch**: 監視 ($10/月)

**Large構成の詳細**（月額$5,000-15,000）:
- **EKS**: コントロールプレーン ($72/月)
- **EC2 GPU Spot**: g5.xlarge × 4-8台、最大70%削減 ($2,000/月)
- **SageMaker**: モデルトレーニング ($3,000/月)
- **S3**: 大量データ保存 ($100/月)
- **Bedrock**: Claude Sonnet（複雑なタスク）($2,000/月)

**コスト削減テクニック**:
- SageMaker Managed Spot Trainingで最大90%削減
- EBS gp3ボリュームでストレージIOPSコスト削減
- SageMaker Model Registryで不要なモデルアーティファクト自動削除

**コスト試算の注意事項**:
- 上記は2026年2月時点のAWS ap-northeast-1（東京）リージョン料金に基づく概算値です
- GPUインスタンスの可用性はリージョン・タイミングにより変動します
- 最新料金は [AWS料金計算ツール](https://calculator.aws/) で確認してください

### Terraformインフラコード

**Small構成: SageMaker + S3 + Step Functions**

```hcl
# --- S3バケット ---
resource "aws_s3_bucket" "ml_data" {
  bucket = "mle-bench-ml-data"
}

# --- IAMロール ---
resource "aws_iam_role" "sagemaker_role" {
  name = "mle-bench-sagemaker-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action    = "sts:AssumeRole"
      Effect    = "Allow"
      Principal = { Service = "sagemaker.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "sagemaker_full" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"
}

resource "aws_iam_role_policy" "s3_access" {
  role = aws_iam_role.sagemaker_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect   = "Allow"
      Action   = ["s3:GetObject", "s3:PutObject", "s3:ListBucket"]
      Resource = [
        aws_s3_bucket.ml_data.arn,
        "${aws_s3_bucket.ml_data.arn}/*"
      ]
    }]
  })
}

# --- SageMaker Processing Job（ML実行環境） ---
resource "aws_sagemaker_processing_job" "ml_evaluation" {
  processing_job_name = "mle-bench-eval"
  role_arn            = aws_iam_role.sagemaker_role.arn

  processing_resources {
    cluster_config {
      instance_count = 1
      instance_type  = "ml.g5.xlarge"  # GPU: A10G 24GB
      volume_size_in_gb = 100
    }
  }

  app_specification {
    image_uri = "763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/pytorch-training:2.1-gpu-py310"
  }

  stopping_condition {
    max_runtime_in_seconds = 28800  # 8時間
  }
}

# --- CloudWatch アラーム ---
resource "aws_cloudwatch_metric_alarm" "gpu_utilization" {
  alarm_name          = "mle-bench-gpu-util-low"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 6
  metric_name         = "GPUUtilization"
  namespace           = "/aws/sagemaker/ProcessingJobs"
  period              = 600
  statistic           = "Average"
  threshold           = 10
  alarm_description   = "GPU使用率10%未満（リソース無駄遣い検知）"
}
```

### セキュリティベストプラクティス

- **Kaggle APIキー**: Secrets Manager経由で管理、環境変数ハードコード禁止
- **ネットワーク隔離**: SageMaker VPC内配置、外部通信制限
- **データ暗号化**: S3 KMS暗号化、EBS暗号化有効

### 運用・監視設定

**CloudWatch Logs Insights**:
```sql
-- コンペあたりのGPU利用効率
fields @timestamp, competition_id, gpu_utilization, training_time_hours
| stats avg(gpu_utilization) as avg_gpu,
        max(training_time_hours) as max_hours
        by competition_id
| filter avg_gpu < 20
```

### コスト最適化チェックリスト

- [ ] SageMaker Managed Spot Training有効化（最大90%削減）
- [ ] GPU利用率監視→低利用時アラート
- [ ] EBS gp3ボリューム使用（IOPSコスト削減）
- [ ] S3ライフサイクルポリシー（古いモデルアーティファクト削除）
- [ ] SageMaker自動停止設定（アイドルノートブック）
- [ ] AWS Budgets月額予算設定
- [ ] タグ戦略: `Competition=xxx`, `Trial=yyy`
- [ ] 並列実行の最適化（GPU Spot可用性に依存）

## 実験結果（Results）

### 主要な結果

| モデル + Scaffolding | pass@1 メダル率 | pass@8 メダル率 | スケーリング倍率 |
|---|---|---|---|
| o1-preview + AIDE | 16.9% | 34.1% | 2.0x |
| GPT-4o + AIDE | 12.0% | 28.0% | 2.3x |
| Claude 3.5 Sonnet + AIDE | 14.0% | 30.0% | 2.1x |
| GPT-4o + OpenHands | 8.0% | 18.0% | 2.3x |

最も注目すべき結果は、**pass@1からpass@8への2倍のスケーリング効果**です。これは、同じモデルでも複数回試行することで大幅にスコアが向上することを意味し、エージェントの非決定論的な性質を如実に示しています。

### コンペティションカテゴリ別の成績

| カテゴリ | コンペ数 | 最高メダル率（pass@8） |
|---------|---------|-------------------|
| 表形式分類 | 25 | 45% |
| 画像認識 | 20 | 30% |
| NLP | 15 | 28% |
| 推薦システム | 10 | 20% |
| その他 | 5 | 15% |

表形式データの分類タスクで最も高いメダル率を達成しており、構造化データの前処理とGBDTモデルの適用はエージェントにとって比較的容易なタスクと言えます。一方、画像認識やNLPではドメイン固有のアーキテクチャ選択が必要で、メダル率が低下します。

## 実運用への応用（Practical Applications）

MLE-benchの知見は、MLOpsパイプラインの自動化に直接応用できます。

**AutoML as a Service**: エージェントによる特徴量エンジニアリングとモデル選択の自動化。MLE-benchで銅メダル率30%以上のエージェントは、人間のジュニアデータサイエンティストと同等の基礎能力を持つ

**コスト効率の考慮**: 8時間 × GPU使用という実行コストは、人間のデータサイエンティストの工数と比較する必要があります。現時点では、エージェントの方が安価ですが、メダル率が低いため人間の監督が必要

**限界**: MLE-benchはKaggleの「スコア最適化」に特化しており、実務のMLエンジニアリングで重要な以下のスキルは評価しません：デプロイメント、監視、モデルの説明可能性、ビジネス要件の理解

## 関連研究（Related Work）

- **MLAgentBench（Huang et al., 2023）**: LLMエージェントのML研究タスク評価。MLE-benchより小規模だが、実験設計能力の評価を含む
- **AIDE（2025）**: MLE-benchで最高スコアを達成した自動MLエンジニアリング環境。反復的なコード生成と実行のサイクルが特徴
- **RE-Bench（2024）**: 研究エンジニアリング評価。MLE-benchとは異なり、オープンエンドなMLリサーチタスクを8時間で評価
- **DS-1000（Lai et al., 2023）**: データサイエンス向けコード生成ベンチマーク。MLE-benchとは異なり、関数レベルのコード生成に限定

## まとめと今後の展望

MLE-benchは、MLエンジニアリング能力の評価において唯一無二のベンチマークです。75のKaggleコンペティションという現実的なタスクセット、メダルベースの直感的なスコアリング、データリーク防止メカニズムが組み合わさり、エージェントの実力を公平に測定します。

ICLR 2025でのOral採択は、この研究の重要性を示しています。今後は、Kaggleコンペの追加によるタスクセット拡張、デプロイメントやMLOpsを含む評価範囲の拡大、および時系列・強化学習等のタスクカテゴリの追加が期待されます。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2410.07095](https://arxiv.org/abs/2410.07095)
- **Code**: [https://github.com/openai/mle-bench](https://github.com/openai/mle-bench)
- **OpenAI Blog**: [https://openai.com/index/mle-bench/](https://openai.com/index/mle-bench/)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/cdb9712312bdbf](https://zenn.dev/0h_n0/articles/cdb9712312bdbf)
