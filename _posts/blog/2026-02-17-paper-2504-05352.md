---
layout: post
title: "論文解説: LLMの完全バイナリ化に挑む — W(1+1)A(1×4)ポストトレーニング量子化の技術詳細"
description: "Hessian情報を活用した重みバイナリ化とINT4→4×INT1活性化分解により、学習不要でLLMを1ビット精度に量子化する手法を詳細解説"
categories: [blog, paper, arxiv]
tags: [BitNet, 1-bit-LLM, binary-quantization, post-training-quantization, Hessian, llm]
date: 2026-02-17 13:00:00 +0900
source_type: arxiv
arxiv_id: "2504.05352"
source_url: https://arxiv.org/abs/2504.05352
zenn_article: 0f6d388e314d70
zenn_url: https://zenn.dev/0h_n0/articles/0f6d388e314d70
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

本論文は、既に学習済みのLLM（Llama等）を**追加学習なし**で重み1ビット＋活性化1ビットに量子化する「ポストトレーニング量子化（PTQ）」手法を提案する。核となるのは W(1+1)A(1×4) 構成と呼ばれるフレームワークで、重みにはHessian情報を活用した細粒度グルーピング＋EM（期待値最大化）ベースの最適量子化点決定を適用し、活性化にはINT4量子化結果を等価な4×INT1形式に分解する手法を採用する。従来のW2A4（重み2ビット・活性化4ビット）ベースラインを複数タスクで上回り、LLMの完全バイナリ化に向けた重要な一歩を示した。

この記事は [Zenn記事: 1-bit LLM入門：BitNet b1.58でGPU不要のLLM推論を実現する実践ガイド](https://zenn.dev/0h_n0/articles/0f6d388e314d70) の深掘りです。

## 情報源

- **arXiv ID**: 2504.05352
- **URL**: [https://arxiv.org/abs/2504.05352](https://arxiv.org/abs/2504.05352)
- **著者**: Siqing Song, Chuang Wang, Ruiqi Wang, Yi Yang, Xu-Yao Zhang
- **発表年**: 2025（ACL 2025 Findings採択）
- **分野**: cs.LG, cs.CL

## 背景と動機（Background & Motivation）

### BitNet vs PTQバイナリ化の違い

BitNet b1.58は**最初から三値で学習する**アプローチであり、高い推論効率を実現するが、大規模な事前学習が必要である。一方、既存の高品質モデル（Llama 3、Mistral等）をそのまま低ビット化できれば、学習コストゼロでエッジ展開が可能になる。

ポストトレーニング量子化（PTQ）は学習不要で既存モデルを量子化する手法だが、従来のPTQには以下の限界があった：

1. **4ビット以下での急激な精度劣化**: GPTQ、AWQ等の手法は4ビットまでは高精度だが、2ビット以下では壊滅的な精度低下が生じる
2. **活性化の量子化困難性**: 重みは比較的均一な分布だが、活性化はトークンごとに分布が大きく変動し、外れ値（outlier）が存在するため低ビット量子化が困難
3. **1ビットの壁**: 重み・活性化ともに1ビット（バイナリ）にすると、表現力が極端に制限され、これまでLLMスケールでの成功例はなかった

本論文はこの「1ビットの壁」を、巧妙な量子化フレームワークで突破する。

### W(1+1)A(1×4) 構成の意味

名称が示す量子化ビット構成：

- **W(1+1)**: 重みは1ビット（バイナリ $\{-1, +1\}$）だが、細粒度グループ内でのサブグルーピングに追加の1ビット（合計2ビット相当の情報量）
- **A(1×4)**: 活性化は1ビットだが、チャネル数を4倍に拡張して情報を保持（INT4 → 4×INT1の等価分解）

実効的には「重み約2ビット、活性化約4ビット」の情報量を「1ビット演算」で処理する巧妙な設計である。

## 技術的詳細（Technical Details）

### 重み量子化: Hessian-Aware Fine-Grained Grouping

#### ステップ1: Hessian情報の計算

量子化誤差がモデル出力に与える影響は、損失関数のHessian行列 $\mathbf{H}$ で評価できる。重み $\mathbf{w}$ の微小な変動 $\delta\mathbf{w}$ による損失変化は：

$$
\delta\mathcal{L} \approx \frac{1}{2} \delta\mathbf{w}^T \mathbf{H} \delta\mathbf{w}
$$

ここで $\mathbf{H} = \nabla^2_{\mathbf{w}} \mathcal{L}$ は重みに対するHessian行列。完全なHessianの計算は $O(n^2)$ メモリで非実用的だが、Fisher情報行列近似を用いて対角要素のみを効率的に推定する：

$$
h_{ii} \approx \mathbb{E}\left[\left(\frac{\partial \mathcal{L}}{\partial w_i}\right)^2\right]
$$

$h_{ii}$ が大きい重みほど、量子化誤差がモデル出力に大きく影響する「敏感な」重みである。

#### ステップ2: 細粒度グルーピング

Hessian対角要素 $h_{ii}$ に基づき、重みを「敏感度」でグルーピングする。各グループ内でバイナリ量子化のスケーリング係数を個別に最適化することで、敏感な重みの量子化誤差を最小化する。

```python
import torch
from typing import Optional

def hessian_aware_grouping(
    weight: torch.Tensor,
    hessian_diag: torch.Tensor,
    num_groups: int = 128
) -> tuple[torch.Tensor, torch.Tensor]:
    """Hessian情報に基づく細粒度グルーピング

    Args:
        weight: 重みテンソル [out_features, in_features]
        hessian_diag: Hessian対角要素 [out_features, in_features]
        num_groups: グループ数

    Returns:
        group_ids: 各重みのグループID
        group_scales: グループごとのスケーリング係数
    """
    flat_w = weight.flatten()
    flat_h = hessian_diag.flatten()
    n = flat_w.shape[0]
    group_size = n // num_groups

    # Hessian値でソートしてグルーピング
    # → 敏感度が近い重みを同一グループにまとめる
    sorted_indices = flat_h.argsort()
    group_ids = torch.zeros(n, dtype=torch.long)

    for g in range(num_groups):
        start = g * group_size
        end = min((g + 1) * group_size, n)
        indices = sorted_indices[start:end]
        group_ids[indices] = g

    # グループ単位のスケーリング係数
    group_scales = torch.zeros(num_groups)
    for g in range(num_groups):
        mask = group_ids == g
        group_scales[g] = flat_w[mask].abs().mean()

    return group_ids.reshape(weight.shape), group_scales
```

#### ステップ3: EMベース量子化

各グループ内で、EM（Expectation-Maximization）アルゴリズムによりバイナリ量子化の最適なスケーリング係数とバイアスを決定する。

**Eステップ**: 現在のパラメータで各重みを $\{-1, +1\}$ に割り当て

$$
q_i = \text{sign}(w_i - b_g)
$$

ここで $b_g$ はグループ $g$ のバイアス項。

**Mステップ**: 割り当て結果から最適なスケーリング係数 $\alpha_g$ とバイアス $b_g$ を更新

$$
\alpha_g = \frac{\sum_{i \in g} h_{ii} \cdot |w_i - b_g|}{\sum_{i \in g} h_{ii}}, \quad b_g = \frac{\sum_{i \in g} h_{ii} \cdot w_i}{\sum_{i \in g} h_{ii}}
$$

Hessian重み付きの平均を用いることで、敏感な重みの量子化誤差をより強く最小化する。

EM反復を5-10回実施すると収束する。最終的な量子化重みは：

$$
\hat{w}_i = \alpha_g \cdot q_i + b_g, \quad q_i \in \{-1, +1\}
$$

### 活性化量子化: INT4 → 4×INT1 分解

活性化のバイナリ化は重みよりも困難である。活性化は入力データに依存して動的に変化し、外れ値がバイナリ表現の精度を壊滅的に低下させる。

本論文の革新的アプローチは、**INT4量子化を経由して4×INT1に分解する**ことである。

#### ステップ1: INT4量子化

まず活性化をINT4（4ビット整数、$[-8, 7]$）に量子化する：

$$
x_{\text{int4}} = \text{Clip}\left(\text{Round}\left(\frac{x}{\gamma_x}\right), -8, 7\right), \quad \gamma_x = \frac{\max(|x|)}{8}
$$

#### ステップ2: ビット分解

INT4値を4つの独立なINT1（ビット）に分解する：

$$
x_{\text{int4}} = \sum_{b=0}^{3} 2^b \cdot x^{(b)}, \quad x^{(b)} \in \{0, 1\}
$$

例えば $x_{\text{int4}} = 5 = 0101_2$ は $x^{(0)} = 1, x^{(1)} = 0, x^{(2)} = 1, x^{(3)} = 0$ に分解される。

#### ステップ3: チャネル4倍化

各ビット位置を独立なチャネルとして扱い、チャネル数を4倍に拡張する。元の1チャネルが4チャネルに分解されるため、重み行列も対応して4倍化が必要になる：

$$
\mathbf{y} = \mathbf{x}_{\text{int4}} \cdot \mathbf{W} = \sum_{b=0}^{3} 2^b \cdot (\mathbf{x}^{(b)} \cdot \mathbf{W})
$$

各 $\mathbf{x}^{(b)} \cdot \mathbf{W}$ はバイナリ活性化（0/1）とバイナリ重み（-1/+1）の積であり、XNOR + popcount で超高速に計算可能。

```python
def int4_to_4xint1_decompose(
    x_int4: torch.Tensor
) -> list[torch.Tensor]:
    """INT4活性化を4×INT1に分解

    Args:
        x_int4: INT4量子化済み活性化 [batch, seq_len, features]

    Returns:
        x_bits: 4つのバイナリテンソル [batch, seq_len, features] × 4
    """
    x_bits = []
    for b in range(4):
        # 各ビット位置を抽出
        bit_b = (x_int4 >> b) & 1
        x_bits.append(bit_b.float())
    return x_bits


def binary_gemm_decomposed(
    x_bits: list[torch.Tensor],
    w_binary: torch.Tensor,
    w_scale: torch.Tensor
) -> torch.Tensor:
    """4×INT1分解によるバイナリGEMM

    Args:
        x_bits: 4つのバイナリ活性化テンソル
        w_binary: バイナリ重み {-1, +1}
        w_scale: 重みスケーリング係数

    Returns:
        y: 出力テンソル
    """
    y = torch.zeros_like(x_bits[0] @ w_binary.T)
    for b, x_b in enumerate(x_bits):
        # バイナリ活性化 × バイナリ重み → XNOR + popcount
        y += (2 ** b) * (x_b @ w_binary.T)
    return y * w_scale
```

### スケーリング係数の平滑化

活性化量子化のスケーリング係数 $\gamma_x$ はトークンごとに大きく変動し、推論時のオーバーヘッドとなる。本論文はSmoothQuantの思想を拡張し、量子化誤差に基づいてスケーリング係数を平滑化する：

$$
\gamma_x^{\text{smooth}} = \gamma_x \cdot \left(\frac{\text{diag}(\mathbf{H}_x)}{\max(\text{diag}(\mathbf{H}_x))}\right)^{-\alpha}
$$

ここで $\alpha$ はハイパーパラメータ（通常0.5）、$\mathbf{H}_x$ は活性化に対するHessian近似。敏感なチャネルのスケーリング係数を大きく保ち、精度を維持する。

## 実験結果（Results）

### 精度比較（Llama 2 7B、零ショット）

| 手法 | 構成 | WikiText2 PPL↓ | ARC-Easy↑ | PIQA↑ | 平均↑ |
|------|------|---------------|-----------|-------|-------|
| FP16（ベースライン） | W16A16 | 5.47 | 74.5 | 79.2 | 76.9 |
| GPTQ | W4A16 | 5.63 | 73.1 | 78.8 | 76.0 |
| AWQ | W4A16 | 5.60 | 73.4 | 78.9 | 76.2 |
| QuIP# | W2A16 | 7.85 | 62.3 | 72.1 | 67.2 |
| **本手法** | **W(1+1)A(1×4)** | **7.12** | **65.8** | **74.3** | **70.1** |

W2A4ベースラインと比較して、本手法は同等以上の精度を1ビット演算で達成している。

### 演算効率の理論値

| 構成 | 行列積の演算方式 | 理論的高速化率 |
|------|-----------------|---------------|
| W16A16 | FP16 MAC | 1.0× |
| W4A16 | INT4×FP16混合 | ~2× |
| W2A4 | INT2×INT4混合 | ~8× |
| W(1+1)A(1×4) | XNOR + popcount | **~16×** |

完全バイナリ化により、行列積がXNOR（排他的論理否定和）とpopcount（ビットカウント）で計算可能となり、理論上FP16比16倍の高速化が見込まれる。

## 実装のポイント（Implementation）

### キャリブレーションデータ

PTQ手法はキャリブレーションデータ（少量のラベルなしテキスト）が必要である。本論文ではWikiText-2の128サンプルを使用。キャリブレーションデータの選択は精度に影響するため、ターゲットドメインに近いデータが推奨される。

### Hessian計算のコスト

Fisher情報行列近似によるHessian対角推定は、キャリブレーションデータに対する順伝播のみで実行可能（逆伝播不要のバリアントも存在）。7Bモデルで約10分（単一GPU）。

### よくある落とし穴

1. **外れ値の処理**: 活性化の外れ値を事前に検出・分離してからINT4量子化を適用すべき。外れ値を含めたままだとスケーリング係数が歪む
2. **グループサイズの選択**: Hessianグルーピングのグループサイズが小さすぎるとメタデータのオーバーヘッドが増大、大きすぎると精度劣化。128が推奨
3. **EM反復回数**: 5回未満では収束不十分、20回以上ではコストに見合う改善なし。5-10回が最適

## 実運用への応用（Practical Applications）

### BitNet vs 本手法の使い分け

| 観点 | BitNet b1.58 | 本手法 (PTQ) |
|------|-------------|-------------|
| 学習コスト | **高い**（スクラッチ学習必要） | **ゼロ**（量子化のみ） |
| 推論精度 | 同規模FP16と同等 | FP16よりやや劣化 |
| 対応モデル | 専用モデルのみ | 既存モデル全般 |
| 推論効率 | 最高（三値専用最適化） | 高（バイナリ演算） |

**BitNet b1.58が最適**: 精度最重要、かつ学習リソースがある場合
**本手法が最適**: 既存モデルをすぐにエッジ展開したい場合。学習コストゼロで1ビット推論が可能

### エッジ展開のシナリオ

Llama 2 7B をW(1+1)A(1×4)で量子化した場合：
- **メモリ使用量**: 約1.75GB（FP16比1/8）→ Raspberry Pi 4（4GB）で動作可能
- **演算方式**: XNOR + popcount → ARM NEONのビット演算命令で高速実行
- **キャリブレーション**: 10分（単一GPU or高性能CPU）→ 量子化後はGPU不要

## 関連研究（Related Work）

- **BitNet b1.58 (Ma et al., 2024)**: 学習時から三値化するアプローチ。本手法はPTQアプローチであり相補的
- **GPTQ (Frantar et al., 2022)**: 代表的なPTQ手法だが4ビット以下で精度劣化が著しい。本手法はHessian-aware groupingで1ビット精度を維持
- **QuIP# (Chee et al., 2024)**: ランダム回転行列によるインコヒーレンス処理で2ビットPTQを改善。本手法は1ビットまで到達
- **DB-LLM (ACL 2024)**: Dual-binarization手法。2つの独立な1ビット重みでFP16を近似する別アプローチ

## まとめと今後の展望

本論文は、学習不要のPTQでLLMの重み・活性化を1ビットに量子化するW(1+1)A(1×4)フレームワークを提案した。Hessian-awareグルーピング、EMベース最適量子化、INT4→4×INT1活性化分解の3つの技術革新により、従来のW2A4ベースラインを上回る精度を1ビット演算で達成している。

BitNet b1.58が「最初から1-bitで学習する」学習時アプローチであるのに対し、本手法は「既存モデルをPTQで1-bit化する」推論時アプローチであり、両者は相補的な関係にある。将来的にはCQAP（記事2で解説）とPTQバイナリ化を組み合わせることで、「16-bit学習 → 1.58-bit継続学習 → PTQ 1-bit化」というパイプラインが実現し、最小コストで最高効率のエッジLLM推論が可能になるだろう。

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2504.05352](https://arxiv.org/abs/2504.05352)
- **Code**: [https://github.com/JimmyCrave/LLM-PTQ-binarization](https://github.com/JimmyCrave/LLM-PTQ-binarization)
- **ACL Anthology**: [https://aclanthology.org/2025.findings-acl.459/](https://aclanthology.org/2025.findings-acl.459/)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/0f6d388e314d70](https://zenn.dev/0h_n0/articles/0f6d388e314d70)
