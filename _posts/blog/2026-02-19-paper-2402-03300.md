---
layout: post
title: "論文解説: DeepSeekMath — GRPOによるLLM数学推論の限界突破"
description: "DeepSeekMathとGRPO（Group Relative Policy Optimization）を解説。価値関数不要のRL手法で数学推論を大幅強化した技術的詳細"
categories: [blog, paper, arxiv]
tags: [GRPO, DeepSeek, reinforcement-learning, math-reasoning, LLM, deeplearning]
date: 2026-02-19 22:20:00 +0900
source_type: arxiv
arxiv_id: "2402.03300"
source_url: https://arxiv.org/abs/2402.03300
zenn_article: a8792c6407d6e3
zenn_url: https://zenn.dev/0h_n0/articles/a8792c6407d6e3
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

DeepSeekMathは、数学推論に特化した7Bパラメータの言語モデルである。Common Crawlから抽出した120Bトークンの数学関連データで継続事前学習し、さらに**GRPO（Group Relative Policy Optimization）**という新しい強化学習手法でPost-trainingを実施した。GRPOはPPOから価値関数（Critic）モデルを除去し、グループ内相対比較でベースラインを推定することで、メモリ効率を大幅に改善しながら数学推論能力を向上させる。結果として、MATHベンチマークで51.7%（self-consistency併用で60.9%）を達成し、Gemini-UltraやGPT-4に匹敵する水準に到達した。

この記事は [Zenn記事: 2026年版 フロンティアLLM学習パイプライン完全解説：事前学習からRLまで](https://zenn.dev/0h_n0/articles/a8792c6407d6e3) の深掘りです。

## 情報源

- **arXiv ID**: 2402.03300
- **URL**: https://arxiv.org/abs/2402.03300
- **著者**: Zhihong Shao, Peiyi Wang, Qihao Zhu et al.
- **発表年**: 2024
- **分野**: cs.CL, cs.AI, cs.LG

## 背景と動機（Background & Motivation）

LLMの数学推論能力の強化は、モデルの汎用的な論理的思考力を測る重要な指標である。従来のアプローチは主に2つあった：(1) 数学データでの継続事前学習、(2) PPO（Proximal Policy Optimization）による強化学習。しかし、PPOには**価値関数モデルの学習が必要**であり、これがGPUメモリの大きなボトルネックとなっていた。7Bモデルに対してPPOを適用する場合、ポリシーモデル（7B）+ 報酬モデル（7B）+ 価値関数モデル（7B）+ 参照モデル（7B）= 計28Bパラメータ相当のメモリが必要となる。

DeepSeekMathは、この問題を**GRPO（Group Relative Policy Optimization）**で解決した。GRPOは価値関数モデルを完全に排除し、同一プロンプトから生成した複数の応答のグループ内平均をベースラインとして使用する。

## 主要な貢献（Key Contributions）

- **貢献1**: GRPO（Group Relative Policy Optimization）の提案。PPOから価値関数を除去し、GPUメモリ約50%削減を実現
- **貢献2**: Common Crawlからの数学データ抽出パイプラインを構築し、120Bトークンの高品質数学コーパスを作成
- **貢献3**: 7Bモデルで MATH 51.7% を達成し、オープンモデルとしてGemini-Ultra/GPT-4に匹敵する性能を実証
- **貢献4**: GRPOはRLVR（Reinforcement Learning with Verifiable Rewards）との組み合わせで特に効果的であることを示した

## 技術的詳細（Technical Details）

### GRPOの数学的定式化

GRPOの目的関数は、PPOのClipped Surrogate Objectiveを改変し、価値関数を排除したものである。

まず、各プロンプト $q$ に対して、旧ポリシー $\pi_{\theta_{\text{old}}}$ からグループサイズ $G$ 個の応答 $\{o_1, o_2, \ldots, o_G\}$ をサンプリングする。各応答に対して報酬 $r_i = R(q, o_i)$ を計算し、グループ内の統計量でアドバンテージを推定する：

$$
\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, \ldots, r_G\})}{\text{std}(\{r_1, \ldots, r_G\})}
$$

ここで、
- $\hat{A}_i$: 応答 $o_i$ のアドバンテージ推定値
- $r_i$: 応答 $o_i$ の報酬スコア
- $\text{mean}$, $\text{std}$: グループ内の平均と標準偏差

GRPOの最適化目的関数は：

$$
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D}, \{o_i\} \sim \pi_{\theta_{\text{old}}}(q)} \left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left( \rho_{i,t} \hat{A}_i, \; \text{clip}(\rho_{i,t}, 1-\epsilon, 1+\epsilon) \hat{A}_i \right) - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \right]
$$

ここで、
- $\rho_{i,t} = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}$: 確率比（importance ratio）
- $\epsilon$: PPOクリッピングパラメータ
- $\beta$: KLペナルティ係数
- $D_{\text{KL}}$: KLダイバージェンス（参照モデルからの乖離を制約）

### PPOとの比較

| 要素 | PPO | GRPO |
|------|-----|------|
| ベースライン | 価値関数 $V_\phi(s)$ | グループ平均 $\text{mean}(r)$ |
| 必要モデル数 | 4（ポリシー+報酬+価値+参照） | 3（ポリシー+報酬+参照） |
| メモリ使用量 | 4×モデルサイズ | **3×モデルサイズ**（約25%削減） |
| 価値関数学習 | 必要（追加の学習ループ） | **不要** |
| バリアンス | 低（学習済みベースライン） | 中（グループサイズ依存） |

### データ抽出パイプライン

DeepSeekMathのデータ抽出は、Common Crawlから数学関連ページを効率的に収集するパイプラインで構成される：

1. **シードURL収集**: 数学関連Webサイト（MathOverflow, Art of Problem Solving等）のURLを収集
2. **ドメイン拡張**: シードURLのドメインから類似ドメインをクロール
3. **品質フィルタリング**: fastTextベースの分類器で数学コンテンツを判定
4. **重複排除**: MinHashベースの近似重複検出

このパイプラインで、Common Crawlの全データから約120Bトークンの数学関連コーパスを抽出した。

### アルゴリズム

以下にGRPOの完全な実装を示す：

```python
import torch
import torch.nn.functional as F
from dataclasses import dataclass

@dataclass
class GRPOConfig:
    """GRPO hyperparameters.

    Attributes:
        group_size: Number of responses per prompt
        clip_eps: PPO clipping parameter
        kl_coeff: KL penalty coefficient
        max_grad_norm: Maximum gradient norm for clipping
    """
    group_size: int = 8
    clip_eps: float = 0.2
    kl_coeff: float = 0.04
    max_grad_norm: float = 1.0


def grpo_loss(
    policy_logprobs: torch.Tensor,
    old_logprobs: torch.Tensor,
    ref_logprobs: torch.Tensor,
    rewards: torch.Tensor,
    response_mask: torch.Tensor,
    config: GRPOConfig,
) -> torch.Tensor:
    """Compute GRPO loss.

    Args:
        policy_logprobs: Log probs from current policy (B*G, T)
        old_logprobs: Log probs from old policy (B*G, T)
        ref_logprobs: Log probs from reference model (B*G, T)
        rewards: Reward scores (B*G,)
        response_mask: Mask for valid response tokens (B*G, T)
        config: GRPO hyperparameters

    Returns:
        Scalar loss tensor
    """
    B_G = rewards.shape[0]
    G = config.group_size
    B = B_G // G

    # Compute group-relative advantages
    rewards_grouped = rewards.view(B, G)
    mean_r = rewards_grouped.mean(dim=1, keepdim=True)
    std_r = rewards_grouped.std(dim=1, keepdim=True).clamp(min=1e-8)
    advantages = ((rewards_grouped - mean_r) / std_r).view(B_G)

    # Importance ratio
    ratio = torch.exp(policy_logprobs - old_logprobs)

    # Clipped surrogate objective (per-token)
    surr1 = ratio * advantages.unsqueeze(-1)
    surr2 = torch.clamp(ratio, 1.0 - config.clip_eps,
                         1.0 + config.clip_eps) * advantages.unsqueeze(-1)
    policy_loss = -torch.min(surr1, surr2)

    # KL penalty (per-token)
    kl = policy_logprobs - ref_logprobs
    kl_loss = config.kl_coeff * kl

    # Combine and mask
    total_loss = (policy_loss + kl_loss) * response_mask
    loss = total_loss.sum() / response_mask.sum()

    return loss


def grpo_step(
    policy_model: torch.nn.Module,
    ref_model: torch.nn.Module,
    reward_fn,
    prompt: str,
    tokenizer,
    config: GRPOConfig,
) -> dict:
    """Execute one GRPO training step.

    Args:
        policy_model: Current policy model
        ref_model: Frozen reference model
        reward_fn: Function (prompt, response) -> float
        prompt: Input prompt
        tokenizer: Tokenizer for encoding/decoding
        config: GRPO configuration

    Returns:
        Dict with loss, mean_reward, advantages
    """
    # 1. Generate G responses from old policy
    responses = []
    for _ in range(config.group_size):
        output = policy_model.generate(
            tokenizer.encode(prompt, return_tensors="pt"),
            max_new_tokens=512,
            temperature=0.7,
        )
        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))

    # 2. Score each response
    rewards = torch.tensor([reward_fn(prompt, r) for r in responses])

    # 3. Compute group-relative advantages
    mean_r = rewards.mean()
    std_r = rewards.std().clamp(min=1e-8)
    advantages = (rewards - mean_r) / std_r

    return {
        "mean_reward": mean_r.item(),
        "advantages": advantages,
        "responses": responses,
    }
```

## 実装のポイント（Implementation）

### グループサイズの選択

グループサイズ $G$ はバリアンスと計算コストのトレードオフである。$G=8$ が標準的な推奨値で、$G=4$ ではアドバンテージ推定のバリアンスが高く、$G=16$ では計算コストが倍増する。数学タスクでは報酬が二値（正解/不正解）になりやすいため、$G=8$ でも十分な統計量が得られる。

### 報酬関数の設計

GRPOは**検証可能な報酬（Verifiable Rewards）**と特に相性が良い。数学問題では最終回答の正誤判定、コーディング問題ではテストケースの通過率が自然な報酬となる。自由記述タスクでは報酬設計が困難であり、DPOやKTOのような選好ベース手法が推奨される。

### KLペナルティの調整

$\beta = 0.04$ が推奨値だが、学習が進むにつれてポリシーが参照モデルから大きく乖離する場合は $\beta$ を増加させる。適応的なKL制御（PPOのKL targetに類似）を実装することで、学習の安定性が向上する。

### メモリ最適化

GRPOでは価値関数モデルが不要だが、グループサイズ $G$ 個の応答を同時に保持する必要がある。推論時にはバッチ生成を活用し、$G$ 個の応答を並列生成することで実行時間を最小化する。

## Production Deployment Guide

### AWS実装パターン（コスト最適化重視）

GRPO学習パイプラインのAWSデプロイ構成を示す。

| 規模 | 月間リクエスト | 推奨構成 | 月額コスト | 主要サービス |
|------|--------------|---------|-----------|------------|
| **Small** | ~3,000 | Serverless | $50-150 | Lambda + Bedrock |
| **Medium** | ~30,000 | Hybrid | $300-800 | ECS Fargate + Bedrock |
| **Large** | 300,000+ | Container | $2,000-5,000 | EKS + GPU Spot |

**コスト削減テクニック**:
- Spot Instances: GPU学習で最大90%削減
- Bedrock Batch API: 非リアルタイム推論で50%割引
- Prompt Caching: 30-90%削減

**コスト試算注意事項**: 2026年2月時点のAWS ap-northeast-1料金に基づく概算値。最新料金は [AWS料金計算ツール](https://calculator.aws/) で確認してください。

### Terraformインフラコード

```hcl
resource "aws_lambda_function" "grpo_inference" {
  filename      = "lambda.zip"
  function_name = "grpo-math-reasoning"
  role          = aws_iam_role.lambda_bedrock.arn
  handler       = "index.handler"
  runtime       = "python3.12"
  timeout       = 120
  memory_size   = 2048

  environment {
    variables = {
      BEDROCK_MODEL_ID = "anthropic.claude-3-5-sonnet-20241022-v2:0"
      GROUP_SIZE       = "8"
    }
  }
}

resource "aws_budgets_budget" "grpo_monthly" {
  name         = "grpo-monthly-budget"
  budget_type  = "COST"
  limit_amount = "5000"
  limit_unit   = "USD"
  time_unit    = "MONTHLY"

  notification {
    comparison_operator        = "GREATER_THAN"
    threshold                  = 80
    threshold_type             = "PERCENTAGE"
    notification_type          = "ACTUAL"
    subscriber_email_addresses = ["ops@example.com"]
  }
}
```

### 運用・監視設定

```python
import boto3

cloudwatch = boto3.client('cloudwatch')
cloudwatch.put_metric_alarm(
    AlarmName='grpo-token-spike',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=1,
    MetricName='TokenUsage',
    Namespace='AWS/Bedrock',
    Period=3600,
    Statistic='Sum',
    Threshold=500000,
    AlarmDescription='GRPOトークン使用量異常'
)
```

### コスト最適化チェックリスト

- [ ] Spot Instances優先（最大90%削減）
- [ ] Reserved Instances 1年コミット（72%削減）
- [ ] Bedrock Batch API使用（50%割引）
- [ ] Prompt Caching有効化（30-90%削減）
- [ ] グループサイズ最適化（G=8推奨）
- [ ] Lambda メモリサイズ最適化
- [ ] ECS/EKS アイドル時スケールダウン
- [ ] AWS Budgets 月額予算設定
- [ ] CloudWatch トークン使用量監視
- [ ] Cost Anomaly Detection有効化
- [ ] 日次コストレポート自動送信
- [ ] 未使用リソース定期削除
- [ ] タグ戦略（環境/プロジェクト別）
- [ ] S3ライフサイクル（30日自動削除）
- [ ] 開発環境夜間停止
- [ ] Savings Plans検討
- [ ] モデル選択ロジック（Haiku/Sonnet使い分け）
- [ ] max_tokens制限設定
- [ ] CloudTrail/Config有効化
- [ ] IAM最小権限設定

## 実験結果（Results）

### MATHベンチマーク

DeepSeekMath-7Bの数学推論ベンチマーク結果を示す。

| 手法 | MATH | GSM8K | 備考 |
|------|------|-------|------|
| DeepSeekMath-Base | 34.2% | 64.1% | 継続事前学習のみ |
| + SFT | 46.8% | 79.8% | 教師あり微調整 |
| + GRPO | **51.7%** | **82.9%** | GRPOによるRL |
| + self-consistency (64) | **60.9%** | **88.2%** | 多数決投票 |

### PPOとの比較

同条件でPPOとGRPOを比較した結果、GRPOはPPO比で同等以上の性能を達成しながらメモリ使用量を約25%削減した。

| 手法 | MATH | GPUメモリ | 学習時間 |
|------|------|----------|---------|
| PPO | 50.3% | 4×7B = 28B相当 | 1.0x |
| **GRPO** | **51.7%** | **3×7B = 21B相当** | **0.85x** |

GRPOはメモリ効率だけでなく、学習時間も15%短縮された。価値関数モデルの学習ループが不要であることが主因である。

### アブレーション実験

グループサイズの影響を調査した結果、$G=8$ が精度と効率のバランスが最適であった。

| グループサイズ G | MATH | 計算コスト（相対） |
|----------------|------|-----------------|
| 4 | 49.8% | 0.5x |
| **8** | **51.7%** | **1.0x** |
| 16 | 52.1% | 2.0x |
| 32 | 52.3% | 4.0x |

$G=4$ → $G=8$ で1.9%の改善があるが、$G=8$ → $G=16$ では0.4%の改善にとどまり、計算コストが倍増するため費用対効果が悪い。

## 実運用への応用（Practical Applications）

### 数学・コーディングタスクでのRL

GRPOの最大の実用価値は、**検証可能な報酬がある領域でのPost-training**である。数学問題の正誤判定やコードのテスト通過率など、客観的に正解/不正解を判定できるタスクでGRPOは最も効果的に機能する。DeepSeek R1はこのアプローチをさらにスケールし、RLVR（Reinforcement Learning with Verifiable Rewards）として体系化した。

### コスト効率の改善

PPO比で25%のメモリ削減は、実務では1ノードあたりのバッチサイズ増加に直結する。例えば、8×A100ノードでPPOが7Bモデルを学習する場合のバッチサイズが16であれば、GRPOでは20〜22に増加でき、学習スループットが25%以上向上する。

### 自由記述タスクへの制約

GRPOは検証可能な報酬がない自由記述タスク（要約、翻訳、クリエイティブ生成）では効果が限定的である。これらのタスクでは、DPO（Direct Preference Optimization）やKTO（Kahneman-Tversky Optimization）のような選好ベースの手法が推奨される。

## 関連研究（Related Work）

- **PPO** (Schulman et al., 2017): GRPOのベースとなるRL手法。Clipped Surrogate Objectiveが基本だが、価値関数モデルの学習がボトルネック。
- **DPO** (Rafailov et al., 2023): 選好データから直接ポリシーを最適化する手法。報酬モデルと価値関数の両方を排除するが、オンライン生成を行わない。
- **DeepSeek R1** (2025): GRPOをRLVRとして大規模にスケールし、推論能力（Chain-of-Thought）の強化に成功。
- **REINFORCE** (Williams, 1992): GRPOの理論的基盤。モンテカルロベースライン推定の現代的再解釈とも言える。

## まとめと今後の展望

GRPOは、PPOから価値関数モデルを除去し、グループ内相対比較でアドバンテージを推定する効率的なRL手法である。DeepSeekMath-7Bで MATH 51.7% を達成し、メモリ使用量を25%削減しながらPPO以上の性能を実現した。特に検証可能な報酬がある数学・コーディングタスクで高い効果を発揮する。

今後は、(1) GRPOの自由記述タスクへの拡張（報酬モデルとの組み合わせ）、(2) グループサイズの適応的調整、(3) より大規模モデル（70B+）でのスケーリング検証が重要な研究方向である。DeepSeek R1はこの方向の最初の大きな成果であり、GRPOはPost-trainingの標準手法として定着しつつある。

## 参考文献

- **arXiv**: https://arxiv.org/abs/2402.03300
- **Related Zenn article**: https://zenn.dev/0h_n0/articles/a8792c6407d6e3
