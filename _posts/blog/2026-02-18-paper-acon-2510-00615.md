---
layout: post
title: "論文解説: ACON — 自然言語ガイドライン最適化による長期LLMエージェントのコンテキスト圧縮"
description: "MicrosoftのACONフレームワークが自然言語空間でのガイドライン最適化により、ピークトークン26-54%削減を達成した研究を解説"
categories: [blog, paper, arxiv]
tags: [context-engineering, llm, agent, context-compression, distillation, microsoft]
date: 2026-02-18 12:00:00 +0900
source_type: arxiv
arxiv_id: "2510.00615"
source_url: https://arxiv.org/abs/2510.00615
zenn_article: e918777f3aa87c
zenn_url: https://zenn.dev/0h_n0/articles/e918777f3aa87c
math: true
mermaid: true
target_audience: "修士学生レベル"
---

## 論文概要（Abstract）

LLMエージェントが動的な環境でタスクを解決する際、ツール呼び出しの繰り返しによりコンテキスト長が膨張する。ACON（Agent Context Optimization）は、**自然言語空間での圧縮ガイドライン最適化**というユニークなアプローチで、環境観測（observation）とインタラクション履歴（history）の両方を圧縮するフレームワークである。「圧縮ありで成功するが、圧縮なしで失敗する」対照的な軌跡ペアの分析を通じてガイドラインを反復的に改善し、ピークトークンを**26〜54%削減**しながらタスク性能を維持する。さらに、最適化されたガイドラインを小型モデルに蒸留することで、推論コストを大幅に削減する。

この記事は [Zenn記事: LLMエージェントのContext Engineering実践：4戦略でトークンコスト50%削減](https://zenn.dev/0h_n0/articles/e918777f3aa87c) の深掘りです。

## 情報源

- **arXiv ID**: 2510.00615
- **URL**: [https://arxiv.org/abs/2510.00615](https://arxiv.org/abs/2510.00615)
- **著者**: Minki Kang, Wei-Ning Chen, Dongge Han, Huseyin A. Inan, Lukas Wutschitz, Yanzhi Chen, Robert Sim, Saravan Rajmohan
- **発表年**: 2025年10月
- **分野**: cs.AI, cs.CL
- **コード**: [microsoft/acon](https://github.com/microsoft/acon)

## 背景と動機（Background & Motivation）

### 長期タスクにおけるコンテキスト問題

LLMエージェントが実世界のタスクを解決する際、数十回から数百回のツール呼び出しが発生する。例えばAppWorldベンチマークでは、1タスクあたり平均**42.5回のAPI呼び出し**が必要であり、各API応答がコンテキストに蓄積される。

既存のコンテキスト圧縮手法には以下の限界がある：

| 手法 | 問題点 |
|------|--------|
| FIFO（先入れ先出し） | 古い重要情報を無条件に削除 |
| 検索ベース（Retrieval） | 類似度が高い≠タスク関連が高い |
| LLMLingua（抽出的圧縮） | 文レベルの圧縮で文脈が失われる |
| Naive Prompting | 汎用的な指示では精度が不十分 |

### ACONの着眼点

ACONは、圧縮ガイドラインを**モデルのパラメータではなく自然言語で表現し最適化する**という独創的なアプローチを採る。これにより：

1. **解釈可能性**: ガイドラインが人間にも読める
2. **転移可能性**: 異なるモデル・タスクへの適用が容易
3. **低コスト**: ファインチューニング不要

## 主要な貢献（Key Contributions）

- **貢献1**: 自然言語空間での圧縮ガイドライン最適化パイプラインの提案
- **貢献2**: 観測圧縮（Observation Compression）と履歴圧縮（History Compression）の統一的フレームワーク
- **貢献3**: 小型モデルへの蒸留により、教師モデル性能の95%以上を保持しながら推論コストを削減

## 技術的詳細（Technical Details）

### 圧縮ガイドライン最適化

ACONの中核は、**対照的失敗分析（Contrastive Failure Analysis）**に基づくガイドライン改善である。

#### Phase 1: Utility Maximization ($u_t$)

「圧縮なしで成功するが、圧縮ありで失敗する」タスクのペアを収集し、失敗の原因を分析する。

$$
\mathcal{D}^- = \{(\tau_i^{\text{full}}, \tau_i^{\text{comp}}) \mid \text{success}(\tau_i^{\text{full}}) \wedge \text{fail}(\tau_i^{\text{comp}})\}
$$

ここで$\tau^{\text{full}}$は非圧縮軌跡、$\tau^{\text{comp}}$は圧縮軌跡。

各対照ペアに対して、Optimizer LLMがテキスト形式のフィードバックを生成する：

$$
f_i = \text{LLM}_{\text{opt}}(\text{AnalyzeInstruction}, \tau_i^{\text{full}}, \tau_i^{\text{comp}})
$$

フィードバックを集約してガイドラインを更新：

$$
P^{(1)} = \text{LLM}_{\text{opt}}(\text{UpdateInstruction}, P^{(0)}, \| f_i)
$$

**具体例**: 「Venmoの支払い履歴APIの応答から、取引IDと金額は保持すべきだが、タイムスタンプの秒以下は不要」というフィードバックが生成され、ガイドラインに反映される。

#### Phase 2: Compression Maximization ($c_o$)

Phase 1で得られたガイドラインをさらに最適化し、**タスク成功を維持しながら圧縮率を最大化**する。

成功した圧縮軌跡を分析し、「実際に活用された情報」を特定する：

$$
\mathcal{D}^+ = \{(\tau_i^{\text{comp}}) \mid \text{success}(\tau_i^{\text{comp}})\}
$$

不要な情報を特定し、ガイドラインをさらに引き締める。

### 観測圧縮と履歴圧縮

ACONは2種類の圧縮を独立に制御する。

**観測圧縮（Observation Compression）**: 環境からの出力（API応答、ファイル内容等）を圧縮する。

$$
o'_t = f(o_t, h_{t-1}; \phi, P_{\text{obs}}) \quad \text{if } |o_t| > T_{\text{obs}}
$$

ここで、
- $o_t$: 時刻$t$の生の観測
- $h_{t-1}$: 時刻$t-1$までの履歴（文脈情報として使用）
- $\phi$: 圧縮LLMのパラメータ
- $P_{\text{obs}}$: 観測圧縮ガイドライン
- $T_{\text{obs}}$: 圧縮トリガー閾値（デフォルト: 1024トークン）

**履歴圧縮（History Compression）**: 過去のインタラクション履歴全体を圧縮する。

$$
h'_t = f(h_t; \phi, P_{\text{hist}}) \quad \text{if } |h_t| > T_{\text{hist}}
$$

ここで$T_{\text{hist}}$は履歴圧縮トリガー閾値（デフォルト: 4096トークン）。

### 蒸留パイプライン

最適化されたガイドラインを大型教師モデル（gpt-4.1）で実行し、成功した圧縮軌跡を収集。小型の生徒モデル（Qwen3-14B等）を交差エントロピー損失で学習する：

$$
\min_{\phi_S} \mathbb{E}_{(x,y) \sim \mathcal{D}^+_{\text{train}}} \left[ -\sum_{n} \log f(y_n | x, y_{<n}; \phi_S, P^*) \right]
$$

ここで$\phi_S$は生徒モデルのパラメータ、$P^*$は最適化済みガイドライン。

**蒸留の利点**: 高コストな意思決定（エージェント本体）と低コストな圧縮（蒸留モデル）を分離できる。

### アルゴリズム全体像

```python
def acon_pipeline(
    train_tasks: list[Task],
    agent_model: str = "gpt-4.1",
    compressor_model: str = "gpt-4.1",
    T_obs: int = 1024,
    T_hist: int = 4096,
) -> tuple[str, str]:
    """ACONパイプライン: ガイドライン最適化

    Args:
        train_tasks: 訓練用タスク
        agent_model: エージェントモデル
        compressor_model: 圧縮モデル
        T_obs: 観測圧縮の閾値（トークン数）
        T_hist: 履歴圧縮の閾値（トークン数）

    Returns:
        最適化された観測・履歴ガイドラインのタプル
    """
    # 初期ガイドライン（汎用的な指示）
    P_obs = "環境出力からタスクに関連する情報のみ保持する"
    P_hist = "過去のインタラクションを簡潔に要約する"

    # Phase 1: Utility Maximization
    for task in train_tasks:
        # 圧縮なしで実行
        traj_full = run_agent(task, compression=None)
        # 圧縮ありで実行
        traj_comp = run_agent(
            task,
            compression=(P_obs, P_hist),
            T_obs=T_obs,
            T_hist=T_hist,
        )

        if traj_full.success and not traj_comp.success:
            # 対照的失敗: フィードバック生成
            feedback = analyze_failure(traj_full, traj_comp)
            P_obs, P_hist = update_guidelines(
                P_obs, P_hist, feedback
            )

    # Phase 2: Compression Maximization
    for task in train_tasks:
        traj_comp = run_agent(
            task,
            compression=(P_obs, P_hist),
            T_obs=T_obs,
            T_hist=T_hist,
        )

        if traj_comp.success:
            # 成功した圧縮: さらに圧縮可能な部分を特定
            tighten_feedback = analyze_success(traj_comp)
            P_obs, P_hist = tighten_guidelines(
                P_obs, P_hist, tighten_feedback
            )

    return P_obs, P_hist
```

## 実験結果（Results）

### ベンチマーク

| ベンチマーク | タスク数 | 平均API呼出数 | 特徴 |
|------------|---------|-------------|------|
| AppWorld | 168 | 42.5 | Venmo, Spotify等のシミュレーション |
| OfficeBench | 95 | - | Word, Excel, PDF等のオフィス作業 |
| 8-Objective QA | 100 | - | 検索ツールを使った8問QA |

### メイン結果（gpt-4.1 Agent/Compressor）

| ベンチマーク | 手法 | ピークトークン削減 | 精度 |
|------------|------|----------------|------|
| AppWorld | No Compression | - | 54.2% |
| AppWorld | FIFO | N/A | 47.0% |
| AppWorld | Retrieval | N/A | 49.4% |
| AppWorld | LLMLingua | N/A | 44.6% |
| AppWorld | Naive Prompt | N/A | 51.8% |
| AppWorld | **ACON ($u_t + c_o$)** | **-26%** | **56.5%** |
| OfficeBench | No Compression | - | 76.84% |
| OfficeBench | **ACON ($u_t + c_o$)** | **-38%** | **72.63%** |
| 8-Obj QA | No Compression | - | 0.313 EM |
| 8-Obj QA | **ACON ($u_t + c_o$)** | **-54.5%** | **0.335 EM** |

**注目すべき点**:

1. AppWorldと8-Objective QAでは、ACONが**非圧縮のベースラインを上回る**精度を達成。圧縮がノイズ除去として機能し、エージェントの判断を改善した。
2. OfficeBenchでは約4%の精度低下があるが、38%のトークン削減とのトレードオフとしては妥当。

### 蒸留結果

| 蒸留モデル | AppWorld精度 | 教師比 |
|-----------|------------|--------|
| gpt-4.1（教師） | 56.5% | 100% |
| Qwen3-14B | 53.6% | 94.9% |
| Qwen3-8B | 52.1% | 92.2% |
| Phi-4 | 51.8% | 91.7% |

蒸留モデルは教師性能の**91〜95%**を保持しながら、推論コストを大幅に削減する。

### 小型エージェントモデルの改善

ACONの圧縮を適用することで、小型エージェントモデルの性能が向上する：

| エージェント | 非圧縮 | ACON適用 | 改善率 |
|------------|--------|---------|--------|
| Qwen3-14B (AppWorld) | 26.8% | 33.9% | **+26.5%** |
| Qwen3-14B (8-Obj QA) | 0.158 EM | 0.197 EM | **+24.7%** |

### 最適閾値

アブレーション実験により、以下の閾値が最適と判明：

- **$T_{\text{obs}}$（観測圧縮閾値）**: 1024トークン
- **$T_{\text{hist}}$（履歴圧縮閾値）**: 4096トークン

## 実装のポイント（Implementation）

### 閾値の設定

- **観測が短い場合**（API応答等、数百トークン）: $T_{\text{obs}} = 1024$で十分。圧縮頻度が低く、オーバーヘッドが小さい
- **観測が長い場合**（HTMLページ、ログ等、数千トークン）: $T_{\text{obs}} = 512$に下げることで、より積極的に圧縮

### KVキャッシュの注意点

履歴圧縮は、既存のKVキャッシュを無効化する。圧縮後に全コンテキストを再計算する必要があるため、**履歴圧縮の頻度を下げる**（$T_{\text{hist}}$を大きくする）ことが実用上重要。

一方、観測圧縮はKVキャッシュに影響を与えないため、より頻繁に適用できる。

### ガイドラインの転移

最適化されたガイドラインは自然言語であるため、異なるモデルやタスクへの転移が比較的容易。ただし、タスク固有の用語やAPI構造を含むため、完全な汎用性は期待できない。新しいドメインでは、少量の訓練データ（20〜50タスク）での追加最適化が推奨される。

## 実運用への応用（Practical Applications）

### Zenn記事との関連

[Zenn記事](https://zenn.dev/0h_n0/articles/e918777f3aa87c)のCompress戦略を、ACONは**タスク適応型の自動最適化**に拡張している。Zenn記事で紹介したObservation Maskingが「古いものを一律マスク」するのに対し、ACONは「何を残し何を捨てるかのガイドラインをタスクデータから自動学習する」アプローチであり、精度とコストのバランスをより細かく制御できる。

### 使い分けの指針

| 条件 | 推奨手法 |
|------|---------|
| 導入が容易で即効性を求める | Observation Masking |
| タスク固有の最適化が必要 | **ACON** |
| 推論コスト最小化が最優先 | ACON + 蒸留 |
| 観測が構造化データ（JSON等） | **ACON**（ガイドラインで保持フィールドを指定） |
| 観測が非構造データ（ログ等） | Observation Masking |

## 関連研究（Related Work）

- **The Complexity Trap** (Lindenbauer et al., 2025): Observation Maskingの有効性を示した研究。ACONはこれをさらに洗練し、タスク適応型のガイドラインを学習する
- **LLMLingua** (Jiang et al., 2023): トークンレベルの抽出的圧縮。ACONの実験ではBaselineとして比較され、性能で劣後
- **MemGPT** (Packer et al., 2023): OS風のメモリ階層管理。ACONの履歴圧縮と組み合わせることで相補的に機能する可能性
- **Active Context Compression** (2601.07190): エージェント自身が圧縮タイミングを決定するアプローチ。ACONの閾値ベースのトリガーと対照的

## まとめと今後の展望

### 主要な成果

ACONは、**自然言語ガイドラインの最適化**という独創的なアプローチで、LLMエージェントのコンテキスト圧縮問題に取り組んだ。ピークトークン26〜54%削減を達成しつつ、一部のベンチマークでは非圧縮ベースラインを上回る精度を示した。蒸留パイプラインにより、教師性能の91〜95%を小型モデルで実現可能。

### 実務への示唆

1. **圧縮をノイズ除去として活用**: 適切な圧縮はコスト削減だけでなく、エージェントの判断精度も向上させる
2. **観測と履歴を独立に制御**: 観測圧縮（頻繁、低コスト）と履歴圧縮（低頻度、KVキャッシュ再計算）を分離する
3. **蒸留で推論コスト削減**: 高性能モデルで最適化→小型モデルに蒸留のパイプラインが実用的

### 今後の研究方向

- **GPT-4.1以外のモデル**での検証（Claude, Gemini, DeepSeek等）
- **ドメイン横断のガイドライン転移**の系統的評価
- **適応的閾値**の導入（固定閾値からタスク状態に応じた動的閾値へ）

## 参考文献

- **arXiv**: [https://arxiv.org/abs/2510.00615](https://arxiv.org/abs/2510.00615)
- **Code**: [https://github.com/microsoft/acon](https://github.com/microsoft/acon)
- **Related Zenn article**: [https://zenn.dev/0h_n0/articles/e918777f3aa87c](https://zenn.dev/0h_n0/articles/e918777f3aa87c)
